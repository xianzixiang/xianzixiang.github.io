<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Zixiang Xian">

  
  
  
    
  
  <meta name="description" content="Math Formula Factor graphs Shows how a function of several variables can be factored into a product of simpler functions.
 $$ f(x,y,z) = (x&#43;y) \cdot (y &#43; z) \cdot (x &#43;z) $$
 Very useful for representing posteriors.
$ $$ P(x1, ..., x_n) = P(x_1) \Pi P( x_i | x_{i-1} ) $$ $
$$ P(m|x1, &hellip;, x_n) = P(m) \cdot \Pi P(x_i|m)$$
modeling  What graph should I use for this data?">

  
  <link rel="alternate" hreflang="en-us" href="https://faithio.cn/post/stochastic-expectation-propagation/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="../../js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="../../css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-159321957-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-159321957-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="../../index.webmanifest">
  <link rel="icon" type="image/png" href="../../images/icon_hu1c14c98ea2b483290770e44504a291fb_22334_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="../../images/icon_hu1c14c98ea2b483290770e44504a291fb_22334_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://faithio.cn/post/stochastic-expectation-propagation/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Zixiang&#39;s Personal Page">
  <meta property="og:url" content="https://faithio.cn/post/stochastic-expectation-propagation/">
  <meta property="og:title" content="Stochastic Expectation Propagation | Zixiang&#39;s Personal Page">
  <meta property="og:description" content="Math Formula Factor graphs Shows how a function of several variables can be factored into a product of simpler functions.
 $$ f(x,y,z) = (x&#43;y) \cdot (y &#43; z) \cdot (x &#43;z) $$
 Very useful for representing posteriors.
$ $$ P(x1, ..., x_n) = P(x_1) \Pi P( x_i | x_{i-1} ) $$ $
$$ P(m|x1, &hellip;, x_n) = P(m) \cdot \Pi P(x_i|m)$$
modeling  What graph should I use for this data?"><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-02-29T11:34:28-05:00">
    
    <meta property="article:modified_time" content="2020-04-06T21:29:38-04:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://faithio.cn/post/stochastic-expectation-propagation/"
  },
  "headline": "Stochastic Expectation Propagation",
  
  "datePublished": "2020-02-29T11:34:28-05:00",
  "dateModified": "2020-04-06T21:29:38-04:00",
  
  "author": {
    "@type": "Person",
    "name": "Zixiang Xian"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Zixiang's Personal Page",
    "logo": {
      "@type": "ImageObject",
      "url": "https://faithio.cn/images/icon_hu1c14c98ea2b483290770e44504a291fb_22334_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Math Formula Factor graphs Shows how a function of several variables can be factored into a product of simpler functions.\n $$ f(x,y,z) = (x+y) \\cdot (y + z) \\cdot (x +z) $$\n Very useful for representing posteriors.\n$ $$ P(x1, ..., x_n) = P(x_1) \\Pi P( x_i | x_{i-1} ) $$ $\n$$ P(m|x1, \u0026hellip;, x_n) = P(m) \\cdot \\Pi P(x_i|m)$$\nmodeling  What graph should I use for this data?"
}
</script>

  

  


  


  <script data-ad-client="ca-pub-1438507724851578" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
<style type="text/css">
code.has-jax {font: inherit;
              font-size: 100%;
              background: inherit;
              border: inherit;
              color: #515151;}
</style>



  <title>Stochastic Expectation Propagation | Zixiang&#39;s Personal Page</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="../../">Zixiang&#39;s Personal Page</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="../../">Zixiang&#39;s Personal Page</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="../../#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="../../#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="../../#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="../../cv/zixiangxian-cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">














  

  
  
  
<div class="article-container pt-3">
  <h1 itemprop="name">Stochastic Expectation Propagation</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
          Last updated on
      
    
    Apr 6, 2020
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    13 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>




<script>
  function getCookie(name) {
    var value = "; " + document.cookie;
    var parts = value.split("; " + name + "=");
    if (parts.length == 2) return parts.pop().split(";").shift();
  }
  function setCookie(name,value,days) {
    var expires = "";
    if (days) {
        var date = new Date();
        date.setTime(date.getTime() + (days*24*60*60*1000));
        expires = "; expires=" + date.toUTCString();
    }
    document.cookie = name + "=" + (value || "")  + expires + "; path=/";
  }
    (function(){
        
    })();
</script>


  <div class="article-container">

    <div class="article-style">
    

<h1 id="math-formula">Math Formula</h1>

<h3 id="factor-graphs">Factor graphs</h3>

<p>Shows how a function of several variables can be factored into a product of simpler functions.</p>

<blockquote>
<p>$$ f(x,y,z) = (x+y) \cdot (y + z) \cdot (x  +z) $$</p>
</blockquote>

<p>Very useful for representing posteriors.</p>

<p><code>$ 
$$
P(x1, ..., x_n) = P(x_1) \Pi P( x_i | x_{i-1} )
$$
$</code></p>

<p>$$ P(m|x1, &hellip;, x_n) = P(m) \cdot \Pi P(x_i|m)$$</p>

<h4 id="modeling">modeling</h4>

<ul>
<li>What graph should I use for this data?</li>
</ul>

<h4 id="inference">Inference</h4>

<ul>
<li>Given the graph and data, what is the mean of x</li>
<li>algorithm

<ul>
<li>Sampling</li>
<li>Variable elimination</li>
<li>Message-passing(Expectation Propagation, Variational Bayes)</li>
</ul></li>
</ul>

<h3 id="cutter-problem">Cutter problem</h3>

<ul>
<li>Want to estimate x given multiple y&rsquo;s</li>
<li>$$ p(x) = \mathcal{N}(x; 0, 100) $$</li>
<li>$$ p(y_i|x) = (0.5)\mathcal{N}(y_i; x, 1) + (0.5)\mathcal{N} (y_i;0,10)$$</li>
</ul>

<p>-&gt; $ P(x|y1, &hellip;, y_n) = P(x) \cdot \Pi P(y_i|x)$</p>

<p>if we only have 2 points:</p>

<p>$$ P(x) \cdot  P(y_1|x) \cdot P(y_2|x)  \rightarrow   p(y_i|x) = (0.5)\mathcal{N}(y_i; x, 1) + (0.5)\mathcal{N} (y_i;0,10)$$</p>

<p>2 points have 4 Gaussians -&gt; N points $$2^N$$ Gaussians</p>

<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gck12m03b5j30nf0f5diu.jpg" alt="image-20200305220454120" /></p>

<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gck1kw7d6ij30mw0h5mz1.jpg" alt="image-20200305222232412" /></p>

<p><img src="https://pic4.zhimg.com/80/v2-1186de08c2ac2ef05b30cd88b5260707_720w.jpg" alt="img" /></p>

<blockquote>
<p><a href="https://zhuanlan.zhihu.com/p/75617364" target="_blank">https://zhuanlan.zhihu.com/p/75617364</a>
$$
p(z | w)=\frac{p(w | z) p(z)}{p(w)}=\frac{p(w | z) p(z)}{\int_{z} p(w | z) p(z) d z}
$$
Because it extends belief propagation. Belief propagation passes the entire distribution is the message. While EP will only pass onto the distribution certain expectation distribution allows you to you get a very compact message.</p>
</blockquote>

<h3 id="expectation-propagation">Expectation Propagation</h3>

<ol>
<li>Fits an exponential-family approximation to the posterior.</li>
<li>Belief propagation is a special case</li>
<li>Kalman filtering is a special case</li>
<li>Does not always converge.

<ol>
<li>May get stuck due to improper distributions</li>
<li>May oscillate due to loopy graph</li>
</ol></li>
</ol>

<hr />

<h3 id="agm">AGM</h3>

<p>$$
p(\mathbf{X} | \Theta)=\sum<em>{j=1}^{M} p</em>{j} p\left(\mathbf{X} | \xi_{j}\right)
$$</p>

<ul>
<li>$\xi_j$ is the set of the parameters of component j.</li>
<li>$ p_j$ are the mixing proptions which must be positive and sum to one.</li>
<li>$\Theta = {p_1, \ldots, p_M, \xi_1, \ldots, \xi_M}$ is the complete set of parameters fully characterizing the mixture.</li>
<li>$ M \geq 1$ is number of components in the mixture.</li>
</ul>

<p>$$
p\left(X | \theta\right)=\prod<em>{d=1}^{D} \sqrt{\frac{2}{\pi}} \frac{1}{\left(\sigma</em>{l<em>{d}}+\sigma</em>{r<em>{d}}\right)} \times\left{\begin{array}{ll}\exp \left[-\frac{\left(X</em>{d}-\mu<em>{d}\right)^{2}}{2 \sigma</em>{l<em>{d} }^{2}}\right] &amp; \text { if } X</em>{d}&lt;\mu<em>{d} \ \exp \left[-\frac{\left(X</em>{d}-\mu<em>{d}\right)^{2}}{2 \sigma</em>{r<em>{d}}^{2}}\right] &amp; \text { if } X</em>{d} \geq \mu<em>{d}\end{array}\right. <br />
$$
- $\vec{\mu}=\left(\mu</em>{1}, \ldots, \mu<em>{D}\right)$ is the mean
- $\vec{\sigma}</em>{l}=\left(\vec{\sigma}<em>{l</em>{1}}, \ldots, \vec{\sigma}<em>{l</em>{D}}\right)$ is the left standard deviation
- $\vec{\sigma}<em>{r}=\left(\vec{\sigma}</em>{r<em>{1}}, \ldots, \vec{\sigma}</em>{r_{D}}\right)$ is the right standard deviation</p>

<p>$$
\log P = \sum<em>{d=1}^{D} \log \sqrt{\frac{2}{\pi}} - \frac{1}{2}\log (\sqrt{v</em>{l<em>{d}}} + \sqrt{v</em>{r<em>{d}}}) -
\left{\begin{array}{ll}
\frac{\left(X</em>{d}-\mu<em>{d}\right)^{2}}{2 v</em>{l<em>{d} }}  &amp; \text { if } X</em>{d}&lt;\mu<em>{d} <br />
\frac{\left(X</em>{d}-\mu<em>{d}\right)^{2}}{2 v</em>{r<em>{d} }} &amp; \text { if } X</em>{d} \geq \mu<em>{d} <br />
\end{array}\right. <br />
\frac{\partial \log P}{\partial v</em>{l<em>{d}}} = -\frac{1}{4}\frac{1}{v</em>{l<em>{d}} + \sqrt{v</em>{l<em>{d}} v</em>{r<em>{d}} }} + \left{\begin{array}{ll}
\frac{\left(X</em>{d}-\mu<em>{d}\right)^{2}}{2 v</em>{l<em>{d}}^2}  &amp; \text { if } X</em>{d}&lt;\mu<em>{d} <br />
0 &amp; \text { if } X</em>{d} \geq \mu_{d} <br />
\end{array}\right.
$$</p>

<hr />

<p>$$
p(X, \boldsymbol{\theta})=\prod<em>{i} f</em>{i}(\boldsymbol{\theta}) = \prod_{i} p(x<em>i|\boldsymbol{\theta})<br />
%p\left(\theta | X \right) = \frac{1}{p(X)} \prod</em>{i} f_{i}(\boldsymbol{\theta}) <br />
$$</p>

<p>$$
%p(X)= \int \prod<em>{i} f</em>{i}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$</p>

<p>Here, $p(\vec{X})$ is very intractable to calculate and we don&rsquo;t know $ f_{i}(\boldsymbol{\theta}) $.</p>

<p>Now we consider using  <strong>EP</strong>. The approximation, $q\left(\theta_j \right)$ , of the posterior,  $p\left( \theta_j | \vec{X} \right)$ , is assumed to have same functional form.
$$
q(\theta_j)=\frac{1}{Z} \prod_i \widetilde{f}_i(\theta_j)
$$</p>

<p>We hope that:
$$
\mathrm{KL}(p | q)=\mathrm{KL}\left(\frac{1}{p(X)} \prod<em>{i} f</em>{i}(\boldsymbol{\theta}) | \frac{1}{Z} \prod<em>{i} \widetilde{f}</em>{i}(\boldsymbol{\theta})\right)
$$</p>

<p>In general, this minimization will be intractable because the KL divergence involves averaging with respect to the true distribution.</p>

<hr />

<p>But We can use EP:</p>

<ol>
<li>first choose a factor $\widetilde{f}_{j}$ to approximate.</li>
</ol>

<p>Begin Loop, until the following steps are convergence.</p>

<ol>
<li>second compute the cavity distribution $q^{\backslash j}(\boldsymbol{\theta})$:</li>
</ol>

<p>$$
q^{\backslash j}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}_{j}(\boldsymbol{\theta})} \</p>

<p>\hat{p} =
\frac{1}{Z<em>{j}} f</em>{j}(\boldsymbol{\theta}) q^{\backslash j}(\boldsymbol{\theta})
$$</p>

<p>Here $q^{\backslash j}(\boldsymbol{\theta})$ is called the cavity distribution. $\hat{p}$ is defined as a product of the <em>exact</em> factor $f_i$ with the rest of the factors <em>approximated</em>,
normalised to 1, and the cavity distribution needs to be computed in order to express $\hat{p}$.</p>

<blockquote>
<p>$$
q^{\text {new }}(\boldsymbol{\theta}) \propto \widetilde{f}<em>{j}(\boldsymbol{\theta}) \prod</em>{i \neq j} \tilde{f}<em>{i}(\boldsymbol{\theta}) <br />
p \propto \hat{p} = f</em>{j}(\boldsymbol{\theta}) \prod<em>{i \neq j} \tilde{f}</em>{i}(\boldsymbol{\theta})
$$</p>
</blockquote>

<ol>
<li>Then compute the approximative distribution $q^{new}$</li>
</ol>

<p>$$
 \arg \min \mathrm{KL}(\hat{p} | q^{new}(\theta)) = \arg \min \mathrm{KL}\left(\frac{f<em>{j}(\boldsymbol{\theta}) q^{\backslash j}(\boldsymbol{\theta})}{Z</em>{j}} | q^{\mathrm{new}}(\boldsymbol{\theta})\right)
$$</p>

<blockquote>
<p>More generally, it is straightforward to obtain the required expectations for any member of the exponential family, provided it can be normalized, because the expected statistics can be related to the derivatives of the normalization coefficient.
$$
\text{bishop:} <br />
p(\mathbf{x} | \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right} <br />
-\nabla \ln g(\boldsymbol{\eta})=\mathbb{E}[\mathbf{u}(\mathbf{x})] <br />
$$</p>
</blockquote>

<ol>
<li>Update the factor</li>
</ol>

<p>$$
q^{\text {new }}(\boldsymbol{\theta})  \propto \hat{p} = \frac{1}{Z<em>{j}} f</em>{j}(\boldsymbol{\theta}) q^{\backslash j}(\boldsymbol{\theta})
$$</p>

<p>Then we easily obtain the formula for the approximation of $f<em>i$:
$$
f</em>{i} \approx \tilde{f}<em>{i}=Z</em>{i} \frac{q^{\text {new }}(\boldsymbol{\theta})}{q^{\backslash j}(\boldsymbol{\theta})}
$$
This division of distributions is from exponetial family, so does the result $\tilde{f}_{i}$. Now repeat it until parameter covergence.</p>

<p>End Loop.</p>

<ol>
<li>Evaluate the approximation to the model evidence</li>
</ol>

<p>After the algorithm has converged to a set of factors $\left{\tilde{f}<em>{i}\right}$, the approximate posterior as well as the model evidence can be computed as following:
$$
p(X, \boldsymbol{\theta}) \simeq \prod</em>{i} \tilde{f}<em>{i}(\boldsymbol{\theta}) <br />
p(X) \simeq \int \prod</em>{i} \tilde{f}_{i}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$</p>

<hr />

<p>$$
p(\mathbf{X} | \boldsymbol{\theta})=(1-w) \mathcal{A}(\mathbf{X} | \boldsymbol{\theta}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{X} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})
$$</p>

<p>where w is the proportion of background clutter. And the prior over $\mathbf{\theta}$(mean) is taken to be Asymmetric Gaussian.</p>

<p>And
$$
p(\boldsymbol{\theta})= \mathcal{A}(\mathbf{X} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I<em>r})
$$
$$
p(X, \boldsymbol{\theta})=p(\boldsymbol{\theta}) \prod</em>{n=1}^{N} p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right)
$$</p>

<h3 id="1-initialize-the-approximating-factors">1. initialize the approximating factors</h3>

<p>we select an approximating distribution from the exponential family to approximate the stochastic variables $\theta$
$$
q_0(\boldsymbol{\theta})</p>

<p>= \mathcal{A}(\boldsymbol{\theta} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I_r})
$$</p>

<p>$$
\widetilde{f}_{n}(\boldsymbol{\theta})=s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}<em>n, \mathbf{\sigma</em>{r<em>n}^2}, \mathbf{\sigma</em>{l_n}^2} \right)
= s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}<em>n, \mathbf{v</em>{r<em>n} I}, \mathbf{v</em>{l_n} I} \right)
$$</p>

<p>$$
s<em>n = \prod</em>{d=1}^{D} \sqrt{\frac{2}{\pi}} \frac{1}{\left(\sigma<em>{l</em>{d}}+\sigma<em>{r</em>{d}}\right)}
$$
While $\sigma_{l<em>n} \rightarrow \infty,
  \sigma</em>{r_n} \rightarrow \infty
$ and $ \mu_n = 0 $.</p>

<p>###2. initialize the posterior  approximation $q(\boldsymbol{\theta})$</p>

<p>We chooses the parameter values a = 10, b = 100 and w = 0.5 and use $v$ denote $ \sigma^2$ as following, then $\mathbf{v_r} = \mathbf{v_l} = b = 100$</p>

<h3 id="3-until-all-mu-n-v-l-n-v-r-n-s-n-converge">3. Until all $(\mu<em>n, v</em>{l<em>n}, v</em>{r_n}, s_n)$ converge:</h3>

<p>$$
q^{\backslash n}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}_n(\boldsymbol{\theta})} = \frac{\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu}, \mathbf{v_r I}, \mathbf{v_l I})}{s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}<em>n, \mathbf{v</em>{r<em>n} I}, \mathbf{v</em>{l_n} I} \right)} <br />
\propto \left{\begin{array}{ll}</p>

<p>{\frac{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_l \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu})\right}}{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu<em>n})^{T}(v</em>{l_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}}}  &amp;&amp; \text { if } X&lt;\mu \</p>

<p>{\frac{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_r \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu})\right}}{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu<em>n})^{T}(v</em>{r_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}}}  &amp;&amp; \text { if } X&gt;\mu</p>

<p>\end{array}\right. \</p>

<p>= \left{\begin{array}{ll}<br />
\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_l \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu}) + \frac{1}{2}(\boldsymbol{X}-\mathbf{\mu<em>n})^{T}(v</em>{l_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}   &amp; \text { if } X&lt;\mu <br />
\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_r \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu}) +<br />
\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu<em>n})^{T}(v</em>{r_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n}) \right} &amp; \text { if } X&gt;\mu
\end{array}\right.
$$
- Remove the current estimate $\widetilde{f}_j(\boldsymbol{\theta})$ from $q(\theta)$, then we has mean and inverse variance given by:
$$
\left{\begin{array}{ll}
\left({v_l}^{\backslash n}\right)^{-1}={v_l}^{-1}-{v<em>l}</em>{n}^{-1} &amp; \text { if } X&lt;\mu <br />
\left({v_r}^{\backslash n}\right)^{-1}={v_r}^{-1}-{v<em>r}</em>{n}^{-1} &amp; \text { if } X&gt;\mu <br />
\end{array}\right.
$$</p>

<p>$$
\mathbf{\mu}^{\backslash n}= \mathbf{\mu}+</p>

<p>\left{\begin{array}{ll}
{v_l}^{\backslash n} {v<em>l}</em>{n}^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_{n}\right) &amp; \text { if } X&lt;\mu \</p>

<p>{v_r}^{\backslash n} {v<em>r}</em>{n}^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_{n}\right) &amp; \text { if } X&gt;\mu \</p>

<p>\end{array}\right. <br />
&mdash;-<br />
\begin{aligned}
{v^{\backslash{n}}}^{-1} &amp;= v^{-1} - v_n^{-1} <br />
{\mu}^{\backslash n} &amp;= v^{\backslash{n}}(\mu v^{-1} - \mu_n v_n^{-1}) <br />
&amp;= v^{\backslash{n}}[\mu ({v^{\backslash{n}}}^{-1} + v_n^{-1}) - \mu_n v_n^{-1}] <br />
&amp;= \mu + v^{\backslash{n}} v_n^{-1} \mu - v^{\backslash{n}} v_n^{-1}  \mu_n <br />
&amp;= \mu + v^{\backslash{n}} v_n^{-1} (\mu -\mu<em>n)
\end{aligned}
$$
&gt; Cavity Distribution:
&gt; $$
&gt; q^{\backslash n}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}</em>{n}(\boldsymbol{\theta})}
&gt; $$</p>

<ul>
<li>Recompute $(\mu, v, Z)$ from $(\mathbf{\mu}^{\backslash n}, {v_l}^{\backslash n}, {v<em>r}^{\backslash n})$
$$
Z</em>{n}=(1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v<em>r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I<em>r}\right)
$$
&gt;$$
&gt;\begin{aligned}
&gt;Z</em>{n} &amp;=\int q^{\backslash n}(\boldsymbol{\theta}) f_{n}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \
&gt;&amp;=\int q^{\backslash n}(\boldsymbol{\theta}) P(X|\mu) \mathrm{d} \boldsymbol{\theta} <br />
&gt;
&gt;&amp;=\int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I}) \cdot { (1-w) \mathcal{A}(\mathbf{x_n} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{x_n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})} \mathrm{d} \boldsymbol{\theta} <br />
&gt;
&gt;&amp;= (1-w)\int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I}) \mathcal{A}(\mathbf{x_n} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r}) \mathrm{d} \boldsymbol{\theta} <br />
&gt;&amp;+ w \int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I})
&gt;\mathcal{A}(\mathbf{x_n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I<em>r})} \mathrm{d} \boldsymbol{\theta} <br />
&gt;&amp;=(1-w) \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v<em>r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)
&gt;\end{aligned}
&gt;$$</li>
</ul>

<p>we  assumed that  $f<em>{0}(\boldsymbol{\theta})=p(\boldsymbol{\theta})$  and $
f</em>{n}(\boldsymbol{\theta})=p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) = (1-w) \mathcal{A}(\mathbf{X} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{X} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})
$, also $q(\boldsymbol{\theta})=\mathcal{A}(\boldsymbol{\theta} | \mathbf{m}, v_l \mathbf{I}, v<em>r \mathbf{I}) $
$$
\begin{aligned}
\rho</em>{n} &amp;=\frac{1}{Z<em>{n}}(1-w) \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v<em>r^{\backslash n}+1\right) \mathbf{I}\right)  <br />
&amp;= \frac{1}{Z</em>{n}}(1-w)\cdot \frac{Z<em>n - w \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)}{1-w} <br />
&amp;= 1 - \frac{w}{Z<em>n} \cdot \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I<em>r}\right)
\end{aligned}
$$
&gt;Then our goal is to minimize:
&gt;$$
&gt;\mathrm{KL}\left(\frac{f</em>{n}(\boldsymbol{\theta}) q^{\backslash n}(\boldsymbol{\theta})}{Z_{n}} | q^{\mathrm{new}}(\boldsymbol{\theta})\right)
&gt;$$</p>

<p>Basic rule for Asymmetric Gaussian:</p>

<p><a href="https://stats.stackexchange.com/questions/27436/how-to-take-derivative-of-multivariate-normal-density" target="_blank">https://stats.stackexchange.com/questions/27436/how-to-take-derivative-of-multivariate-normal-density</a>
$$
\nabla_{\boldsymbol{\mu}} \mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r})=</p>

<p>\left{\begin{array}{ll}</p>

<p>\mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r}) \cdot(\mathbf{x}-\boldsymbol{\mu}) \mathbf{v_l}^{-1}  &amp; \text { if } X&lt;\mu \</p>

<p>\mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r}) \cdot(\mathbf{x}-\boldsymbol{\mu}) \mathbf{v_r}^{-1}  &amp; \text { if } X&gt;\mu \</p>

<p>\end{array}\right. <br />
$$
So we compute the mean and variance:
$$
\begin{aligned}
\nabla<em>{\mathbf{\mu}^{\backslash n}} \ln Z</em>{n} &amp;=\frac{1}{Z<em>{n}} \cdot \nabla</em>{\mathbf{\mu}^{\backslash n}} Z_{n} \</p>

<p>&amp;=\frac{1}{Z<em>{n}} \cdot \nabla</em>{\mathbf{\mu}^{\backslash n}} \int q^{\backslash n}(\boldsymbol{\theta})f_{n}(\boldsymbol{\theta}) d \boldsymbol{\theta} \</p>

<p>&amp;=\frac{1}{Z<em>{n}} \cdot \nabla</em>{\mathbf{\mu}^{\backslash n}} \int q^{\backslash n}(\boldsymbol{\theta}) p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \</p>

<p>&amp;=\frac{1}{Z<em>{n}} \cdot \int\left{\nabla</em>{\mathbf{\mu}^{\backslash n}} q^{\backslash n}(\boldsymbol{\theta})\right} \cdot p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \</p>

<p>&amp;=\frac{1}{Z<em>{n}} \cdot \int \frac{1}{v^{\backslash n}}\left(\boldsymbol{\theta}-\mathbf{\mu}^{\backslash n}\right) \cdot q^{\backslash n}(\boldsymbol{\theta})
\cdot p\left(\mathbf{x}</em>{n} | \boldsymbol{\theta}\right) d \theta\</p>

<p>&amp;=\frac{1}{Z<em>{n}} \cdot \frac{1}{v^{\backslash n}} \cdot\left{\int \boldsymbol{\theta} \cdot q^{\backslash n}(\boldsymbol{\theta}) \cdot p\left(\mathbf{x}</em>{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta}-\int \mathbf{\mu}^{\backslash n} \cdot q^{\backslash n}(\boldsymbol{\theta}) \cdot p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta}\right} \</p>

<p>&amp;=\frac{1}{v^{\backslash n}} \cdot\left{\mathbb{E}[\boldsymbol{\theta}]-\mathbf{\mu}^{\backslash n}\right} \</p>

<p>&amp;= \left{\mathbb{E}[\boldsymbol{\theta}]-\mathbf{\mu}^{\backslash n}\right} \cdot
\left{\begin{array}{ll}
\frac{1}{v<em>l^{\backslash n}} &amp; \text { if } X</em>{d}&lt;\mu_{d} <br />
\frac{1}{v<em>r^{\backslash n}}  &amp; \text { if } X</em>{d} \geqslant \mu_{d} <br />
\end{array}\right.
\end{aligned}
$$</p>

<p>$$
\text{According to the following}: <br />
q^{\backslash n}(\boldsymbol{\theta})=\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v<em>r^{\backslash n} \mathbf{I}) <br />
q^{\backslash n}(\boldsymbol{\theta}) \cdot p\left(\mathbf{x}</em>{n} | \boldsymbol{\theta}\right)=Z_{n} \cdot q^{new}(\theta) <br />
$$</p>

<p>$$
\begin{aligned} \mathbb{E}[\boldsymbol{\theta}]
&amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \nabla<em>{\mathbf{\mu}^{\backslash n}} \ln Z</em>{n} \
&amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \frac{1}{Z<em>{n}} \nabla</em>{\mathbf{\mu}^{\backslash n}} Z_n \</p>

<p>&amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \frac{1}{Z<em>{n}} \nabla</em>{\mathbf{\mu}^{\backslash n}} (1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v<em>r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)
\</p>

<p>&amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \frac{1}{Z<em>{n}}(1-w)
\nabla</em>{\mathbf{\mu}^{\backslash n}}
\mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v_r^{\backslash n}+1\right) \mathbf{I}\right)</p>

<p>%\cdot \frac{1}{v^{\backslash n}+1}\left(\mathbf{x}_{n}-\mathbf{\mu}^{\backslash n}\right)</p>

<p>\</p>

<p>&amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \rho<em>{n} \cdot \frac{1}{v^{\backslash n}+1}\left(\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right)  \</p>

<p>&amp;=\mathbf{\mu}^{\backslash n}+ \rho_{n} \cdot</p>

<p>\left{\begin{array}{ll}
\frac{1}{v<em>l^{\backslash n}+1}\left(\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right) \cdot v_l^{\backslash n}</p>

<p>&amp; \text { if } X<em>{d}&lt;\mu</em>{d} <br />
\frac{1}{v<em>r^{\backslash n}+1}\left(\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right)  \cdot v<em>l^{\backslash n}
&amp; \text { if } X</em>{d} \geqslant \mu_{d} <br />
\end{array}\right.</p>

<p>\</p>

<p>\text{According to: }&amp; \rho{n} = 1 - \frac{w}{Z<em>n} \cdot \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)    \</p>

<p>&amp;\text{Here we match first moment}:
\mathbb{E}[\boldsymbol{\theta}] = \mathbf{\mu^{new}}
\end{aligned}
$$</p>

<p>Now we consider when:
$$
\text { if } X<em>{d}&lt;\mu</em>{d}
$$</p>

<p>$$
\begin{aligned} \nabla_{v<em>l^{\backslash n}} \ln Z</em>{n}
&amp;=\frac{1}{Z<em>{n}} \cdot \nabla</em>{v<em>l^{\backslash n}} Z</em>{n} \ &amp;=\frac{1}{Z<em>{n}} \cdot \nabla</em>{v<em>l^{\backslash n}} \int q^{\backslash n}(\boldsymbol{\theta}) p\left(\mathbf{x}</em>{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \</p>

<p>&amp;=\frac{1}{Z<em>{n}} \cdot \int\left{\nabla</em>{v<em>l^{\backslash n}} q^{\backslash n}(\boldsymbol{\theta})\right} p\left(\mathbf{x}</em>{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \</p>

<p>&amp;=\frac{1}{Z_{n}} \cdot \int\left{</p>

<p>\frac{1}{2\left(v_l^{\backslash n}\right)^{2}}\left| \boldsymbol{\theta} - \mathbf{\mu}^{\backslash n}\right|^{2}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} }   }</p>

<p>\right}</p>

<p>q^{\backslash n}(\boldsymbol{\theta}) \cdot p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \</p>

<p>&amp;=\int q^{\mathrm{new}}(\boldsymbol{\theta}) \cdot\left{\frac{1}{2\left(v_l^{\backslash n}\right)^{2}}\left(\mathbf{\mu}^{\backslash n}-\boldsymbol{\theta}\right)^{T}\left(\mathbf{\mu}^{\backslash n}-\boldsymbol{\theta}\right)-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }\right} d \boldsymbol{\theta} \</p>

<p>&amp;=\frac{1}{2\left(v_l^{\backslash n}\right)^{2}}\left{\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{T}\right]-2 \mathbb{E}[\boldsymbol{\theta}] \mathbf{\mu}^{\backslash n}+\left|\mathbf{\mu}^{\backslash n}\right|^{2}\right}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } } \end{aligned}
$$</p>

<p>So we rearrange the above equation:
$$
\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{T}\right]=2\left(v<em>l^{\backslash n}\right)^{2} \cdot \nabla</em>{v<em>l^{\backslash n}} \ln Z</em>{n}+2 \mathbb{E}[\boldsymbol{\theta}] \mathbf{m}^{\backslash n}-\left|\mathbf{m}^{\backslash n}\right|^{2}+ \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v<em>r^{\backslash n} } }
$$
Also according to:
$$
Z</em>{n}=(1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v<em>r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}</em>{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)
$$</p>

<p>$$
\nabla_{v<em>l^{\backslash n}} \ln Z</em>{n}  = (1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v_r^{\backslash n}+1\right) \mathbf{I}\right) \cdot \</p>

<p>\left(
\frac{1}{2\left(v_l^{\backslash n} + 1\right)^{2}}\left| \mathbf{x_n} - \mathbf{\mu}^{\backslash n}\right|^{2}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }</p>

<p>\right)  \</p>

<p>= \rho_n \cdot \left(
\frac{1}{2\left(v_l^{\backslash n} + 1\right)^{2}}\left| \mathbf{x_n} - \mathbf{\mu}^{\backslash n}\right|^{2}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }</p>

<p>\right) <br />
$$</p>

<p>According to the bellow formula:
$$
v \mathbf{I}=\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{T}\right]-\mathbb{E}[\boldsymbol{\theta}] \mathbb{E}\left[\boldsymbol{\theta}^{T}\right]
$$</p>

<p>$$
\begin{aligned}
v_l^{new} &amp;=\frac{1}{D} \cdot\left{\mathbb{E}\left[\boldsymbol{\theta}^{T} \boldsymbol{\theta}\right]-\mathbb{E}\left[\boldsymbol{\theta}^{T}\right] \mathbb{E}[\boldsymbol{\theta}]\right}=\frac{1}{D} \cdot\left{\mathbb{E}\left[\boldsymbol{\theta}^{T} \boldsymbol{\theta}\right]-|\mathbb{E}[\boldsymbol{\theta}]|^{2}\right} <br />
&amp;=\frac{1}{D} \cdot\left{ 2\left(v<em>l^{\backslash n}\right)^{2} \cdot \nabla</em>{v<em>l^{\backslash n}} \ln Z</em>{n}+2 \mathbb{E}[\boldsymbol{\theta}] \mathbf{\mu}^{\backslash n}-\left|\mathbf{\mu}^{\backslash n}\right|^{2}</p>

<ul>
<li>\frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } } - |\mathbb{E}[\boldsymbol{\theta}]|^{2} \right}<br /></li>
</ul>

<p>&amp;=\frac{1}{D} \cdot\left{ 2\left(v<em>l^{\backslash n}\right)^{2} \cdot \nabla</em>{v<em>l^{\backslash n}} \ln Z</em>{n}
- \left|\mathbb{E}[\boldsymbol{\theta}]-\mathbf{\mu}^{\backslash n}\right|^{2} + \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }
\right} \</p>

<p>&amp;= \frac{1}{D} \cdot\left{ 2\left(v<em>l^{\backslash n}\right)^{2} \cdot \nabla</em>{v<em>l^{\backslash n}} \ln Z</em>{n}
- \left|
\rho_n \cdot \frac{1}{v<em>l^{\backslash n}+1}\left(\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right) \cdot v_l^{\backslash n}
\right|^{2} + \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }
\right}</p>

<p>\end{aligned}
$$
substitute $\nabla_{v<em>l^{\backslash n}} \ln Z</em>{n}$:
$$
\begin{aligned}
v_l^{new} &amp;=
\frac{1}{D} \cdot\left{ 2\left(v<em>l^{\backslash n}\right)^{2} \cdot \nabla</em>{v<em>l^{\backslash n}} \ln Z</em>{n}
- \left|
\rho_n \cdot \frac{1}{v<em>l^{\backslash n}+1}\left(\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right) \cdot v_l^{\backslash n}
\right|^{2} + \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }
\right} \</p>

<p>&amp;= \frac{1}{D} \cdot\left{ 2\left(v_l^{\backslash n}\right)^{2} \cdot</p>

<p>\rho_n \cdot \left(
\frac{1}{2\left(v_l^{\backslash n} + 1\right)^{2}}\left| \mathbf{x_n} - \mathbf{\mu}^{\backslash n}\right|^{2}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }</p>

<p>\right)</p>

<ul>
<li>\left|
\rho_n \cdot \frac{1}{v<em>l^{\backslash n}+1}\left(\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right) \cdot v_l^{\backslash n}
\right|^{2} + \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }
\right} <br />
&amp;=\frac{\left(v_l^{\backslash n}\right)^{2} - 2 \left(v_l^{\backslash n}\right)^{2} \rho_n}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v<em>r^{\backslash n} } } + \rho</em>{n}\left(1-\rho_{n}\right)
\frac{\left(v<em>l^{\backslash n}\right)^{2}\left|\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right|^{2}}{D\left(v_l^{\backslash n}+1\right)^{2}}</li>
</ul>

<p>\end{aligned}
$$
So in conclusions: we have:</p>

<p>$$
\mathbf{\mu^{new}}=\mathbf{\mu}^{\backslash n}+</p>

<p>\left{\begin{array}{ll}</p>

<p>\rho_{n} \frac{v_l^{\backslash n}}{v<em>l^{\backslash n}+1}\left(\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right)  &amp; \text { if } X&lt;\mu \</p>

<p>\rho_{n} \frac{v_r^{\backslash n}}{v<em>r^{\backslash n}+1}\left(\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right)  &amp; \text { if } X&gt;\mu \</p>

<p>\end{array}\right.
$$</p>

<p>$$
\left{\begin{array}{ll}
v_l^{new}= \frac{\left(v_l^{\backslash n}\right)^{2} - 2 \left(v_l^{\backslash n}\right)^{2} \rho_n}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v<em>r^{\backslash n} } } + \rho</em>{n}\left(1-\rho_{n}\right)
\frac{\left(v<em>l^{\backslash n}\right)^{2}\left|\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right|^{2}}{D\left(v_l^{\backslash n}+1\right)^{2}}</p>

<p>&amp; \text { if } X&lt;\mu \</p>

<p>v_r^{new}=
\frac{\left(v_r^{\backslash n}\right)^{2} - 2 \left(v_r^{\backslash n}\right)^{2} \rho_n}{4 v_r^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }</p>

<p>+\rho<em>{n}\left(1-\rho</em>{n}\right) \frac{\left(v<em>r^{\backslash n}\right)^{2}\left|\mathbf{x}</em>{n}-\mathbf{\mu}^{\backslash n}\right|^{2}}{D\left(v_r^{\backslash n}+1\right)^{2}}
&amp; \text { if } X&gt;\mu \</p>

<p>\end{array}\right.
$$
- Evaluate and store the new factor</p>

<blockquote>
<p>Update formula for $\hat{f}<em>i$
$$
\left{\begin{array}{ll}
\left({v</em>{l_n}}\right)^{-1}={(v_l^{new})}^{-1}-({v<em>l}^{\backslash n})^{-1} &amp; \text { if } X&lt;\mu <br />
\left({v</em>{r_n}}\right)^{-1}={(v_r^{new})}^{-1}-({v_r}^{ \backslash n})^{-1} &amp; \text { if } X&gt;\mu <br />
\end{array}\right.
$$</p>
</blockquote>

<p>$$
\mathbf{\mu}_{n}=\mathbf{\mu}^{\backslash n}+
\left{\begin{array}{ll}</p>

<p>\left(v_{n}+v^{\backslash n}\right)\left(v^{\backslash n}\right)^{-1}\left(\mathbf{\mu}^{\mathrm{new}}-\mathbf{\mu}^{\backslash n}\right)  &amp; \text { if } X&lt;\mu \</p>

<p>\left(v_{n}+v^{\backslash n}\right)\left(v^{\backslash n}\right)^{-1}\left(\mathbf{\mu}^{\mathrm{new}}-\mathbf{\mu}^{\backslash n}\right)  &amp; \text { if } X&gt;\mu \</p>

<p>\end{array}\right.
$$</p>

<p>$$
\begin{aligned}
&amp; \widetilde{f}<em>{n}(\boldsymbol{\theta})=Z</em>{n} \frac{q^{\mathrm{new}}(\boldsymbol{\theta})}{q^{\backslash n}(\boldsymbol{\theta})} \</p>

<p>\Rightarrow &amp; Z_n q^{\mathrm{new}}(\boldsymbol{\theta}) = s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}<em>n, \mathbf{v</em>{r<em>n} I}, \mathbf{v</em>{l_n} I} \right) q^{\backslash n}(\boldsymbol{\theta}) =
s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}<em>n, \mathbf{v</em>{r<em>n} I}, \mathbf{v</em>{l_n} I} \right)
\mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) \</p>

<p>\Rightarrow &amp; \int Z_n q^{\mathrm{new}}(\boldsymbol{\theta}) d\theta =
\int s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}<em>n, \mathbf{v</em>{r<em>n} I}, \mathbf{v</em>{l_n} I} \right)
\mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) d \theta \</p>

<p>\Rightarrow &amp; Z_n = s_n \int q^{\mathrm{new}}(\boldsymbol{\theta}) d\theta =</p>

<p>\int s_n \mathcal{A}\left( \mathbf{\mu}<em>n - \boldsymbol{\theta} | 0, \mathbf{v</em>{r<em>n} I}, \mathbf{v</em>{l_n} I} \right) \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) d \theta \</p>

<p>\Rightarrow &amp; Z_n  = s_n \mathcal{A}\left(\mathbf{\mu}_n | \mathbf{\mu}^{\backslash n}, \mathbf{(v<em>r^{\backslash n}+ v</em>{r_n}) I}, \mathbf{(v<em>l^{\backslash n}+v</em>{r_n}) I} \right)
\end{aligned}
$$</p>

<p>$$
s_n  = \frac{Z_n} {\mathcal{A}\left(\mathbf{\mu}_n | \mathbf{\mu}^{\backslash n}, \mathbf{(v<em>r^{\backslash n}+ v</em>{r_n}) I}, \mathbf{(v<em>l^{\backslash n}+v</em>{r_n}) I} \right)}
$$</p>

<ul>
<li>Evaluate the approximation to the model evidence - Posterior probability. (When $(\mathbf{\mu}_n, {v<em>l}</em> n, {v_r}_n, S_n)$ unchanged )</li>
</ul>

<p>$$
p(X) \simeq q(\boldsymbol{\theta})= \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu},  \mathbf{v_l},  \mathbf{v<em>r}) =
\prod</em>{n=0}^{N} \widetilde{f}<em>{n}(\boldsymbol{\theta})=f</em>{0}(\boldsymbol{\theta}) \prod<em>{n=1}^{N} \widetilde{f}</em>{n}(\boldsymbol{\theta})</p>

<p>=  \mathcal{A}(\boldsymbol{\theta} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I<em>r})\cdot \prod</em>{i=1}^{N} \mathcal{A}\left(\boldsymbol{\mu}_{i}, \mathbf{v<em>l}</em>{i}, \mathbf{v<em>r}</em>{i}\right)
$$</p>

    </div>

  







<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://faithio.cn/post/stochastic-expectation-propagation/&amp;text=Stochastic%20Expectation%20Propagation" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://faithio.cn/post/stochastic-expectation-propagation/&amp;t=Stochastic%20Expectation%20Propagation" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook-f"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Stochastic%20Expectation%20Propagation&amp;body=https://faithio.cn/post/stochastic-expectation-propagation/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://faithio.cn/post/stochastic-expectation-propagation/&amp;title=Stochastic%20Expectation%20Propagation" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Stochastic%20Expectation%20Propagation%20https://faithio.cn/post/stochastic-expectation-propagation/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://faithio.cn/post/stochastic-expectation-propagation/&amp;title=Stochastic%20Expectation%20Propagation" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  






  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="../../authors/admin/avatar_hu1b23c38995cf82eb274855ad3fbd1075_9028_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://faithio.cn/">Zixiang Xian</a></h5>
      <h6 class="card-subtitle">Master Student of Concordia University</h6>
      <p class="card-text">My research interests include Computer Vision, Machine Learning, especially Expectation Propagation.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:xianzixiang@gmail.com" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=EmA1SAkAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/iamfaith" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="../../img/wechat.jpg" >
        <i class="fab fa-weixin"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="../../cv/zixiangxian-cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.zhihu.com/people/xian-zi-xiang" target="_blank" rel="noopener">
        <i class="fab fa-zhihu"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://paypal.me/xianzixiang" target="_blank" rel="noopener">
        <i class="fas fa-donate"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>




<section id="comments">
  
    <div id="commento"></div>

<script src="https://cdn.commento.io/js/commento.js" defer></script>

  
</section>




<div class="article-widget">
  
<div class="post-nav">

  
    
<div style="padding: 10px 0; margin: 20px auto; width: 100%; font-size:16px; text-align: center;">
    <h5>「Thans for supporting」</h5>
    <button id="rewardButton" disable="enable"
        onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
        <span>Denote</span></button>
    <div id="QR" style="display: none;">
        <div id="wechat" style="display: inline-block">
            <h3>
                <a class="fancybox" rel="group" href="https://paypal.me/xianzixiang">Paypal</a>
            </h3>
        </div>

    </div>
</div>



</div>


<style type="text/css">
  #QR {
    padding-top:20px;
}
#QR a {
    border:0
}

#rewardButton {
    border:1px solid #ec7259;
    line-height:36px;
    text-align:center;
    height:36px;
    display:block;
    border-radius:4px;
    -webkit-transition-duration:.4s;
    transition-duration:.4s;
    background-color:#ec7259;
    color:#fff;
    margin:0 auto;
    padding:0 25px;
    cursor: pointer;
    outline: none;
}
#rewardButton:hover {
    color:#fff;
    border-color:#fff;
}

</style>

  




  
  
<div class="post-nav-item">
<div class="meta-nav">Next</div>
  <a href="../../post/run-issue-with-matplotlib-in-mac-os-x/" rel="next">Run Issue With Matplotlib in Mac OS X</a>
</div>

  
  
<div class="post-nav-item">
<div class="meta-nav">Previous</div>
  <a href="../../post/logistic-regression-solvers-defintions-in-sklearn/" rel="prev">Logistic Regression   Solvers&#39; Defintions in Sklearn</a>
</div>

</div>




</div>



  
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.4.4/mermaid.min.js" integrity="sha256-JEqEejGt4tR35L0a1zodzsV0/PJ6GIf7J4yDtywdrH8=" crossorigin="anonymous" title="mermaid"></script>
      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="../../js/academic.min.a8d7005002cb4a052fd6d721e83df9ba.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    

   
      <p>&copy; Faith 11/04/2020 &middot;
                    Built with <a href="https://github.com/gohugoio/hugo">Hugo</a>

       <a href="https://github.com/iamfaith"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path></svg>
</span><span class="username">iamfaith</span></a>



</p>





    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
