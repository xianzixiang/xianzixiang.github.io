<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Zixiang&#39;s Personal Page</title>
    <link>https://faithio.cn/post/</link>
      <atom:link href="https://faithio.cn/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 09 Mar 2020 13:37:09 -0400</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>https://faithio.cn/post/</link>
    </image>
    
    <item>
      <title>Run Issue With Matplotlib in Mac OS X</title>
      <link>https://faithio.cn/post/run-issue-with-matplotlib-in-mac-os-x/</link>
      <pubDate>Mon, 09 Mar 2020 13:37:09 -0400</pubDate>
      <guid>https://faithio.cn/post/run-issue-with-matplotlib-in-mac-os-x/</guid>
      <description>&lt;p&gt;When I run some python code from github, it occur the following problem as screenshot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gco76fc96qj31fo01paai.jpg&#34; alt=&#34;image-20200309133826149&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of &amp;lsquo;python&amp;rsquo; with &amp;lsquo;pythonw&amp;rsquo;. See &amp;lsquo;Working with Matplotlib on OSX&amp;rsquo; in the Matplotlib FAQ for more information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Solution:(&lt;a href=&#34;https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem Cause&lt;/strong&gt; In mac os image rendering back end of matplotlib (what-is-a-backend to render using the API of Cocoa by default). There is Qt4Agg and GTKAgg and as a back-end is not the default. Set the back end of macosx that is differ compare with other windows or linux os.&lt;/p&gt;

&lt;p&gt;I resolve this issue following ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I assume you have installed the pip matplotlib, there is a directory in you root called &lt;code&gt;~/.matplotlib&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Create a file &lt;code&gt;~/.matplotlib/matplotlibrc&lt;/code&gt; there and add the following code: &lt;code&gt;backend: TkAgg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From this &lt;a href=&#34;http://matplotlib.org/examples/index.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; you can try different diagram.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Expectation Propagation</title>
      <link>https://faithio.cn/post/stochastic-expectation-propagation/</link>
      <pubDate>Sat, 29 Feb 2020 11:34:28 -0500</pubDate>
      <guid>https://faithio.cn/post/stochastic-expectation-propagation/</guid>
      <description>

&lt;h1 id=&#34;math-formula&#34;&gt;Math Formula&lt;/h1&gt;

&lt;h3 id=&#34;factor-graphs&#34;&gt;Factor graphs&lt;/h3&gt;

&lt;p&gt;Shows how a function of several variables can be factored into a product of simpler functions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;$$ f(x,y,z) = (x+y) \cdot (y + z) \cdot (x  +z) $$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Very useful for representing posteriors.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ 
$$
P(x1, ..., x_n) = P(x_1) \Pi P( x_i | x_{i-1} )
$$
$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;$$ P(m|x1, &amp;hellip;, x_n) = P(m) \cdot \Pi P(x_i|m)$$&lt;/p&gt;

&lt;h4 id=&#34;modeling&#34;&gt;modeling&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;What graph should I use for this data?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;inference&#34;&gt;Inference&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Given the graph and data, what is the mean of x&lt;/li&gt;
&lt;li&gt;algorithm

&lt;ul&gt;
&lt;li&gt;Sampling&lt;/li&gt;
&lt;li&gt;Variable elimination&lt;/li&gt;
&lt;li&gt;Message-passing(Expectation Propagation, Variational Bayes)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;cutter-problem&#34;&gt;Cutter problem&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Want to estimate x given multiple y&amp;rsquo;s&lt;/li&gt;
&lt;li&gt;$$ p(x) = \mathcal{N}(x; 0, 100) $$&lt;/li&gt;
&lt;li&gt;$$ p(y_i|x) = (0.5)\mathcal{N}(y_i; x, 1) + (0.5)\mathcal{N} (y_i;0,10)$$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-&amp;gt; $ P(x|y1, &amp;hellip;, y_n) = P(x) \cdot \Pi P(y_i|x)$&lt;/p&gt;

&lt;p&gt;if we only have 2 points:&lt;/p&gt;

&lt;p&gt;$$ P(x) \cdot  P(y_1|x) \cdot P(y_2|x)  \rightarrow   p(y_i|x) = (0.5)\mathcal{N}(y_i; x, 1) + (0.5)\mathcal{N} (y_i;0,10)$$&lt;/p&gt;

&lt;p&gt;2 points have 4 Gaussians -&amp;gt; N points $$2^N$$ Gaussians&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gck12m03b5j30nf0f5diu.jpg&#34; alt=&#34;image-20200305220454120&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gck1kw7d6ij30mw0h5mz1.jpg&#34; alt=&#34;image-20200305222232412&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://pic4.zhimg.com/80/v2-1186de08c2ac2ef05b30cd88b5260707_720w.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/75617364&#34; target=&#34;_blank&#34;&gt;https://zhuanlan.zhihu.com/p/75617364&lt;/a&gt;
$$
p(z | w)=\frac{p(w | z) p(z)}{p(w)}=\frac{p(w | z) p(z)}{\int_{z} p(w | z) p(z) d z}
$$
Because it extends belief propagation. Belief propagation passes the entire distribution is the message. While EP will only pass onto the distribution certain expectation distribution allows you to you get a very compact message.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;expectation-propagation&#34;&gt;Expectation Propagation&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Fits an exponential-family approximation to the posterior.&lt;/li&gt;
&lt;li&gt;Belief propagation is a special case&lt;/li&gt;
&lt;li&gt;Kalman filtering is a special case&lt;/li&gt;
&lt;li&gt;Does not always converge.

&lt;ol&gt;
&lt;li&gt;May get stuck due to improper distributions&lt;/li&gt;
&lt;li&gt;May oscillate due to loopy graph&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;agm&#34;&gt;AGM&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;$
$$
p(\mathbf{X} | \Theta)=\sum_{j=1}^{M} p_{j} p\left(\mathbf{X} | \xi_{j}\right)
$$
$&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\xi_j$ is the set of the parameters of component j.&lt;/li&gt;
&lt;li&gt;$ p_j$ are the mixing proptions which must be positive and sum to one.&lt;/li&gt;
&lt;li&gt;$\Theta = {p_1, \ldots, p_M, \xi_1, \ldots, \xi_M}$ is the complete set of parameters fully characterizing the mixture.&lt;/li&gt;
&lt;li&gt;$ M \geq 1$ is number of components in the mixture.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;$
$$
p\left(\vec{X} | \theta_{j}\right)=\prod_{d=1}^{D} \sqrt{\frac{2}{\pi}} \frac{1}{\left(\sigma_{l_{j d}}+\sigma_{r_{j d}}\right)} \times\left\{\begin{array}{ll}\exp \left[-\frac{\left(X_{d}-\mu_{j d}\right)^{2}}{2 \sigma_{l_{j d} }^{2}}\right] &amp;amp; \text { if } X_{d}&amp;lt;\mu_{j d} \\ \exp \left[-\frac{\left(X_{d}-\mu_{j d}\right)^{2}}{2 \sigma_{r_{j d}}^{2}}\right] &amp;amp; \text { if } X_{d} \geq \mu_{j d}\end{array}\right.
$$
$&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$\xi_{j}=\left(\vec{\mu}_{j}, \vec{\sigma}_{l_{j}}, \vec{\sigma}_{r_{j}}\right)$&lt;/code&gt; is the set of the parameters of components $j$&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\vec{\mu}_{j}=\left(\mu_{j 1}, \ldots, \mu_{j D}\right)$&lt;/code&gt; is the mean&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\vec{\sigma}_{l_{j}}=\left(\vec{\sigma}_{l_{j 1}}, \ldots, \vec{\sigma}_{l_{j D}}\right)$&lt;/code&gt; is the left standard deviation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\vec{\sigma}_{r_{j}}=\left(\vec{\sigma}_{r_{j 1}}, \ldots, \vec{\sigma}_{r_{j D}}\right)$&lt;/code&gt; is the right standard deviation&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;$$
p\left(\theta_j | \vec{X} \right)= \frac{p(\theta_j)\times p\left(\vec{X} | \theta&lt;em&gt;j\right)}{p(\vec{X})} = \frac{1}{p(\vec{X})} \prod&lt;/em&gt;{i} f_{i}(\boldsymbol{\theta})
$$&lt;/p&gt;

&lt;p&gt;$$
p(\vec{X})= \int \prod&lt;em&gt;{i} f&lt;/em&gt;{i}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$&lt;/p&gt;

&lt;p&gt;Here, $p(\vec{X})$ is very intractable to calculate and we don&amp;rsquo;t know $ f_{i}(\boldsymbol{\theta}) $.&lt;/p&gt;

&lt;p&gt;Now we consider using  &lt;strong&gt;EP&lt;/strong&gt;. The approximation, $q\left(\theta_j \right)$ , of the posterior,  $p\left( \theta_j | \vec{X} \right)$ , is assumed to have same functional form.
$$
q(\theta_j)=\frac{1}{Z} \prod_i \widetilde{f}_i(\theta_j)
$$&lt;/p&gt;

&lt;p&gt;in which each factor $\widetilde{f}_i(\theta_j)$ in the approximation corresponds to one of the factors
 $f_i(\theta_j)$ in the true posterior. $\widetilde{f}_i(\theta_j) $ is a asymetric Gaussian.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;$$
p(\mathbf{X} | \boldsymbol{\theta})=(1-w) \mathcal{A}(\mathbf{X} | \boldsymbol{\theta}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{X} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})
$$&lt;/p&gt;

&lt;p&gt;where w is the proportion of background clutter. And the prior over $\mathbf{\theta}$(mean) is taken to be Asymmetric Gaussian.&lt;/p&gt;

&lt;p&gt;And
$$
p(\boldsymbol{\theta})= \mathcal{A}(\mathbf{X} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I&lt;em&gt;r})
$$
$$
p(\mathcal{X}, \boldsymbol{\theta})=p(\boldsymbol{\theta}) \prod&lt;/em&gt;{n=1}^{N} p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right)
$$&lt;/p&gt;

&lt;h3 id=&#34;1-initialize-the-approximating-factors&#34;&gt;1. initialize the approximating factors&lt;/h3&gt;

&lt;p&gt;we select an approximating distribution from the exponential family to approximate the stochastic variables $\theta$
$$
q(\boldsymbol{\theta})=\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu}, \mathbf{\sigma_r^2}, \mathbf{\sigma_l^2}) =
\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu}, v_l \mathbf{I}, v_r \mathbf{I})&lt;/p&gt;

&lt;p&gt;= \mathcal{A}(\mathbf{X} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I_r})
$$&lt;/p&gt;

&lt;p&gt;$$
\widetilde{f}_{n}(\boldsymbol{\theta})=s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{\sigma&lt;/em&gt;{r&lt;em&gt;n}^2}, \mathbf{\sigma&lt;/em&gt;{l_n}^2} \right)
$$&lt;/p&gt;

&lt;p&gt;$$
s&lt;em&gt;n = \prod&lt;/em&gt;{d=1}^{D} \sqrt{\frac{2}{\pi}} \frac{1}{\left(\sigma&lt;em&gt;{l&lt;/em&gt;{d}}+\sigma&lt;em&gt;{r&lt;/em&gt;{d}}\right)}
$$
While $\sigma_{l&lt;em&gt;n} \rightarrow \infty,
  \sigma&lt;/em&gt;{r_n} \rightarrow \infty
$ and $ \mu_n = 0 $.&lt;/p&gt;

&lt;p&gt;###2. initialize the posterior  approximation $q(\boldsymbol{\theta})$&lt;/p&gt;

&lt;p&gt;We chooses the parameter values a = 10, b = 100 and w = 0.5 and use $v$ denote $ \sigma^2$ as following, then $\mathbf{v_r} = \mathbf{v_l} = b = 100$&lt;/p&gt;

&lt;h3 id=&#34;3-until-all-mu-n-v-l-n-v-r-n-s-n-converge&#34;&gt;3. Until all $(\mu&lt;em&gt;n, v&lt;/em&gt;{l&lt;em&gt;n}, v&lt;/em&gt;{r_n}, s_n)$ converge:&lt;/h3&gt;

&lt;p&gt;$$
q^{\backslash n}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}_n(\boldsymbol{\theta})} = \frac{\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu}, \mathbf{v_r I}, \mathbf{v_l I})}{s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)} &lt;br /&gt;
\propto \left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;{\frac{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_l \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu})\right}}{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{l_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}}}  &amp;amp;&amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;{\frac{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_r \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu})\right}}{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{r_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}}}  &amp;amp;&amp;amp; \text { if } X&amp;gt;\mu&lt;/p&gt;

&lt;p&gt;\end{array}\right. \&lt;/p&gt;

&lt;p&gt;= \left{\begin{array}{ll}&lt;br /&gt;
\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_l \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu}) + \frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{l_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}   &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_r \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu}) +&lt;br /&gt;
\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{r_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n}) \right} &amp;amp; \text { if } X&amp;gt;\mu
\end{array}\right.
$$
- Remove the current estimate $\widetilde{f}_j(\boldsymbol{\theta})$ from $q(\theta)$, then we has mean and inverse variance given by:
$$
\left{\begin{array}{ll}
\left({v_l}^{\backslash n}\right)^{-1}={v_l}^{-1}-{v&lt;em&gt;l}&lt;/em&gt;{n}^{-1} &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\left({v_r}^{\backslash n}\right)^{-1}={v_r}^{-1}-{v&lt;em&gt;r}&lt;/em&gt;{n}^{-1} &amp;amp; \text { if } X&amp;gt;\mu &lt;br /&gt;
\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\mathbf{\mu}^{\backslash n}= \mathbf{\mu}+&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}
{v_l}^{\backslash n} {v&lt;em&gt;l}&lt;/em&gt;{n}^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_{n}\right) &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;{v_r}^{\backslash n} {v&lt;em&gt;r}&lt;/em&gt;{n}^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_{n}\right) &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$
&amp;gt; Cavity Distribution:
&amp;gt; $$
&amp;gt; q^{\backslash j}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}_{j}(\boldsymbol{\theta})}
&amp;gt; $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Recompute $(\mu, v, Z)$ from $(\mathbf{\mu}^{\backslash n}, {v_l}^{\backslash n}, {v&lt;em&gt;r}^{\backslash n})$
$$
Z&lt;/em&gt;{n}=(1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r}\right)
$$
&amp;gt;$$
&amp;gt;\begin{aligned}
&amp;gt;Z&lt;/em&gt;{n} &amp;amp;=\int q^{\backslash n}(\boldsymbol{\theta}) f&lt;em&gt;{n}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \
&amp;gt;&amp;amp;=\int q^{\backslash n}(\boldsymbol{\theta}) \widetilde{f}&lt;/em&gt;{n}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;
&amp;gt;&amp;amp;=\int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I}) \cdot { (1-w) \mathcal{A}(\mathbf{x_n} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{x_n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})} \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;
&amp;gt;&amp;amp;= (1-w)\int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I}) \mathcal{A}(\mathbf{x_n} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r}) \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;&amp;amp;+ w \int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I})
&amp;gt;\mathcal{A}(\mathbf{x_n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r})} \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;&amp;amp;=(1-w) \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)
&amp;gt;\end{aligned}
&amp;gt;$$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;we  assumed that  $f&lt;em&gt;{0}(\boldsymbol{\theta})=p(\boldsymbol{\theta})$  and $
f&lt;/em&gt;{n}(\boldsymbol{\theta})=p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) = (1-w) \mathcal{A}(\mathbf{X} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{X} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})
$, also $q(\boldsymbol{\theta})=\mathcal{A}(\boldsymbol{\theta} | \mathbf{m}, v_l \mathbf{I}, v&lt;em&gt;r \mathbf{I}) $
$$
\begin{aligned}
\rho&lt;/em&gt;{n} &amp;amp;=\frac{1}{Z&lt;em&gt;{n}}(1-w) \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)  &lt;br /&gt;
&amp;amp;= \frac{1}{Z&lt;/em&gt;{n}}(1-w)\cdot \frac{Z&lt;em&gt;n - w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)}{1-w} &lt;br /&gt;
&amp;amp;= 1 - \frac{w}{Z&lt;em&gt;n} \cdot \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r}\right)
\end{aligned}
$$
Basic rule for Asymmetric Gaussian:
$$
\nabla&lt;/em&gt;{\boldsymbol{\mu}} \mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r})=&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r}) \cdot(\mathbf{x}-\boldsymbol{\mu}) \mathbf{v_l}^{-1}  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r}) \cdot(\mathbf{x}-\boldsymbol{\mu}) \mathbf{v_r}^{-1}  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$
So we compute the mean and variance:
$$
\mathbf{\mu^{new}}=\mathbf{\mu}^{\backslash n}+&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\rho_{n} \frac{v_l^{\backslash n}}{v&lt;em&gt;l^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\rho_{n} \frac{v_r^{\backslash n}}{v&lt;em&gt;r^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\left{\begin{array}{ll}
v_l^{new}=v&lt;em&gt;l^{\backslash n}-\rho&lt;/em&gt;{n} \frac{\left(v_l^{\backslash n}\right)^{2}}{v&lt;em&gt;l^{\backslash n}+1}+\rho&lt;/em&gt;{n}\left(1-\rho_{n}\right) \frac{\left(v&lt;em&gt;l^{\backslash n}\right)^{2}\left|\mathbf{x}&lt;/em&gt;{n}-\mathbf{m}^{\backslash n}\right|^{2}}{D\left(v_l^{\backslash n}+1\right)^{2}} &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;v_r^{new}=v&lt;em&gt;r^{\backslash n}-\rho&lt;/em&gt;{n} \frac{\left(v_r^{\backslash n}\right)^{2}}{v&lt;em&gt;r^{\backslash n}+1}+\rho&lt;/em&gt;{n}\left(1-\rho_{n}\right) \frac{\left(v&lt;em&gt;r^{\backslash n}\right)^{2}\left|\mathbf{x}&lt;/em&gt;{n}-\mathbf{m}^{\backslash n}\right|^{2}}{D\left(v_r^{\backslash n}+1\right)^{2}}
&amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$
- Evaluate and store the new factor
$$
\left{\begin{array}{ll}
\left({v_{l_n}}\right)^{-1}={(v_l^{new})}^{-1}-({v&lt;em&gt;l}^{\backslash n})^{-1} &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\left({v&lt;/em&gt;{r_n}}\right)^{-1}={(v_r^{new})}^{-1}-({v_r}^{ \backslash n})^{-1} &amp;amp; \text { if } X&amp;gt;\mu &lt;br /&gt;
\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\mathbf{m}_{n}=\mathbf{m}^{\backslash n}+
\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\left(v_{n}+v^{\backslash n}\right)\left(v^{\backslash n}\right)^{-1}\left(\mathbf{m}^{\mathrm{new}}-\mathbf{m}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\left(v_{n}+v^{\backslash n}\right)\left(v^{\backslash n}\right)^{-1}\left(\mathbf{m}^{\mathrm{new}}-\mathbf{m}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
