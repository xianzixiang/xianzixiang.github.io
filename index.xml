<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zixiang&#39;s Personal Page</title>
    <link>https://faithio.cn/</link>
      <atom:link href="https://faithio.cn/index.xml" rel="self" type="application/rss+xml" />
    <description>Zixiang&#39;s Personal Page</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 19 Mar 2020 18:21:57 -0400</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Zixiang&#39;s Personal Page</title>
      <link>https://faithio.cn/</link>
    </image>
    
    <item>
      <title>Approximate Inference</title>
      <link>https://faithio.cn/post/approximate-inference/</link>
      <pubDate>Thu, 19 Mar 2020 18:21:57 -0400</pubDate>
      <guid>https://faithio.cn/post/approximate-inference/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;./Approximate.pdf&#34;&gt;Approximate Inference.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../fomula/Approximate.html&#34;&gt;Approximate Inference.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Exponential Family</title>
      <link>https://faithio.cn/post/the-exponential-family/</link>
      <pubDate>Thu, 19 Mar 2020 18:21:43 -0400</pubDate>
      <guid>https://faithio.cn/post/the-exponential-family/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;./Exponential.pdf&#34;&gt;Exponential.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../fomula/Exponential.html&#34;&gt;Exponential.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic Graphical Model</title>
      <link>https://faithio.cn/post/probabilistic-graphical-model/</link>
      <pubDate>Thu, 19 Mar 2020 17:56:38 -0400</pubDate>
      <guid>https://faithio.cn/post/probabilistic-graphical-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Run Issue With Matplotlib in Mac OS X</title>
      <link>https://faithio.cn/post/run-issue-with-matplotlib-in-mac-os-x/</link>
      <pubDate>Mon, 09 Mar 2020 13:37:09 -0400</pubDate>
      <guid>https://faithio.cn/post/run-issue-with-matplotlib-in-mac-os-x/</guid>
      <description>&lt;p&gt;When I run some python code from github, it occur the following problem as screenshot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gco76fc96qj31fo01paai.jpg&#34; alt=&#34;image-20200309133826149&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of &amp;lsquo;python&amp;rsquo; with &amp;lsquo;pythonw&amp;rsquo;. See &amp;lsquo;Working with Matplotlib on OSX&amp;rsquo; in the Matplotlib FAQ for more information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Solution:(&lt;a href=&#34;https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem Cause&lt;/strong&gt; In mac os image rendering back end of matplotlib (what-is-a-backend to render using the API of Cocoa by default). There is Qt4Agg and GTKAgg and as a back-end is not the default. Set the back end of macosx that is differ compare with other windows or linux os.&lt;/p&gt;

&lt;p&gt;I resolve this issue following ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I assume you have installed the pip matplotlib, there is a directory in you root called &lt;code&gt;~/.matplotlib&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Create a file &lt;code&gt;~/.matplotlib/matplotlibrc&lt;/code&gt; there and add the following code: &lt;code&gt;backend: TkAgg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From this &lt;a href=&#34;http://matplotlib.org/examples/index.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; you can try different diagram.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Expectation Propagation</title>
      <link>https://faithio.cn/post/stochastic-expectation-propagation/</link>
      <pubDate>Sat, 29 Feb 2020 11:34:28 -0500</pubDate>
      <guid>https://faithio.cn/post/stochastic-expectation-propagation/</guid>
      <description>

&lt;h1 id=&#34;math-formula&#34;&gt;Math Formula&lt;/h1&gt;

&lt;h3 id=&#34;factor-graphs&#34;&gt;Factor graphs&lt;/h3&gt;

&lt;p&gt;Shows how a function of several variables can be factored into a product of simpler functions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;$$ f(x,y,z) = (x+y) \cdot (y + z) \cdot (x  +z) $$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Very useful for representing posteriors.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ 
$$
P(x1, ..., x_n) = P(x_1) \Pi P( x_i | x_{i-1} )
$$
$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;$$ P(m|x1, &amp;hellip;, x_n) = P(m) \cdot \Pi P(x_i|m)$$&lt;/p&gt;

&lt;h4 id=&#34;modeling&#34;&gt;modeling&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;What graph should I use for this data?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;inference&#34;&gt;Inference&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Given the graph and data, what is the mean of x&lt;/li&gt;
&lt;li&gt;algorithm

&lt;ul&gt;
&lt;li&gt;Sampling&lt;/li&gt;
&lt;li&gt;Variable elimination&lt;/li&gt;
&lt;li&gt;Message-passing(Expectation Propagation, Variational Bayes)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;cutter-problem&#34;&gt;Cutter problem&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Want to estimate x given multiple y&amp;rsquo;s&lt;/li&gt;
&lt;li&gt;$$ p(x) = \mathcal{N}(x; 0, 100) $$&lt;/li&gt;
&lt;li&gt;$$ p(y_i|x) = (0.5)\mathcal{N}(y_i; x, 1) + (0.5)\mathcal{N} (y_i;0,10)$$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-&amp;gt; $ P(x|y1, &amp;hellip;, y_n) = P(x) \cdot \Pi P(y_i|x)$&lt;/p&gt;

&lt;p&gt;if we only have 2 points:&lt;/p&gt;

&lt;p&gt;$$ P(x) \cdot  P(y_1|x) \cdot P(y_2|x)  \rightarrow   p(y_i|x) = (0.5)\mathcal{N}(y_i; x, 1) + (0.5)\mathcal{N} (y_i;0,10)$$&lt;/p&gt;

&lt;p&gt;2 points have 4 Gaussians -&amp;gt; N points $$2^N$$ Gaussians&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gck12m03b5j30nf0f5diu.jpg&#34; alt=&#34;image-20200305220454120&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gck1kw7d6ij30mw0h5mz1.jpg&#34; alt=&#34;image-20200305222232412&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://pic4.zhimg.com/80/v2-1186de08c2ac2ef05b30cd88b5260707_720w.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/75617364&#34; target=&#34;_blank&#34;&gt;https://zhuanlan.zhihu.com/p/75617364&lt;/a&gt;
$$
p(z | w)=\frac{p(w | z) p(z)}{p(w)}=\frac{p(w | z) p(z)}{\int_{z} p(w | z) p(z) d z}
$$
Because it extends belief propagation. Belief propagation passes the entire distribution is the message. While EP will only pass onto the distribution certain expectation distribution allows you to you get a very compact message.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;expectation-propagation&#34;&gt;Expectation Propagation&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Fits an exponential-family approximation to the posterior.&lt;/li&gt;
&lt;li&gt;Belief propagation is a special case&lt;/li&gt;
&lt;li&gt;Kalman filtering is a special case&lt;/li&gt;
&lt;li&gt;Does not always converge.

&lt;ol&gt;
&lt;li&gt;May get stuck due to improper distributions&lt;/li&gt;
&lt;li&gt;May oscillate due to loopy graph&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;agm&#34;&gt;AGM&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;$
$$
p(\mathbf{X} | \Theta)=\sum_{j=1}^{M} p_{j} p\left(\mathbf{X} | \xi_{j}\right)
$$
$&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\xi_j$ is the set of the parameters of component j.&lt;/li&gt;
&lt;li&gt;$ p_j$ are the mixing proptions which must be positive and sum to one.&lt;/li&gt;
&lt;li&gt;$\Theta = {p_1, \ldots, p_M, \xi_1, \ldots, \xi_M}$ is the complete set of parameters fully characterizing the mixture.&lt;/li&gt;
&lt;li&gt;$ M \geq 1$ is number of components in the mixture.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code&gt;$
$$
p\left(\vec{X} | \theta_{j}\right)=\prod_{d=1}^{D} \sqrt{\frac{2}{\pi}} \frac{1}{\left(\sigma_{l_{j d}}+\sigma_{r_{j d}}\right)} \times\left\{\begin{array}{ll}\exp \left[-\frac{\left(X_{d}-\mu_{j d}\right)^{2}}{2 \sigma_{l_{j d} }^{2}}\right] &amp;amp; \text { if } X_{d}&amp;lt;\mu_{j d} \\ \exp \left[-\frac{\left(X_{d}-\mu_{j d}\right)^{2}}{2 \sigma_{r_{j d}}^{2}}\right] &amp;amp; \text { if } X_{d} \geq \mu_{j d}\end{array}\right.
$$
$&lt;/code&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;$\xi_{j}=\left(\vec{\mu}_{j}, \vec{\sigma}_{l_{j}}, \vec{\sigma}_{r_{j}}\right)$&lt;/code&gt; is the set of the parameters of components $j$&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\vec{\mu}_{j}=\left(\mu_{j 1}, \ldots, \mu_{j D}\right)$&lt;/code&gt; is the mean&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\vec{\sigma}_{l_{j}}=\left(\vec{\sigma}_{l_{j 1}}, \ldots, \vec{\sigma}_{l_{j D}}\right)$&lt;/code&gt; is the left standard deviation&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$\vec{\sigma}_{r_{j}}=\left(\vec{\sigma}_{r_{j 1}}, \ldots, \vec{\sigma}_{r_{j D}}\right)$&lt;/code&gt; is the right standard deviation&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;$$
p\left(\theta_j | \vec{X} \right)= \frac{p(\theta_j)\times p\left(\vec{X} | \theta&lt;em&gt;j\right)}{p(\vec{X})} = \frac{1}{p(\vec{X})} \prod&lt;/em&gt;{i} f_{i}(\boldsymbol{\theta})
$$&lt;/p&gt;

&lt;p&gt;$$
p(\vec{X})= \int \prod&lt;em&gt;{i} f&lt;/em&gt;{i}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$&lt;/p&gt;

&lt;p&gt;Here, $p(\vec{X})$ is very intractable to calculate and we don&amp;rsquo;t know $ f_{i}(\boldsymbol{\theta}) $.&lt;/p&gt;

&lt;p&gt;Now we consider using  &lt;strong&gt;EP&lt;/strong&gt;. The approximation, $q\left(\theta_j \right)$ , of the posterior,  $p\left( \theta_j | \vec{X} \right)$ , is assumed to have same functional form.
$$
q(\theta_j)=\frac{1}{Z} \prod_i \widetilde{f}_i(\theta_j)
$$&lt;/p&gt;

&lt;p&gt;in which each factor $\widetilde{f}_i(\theta_j)$ in the approximation corresponds to one of the factors
 $f_i(\theta_j)$ in the true posterior. $\widetilde{f}_i(\theta_j) $ is a asymetric Gaussian.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;$$
p(\mathbf{X} | \boldsymbol{\theta})=(1-w) \mathcal{A}(\mathbf{X} | \boldsymbol{\theta}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{X} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})
$$&lt;/p&gt;

&lt;p&gt;where w is the proportion of background clutter. And the prior over $\mathbf{\theta}$(mean) is taken to be Asymmetric Gaussian.&lt;/p&gt;

&lt;p&gt;And
$$
p(\boldsymbol{\theta})= \mathcal{A}(\mathbf{X} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I&lt;em&gt;r})
$$
$$
p(\mathcal{X}, \boldsymbol{\theta})=p(\boldsymbol{\theta}) \prod&lt;/em&gt;{n=1}^{N} p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right)
$$&lt;/p&gt;

&lt;h3 id=&#34;1-initialize-the-approximating-factors&#34;&gt;1. initialize the approximating factors&lt;/h3&gt;

&lt;p&gt;we select an approximating distribution from the exponential family to approximate the stochastic variables $\theta$
$$
q(\boldsymbol{\theta})=\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu}, \mathbf{\sigma_r^2}, \mathbf{\sigma_l^2}) =
\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu}, v_l \mathbf{I}, v_r \mathbf{I})&lt;/p&gt;

&lt;p&gt;= \mathcal{A}(\boldsymbol{\theta} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I_r})
$$&lt;/p&gt;

&lt;p&gt;$$
\widetilde{f}_{n}(\boldsymbol{\theta})=s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{\sigma&lt;/em&gt;{r&lt;em&gt;n}^2}, \mathbf{\sigma&lt;/em&gt;{l_n}^2} \right)
= s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)
$$&lt;/p&gt;

&lt;p&gt;$$
s&lt;em&gt;n = \prod&lt;/em&gt;{d=1}^{D} \sqrt{\frac{2}{\pi}} \frac{1}{\left(\sigma&lt;em&gt;{l&lt;/em&gt;{d}}+\sigma&lt;em&gt;{r&lt;/em&gt;{d}}\right)}
$$
While $\sigma_{l&lt;em&gt;n} \rightarrow \infty,
  \sigma&lt;/em&gt;{r_n} \rightarrow \infty
$ and $ \mu_n = 0 $.&lt;/p&gt;

&lt;p&gt;###2. initialize the posterior  approximation $q(\boldsymbol{\theta})$&lt;/p&gt;

&lt;p&gt;We chooses the parameter values a = 10, b = 100 and w = 0.5 and use $v$ denote $ \sigma^2$ as following, then $\mathbf{v_r} = \mathbf{v_l} = b = 100$&lt;/p&gt;

&lt;h3 id=&#34;3-until-all-mu-n-v-l-n-v-r-n-s-n-converge&#34;&gt;3. Until all $(\mu&lt;em&gt;n, v&lt;/em&gt;{l&lt;em&gt;n}, v&lt;/em&gt;{r_n}, s_n)$ converge:&lt;/h3&gt;

&lt;p&gt;$$
q^{\backslash n}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}_n(\boldsymbol{\theta})} = \frac{\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu}, \mathbf{v_r I}, \mathbf{v_l I})}{s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)} &lt;br /&gt;
\propto \left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;{\frac{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_l \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu})\right}}{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{l_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}}}  &amp;amp;&amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;{\frac{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_r \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu})\right}}{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{r_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}}}  &amp;amp;&amp;amp; \text { if } X&amp;gt;\mu&lt;/p&gt;

&lt;p&gt;\end{array}\right. \&lt;/p&gt;

&lt;p&gt;= \left{\begin{array}{ll}&lt;br /&gt;
\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_l \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu}) + \frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{l_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}   &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_r \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu}) +&lt;br /&gt;
\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{r_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n}) \right} &amp;amp; \text { if } X&amp;gt;\mu
\end{array}\right.
$$
- Remove the current estimate $\widetilde{f}_j(\boldsymbol{\theta})$ from $q(\theta)$, then we has mean and inverse variance given by:
$$
\left{\begin{array}{ll}
\left({v_l}^{\backslash n}\right)^{-1}={v_l}^{-1}-{v&lt;em&gt;l}&lt;/em&gt;{n}^{-1} &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\left({v_r}^{\backslash n}\right)^{-1}={v_r}^{-1}-{v&lt;em&gt;r}&lt;/em&gt;{n}^{-1} &amp;amp; \text { if } X&amp;gt;\mu &lt;br /&gt;
\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\mathbf{\mu}^{\backslash n}= \mathbf{\mu}+&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}
{v_l}^{\backslash n} {v&lt;em&gt;l}&lt;/em&gt;{n}^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_{n}\right) &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;{v_r}^{\backslash n} {v&lt;em&gt;r}&lt;/em&gt;{n}^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_{n}\right) &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$
&amp;gt; Cavity Distribution:
&amp;gt; $$
&amp;gt; q^{\backslash j}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}_{j}(\boldsymbol{\theta})}
&amp;gt; $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Recompute $(\mu, v, Z)$ from $(\mathbf{\mu}^{\backslash n}, {v_l}^{\backslash n}, {v&lt;em&gt;r}^{\backslash n})$
$$
Z&lt;/em&gt;{n}=(1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r}\right)
$$
&amp;gt;$$
&amp;gt;\begin{aligned}
&amp;gt;Z&lt;/em&gt;{n} &amp;amp;=\int q^{\backslash n}(\boldsymbol{\theta}) f&lt;em&gt;{n}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \
&amp;gt;&amp;amp;=\int q^{\backslash n}(\boldsymbol{\theta}) \widetilde{f}&lt;/em&gt;{n}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;
&amp;gt;&amp;amp;=\int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I}) \cdot { (1-w) \mathcal{A}(\mathbf{x_n} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{x_n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})} \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;
&amp;gt;&amp;amp;= (1-w)\int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I}) \mathcal{A}(\mathbf{x_n} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r}) \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;&amp;amp;+ w \int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I})
&amp;gt;\mathcal{A}(\mathbf{x_n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r})} \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;&amp;amp;=(1-w) \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)
&amp;gt;\end{aligned}
&amp;gt;$$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;we  assumed that  $f&lt;em&gt;{0}(\boldsymbol{\theta})=p(\boldsymbol{\theta})$  and $
f&lt;/em&gt;{n}(\boldsymbol{\theta})=p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) = (1-w) \mathcal{A}(\mathbf{X} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{X} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})
$, also $q(\boldsymbol{\theta})=\mathcal{A}(\boldsymbol{\theta} | \mathbf{m}, v_l \mathbf{I}, v&lt;em&gt;r \mathbf{I}) $
$$
\begin{aligned}
\rho&lt;/em&gt;{n} &amp;amp;=\frac{1}{Z&lt;em&gt;{n}}(1-w) \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)  &lt;br /&gt;
&amp;amp;= \frac{1}{Z&lt;/em&gt;{n}}(1-w)\cdot \frac{Z&lt;em&gt;n - w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)}{1-w} &lt;br /&gt;
&amp;amp;= 1 - \frac{w}{Z&lt;em&gt;n} \cdot \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r}\right)
\end{aligned}
$$
Basic rule for Asymmetric Gaussian:
$$
\nabla&lt;/em&gt;{\boldsymbol{\mu}} \mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r})=&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r}) \cdot(\mathbf{x}-\boldsymbol{\mu}) \mathbf{v_l}^{-1}  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r}) \cdot(\mathbf{x}-\boldsymbol{\mu}) \mathbf{v_r}^{-1}  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$
So we compute the mean and variance:
$$
\mathbf{\mu^{new}}=\mathbf{\mu}^{\backslash n}+&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\rho_{n} \frac{v_l^{\backslash n}}{v&lt;em&gt;l^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\rho_{n} \frac{v_r^{\backslash n}}{v&lt;em&gt;r^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\left{\begin{array}{ll}
v_l^{new}=v&lt;em&gt;l^{\backslash n}-\rho&lt;/em&gt;{n} \frac{\left(v_l^{\backslash n}\right)^{2}}{v&lt;em&gt;l^{\backslash n}+1}+\rho&lt;/em&gt;{n}\left(1-\rho_{n}\right) \frac{\left(v&lt;em&gt;l^{\backslash n}\right)^{2}\left|\mathbf{x}&lt;/em&gt;{n}-\mathbf{m}^{\backslash n}\right|^{2}}{D\left(v_l^{\backslash n}+1\right)^{2}} &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;v_r^{new}=v&lt;em&gt;r^{\backslash n}-\rho&lt;/em&gt;{n} \frac{\left(v_r^{\backslash n}\right)^{2}}{v&lt;em&gt;r^{\backslash n}+1}+\rho&lt;/em&gt;{n}\left(1-\rho_{n}\right) \frac{\left(v&lt;em&gt;r^{\backslash n}\right)^{2}\left|\mathbf{x}&lt;/em&gt;{n}-\mathbf{m}^{\backslash n}\right|^{2}}{D\left(v_r^{\backslash n}+1\right)^{2}}
&amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$
- Evaluate and store the new factor
$$
\left{\begin{array}{ll}
\left({v_{l_n}}\right)^{-1}={(v_l^{new})}^{-1}-({v&lt;em&gt;l}^{\backslash n})^{-1} &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\left({v&lt;/em&gt;{r_n}}\right)^{-1}={(v_r^{new})}^{-1}-({v_r}^{ \backslash n})^{-1} &amp;amp; \text { if } X&amp;gt;\mu &lt;br /&gt;
\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\mathbf{m}_{n}=\mathbf{m}^{\backslash n}+
\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\left(v_{n}+v^{\backslash n}\right)\left(v^{\backslash n}\right)^{-1}\left(\mathbf{m}^{\mathrm{new}}-\mathbf{m}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\left(v_{n}+v^{\backslash n}\right)\left(v^{\backslash n}\right)^{-1}\left(\mathbf{m}^{\mathrm{new}}-\mathbf{m}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
&amp;amp; \widetilde{f}&lt;em&gt;{n}(\boldsymbol{\theta})=Z&lt;/em&gt;{n} \frac{q^{\mathrm{new}}(\boldsymbol{\theta})}{q^{\backslash n}(\boldsymbol{\theta})} \&lt;/p&gt;

&lt;p&gt;\Rightarrow &amp;amp; Z_n q^{\mathrm{new}}(\boldsymbol{\theta}) = s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right) q^{\backslash n}(\boldsymbol{\theta}) =
s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)
\mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) \&lt;/p&gt;

&lt;p&gt;\Rightarrow &amp;amp; \int Z_n q^{\mathrm{new}}(\boldsymbol{\theta}) d\theta =
\int s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)
\mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) d \theta \&lt;/p&gt;

&lt;p&gt;\Rightarrow &amp;amp; Z_n = s_n \int q^{\mathrm{new}}(\boldsymbol{\theta}) d\theta =&lt;/p&gt;

&lt;p&gt;\int s_n \mathcal{A}\left( \mathbf{\mu}&lt;em&gt;n - \boldsymbol{\theta} | 0, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right) \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) d \theta \&lt;/p&gt;

&lt;p&gt;\Rightarrow &amp;amp; Z_n  = s_n \mathcal{A}\left(\mathbf{\mu}_n | \mathbf{\mu}^{\backslash n}, \mathbf{(v&lt;em&gt;r^{\backslash n}+ v&lt;/em&gt;{r_n}) I}, \mathbf{(v&lt;em&gt;l^{\backslash n}+v&lt;/em&gt;{r_n}) I} \right)
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;$$
s_n  = \frac{Z_n} {\mathcal{A}\left(\mathbf{\mu}_n | \mathbf{\mu}^{\backslash n}, \mathbf{(v&lt;em&gt;r^{\backslash n}+ v&lt;/em&gt;{r_n}) I}, \mathbf{(v&lt;em&gt;l^{\backslash n}+v&lt;/em&gt;{r_n}) I} \right)}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logistic Regression   Solvers&#39; Defintions in Sklearn</title>
      <link>https://faithio.cn/post/logistic-regression-solvers-defintions-in-sklearn/</link>
      <pubDate>Sat, 29 Feb 2020 02:46:04 -0500</pubDate>
      <guid>https://faithio.cn/post/logistic-regression-solvers-defintions-in-sklearn/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;A hypothesis &lt;code&gt;h(x)&lt;/code&gt;, takes an &lt;em&gt;input&lt;/em&gt; and gives us the &lt;em&gt;estimated output value&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This hypothesis can be a as simple as a one variable linear equation, .. up to a very complicated and long multivariate equation with respect to the type of the algorithm we’re using (&lt;em&gt;i.e. linear regression, logistic regression..etc&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9e4pcalj308k05xgls.jpg&#34; alt=&#34;h(x)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our task is to find the &lt;strong&gt;best Parameters&lt;/strong&gt; (a.k.a Thetas or Weights) that give us the &lt;strong&gt;least error&lt;/strong&gt; in predicting the output. We call this error a &lt;strong&gt;Cost or Loss Function&lt;/strong&gt; and apparently our goal is to &lt;strong&gt;minimize&lt;/strong&gt; it in order to get the best predicted output!&lt;/p&gt;

&lt;p&gt;One more thing to recall, that the relation between the parameter value and its effect on the cost function (i.e. the error) looks like a &lt;strong&gt;bell curve&lt;/strong&gt; (i.e. &lt;strong&gt;Quadratic&lt;/strong&gt;; recall this because it’s very important) .&lt;/p&gt;

&lt;p&gt;So if we start at any point in that curve and if we keep taking the derivative (i.e. tangent line) of each point we stop at, we will end up at what so called the &lt;strong&gt;Global Optima&lt;/strong&gt; as shown in this image:
&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9empzi9j30bm07twem.jpg&#34; alt=&#34;J(w) bell curve&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If we take the partial derivative at minimum cost point (i.e. global optima) we find the &lt;strong&gt;slope&lt;/strong&gt; of the tangent line = &lt;strong&gt;0&lt;/strong&gt; (then we know that we reached our target).&lt;/p&gt;

&lt;p&gt;That’s valid only if we have &lt;em&gt;Convex&lt;/em&gt; Cost Function, but if we don’t, we may end up stuck at what so called &lt;strong&gt;Local Optima&lt;/strong&gt;; consider this non-convex function:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9ffxzczj30ac07qwex.jpg&#34; alt=&#34;non-convex&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now you should have the intuition about the hack relationship between what we are doing and the terms: &lt;em&gt;Deravative&lt;/em&gt;, &lt;em&gt;Tangent Line&lt;/em&gt;, &lt;em&gt;Cost Function&lt;/em&gt;, &lt;em&gt;Hypothesis&lt;/em&gt; ..etc.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Side Note: The above mentioned intuition also related to the Gradient Descent Algorithm (see later).&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Linear Approximation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given a function, &lt;code&gt;f(x)&lt;/code&gt;, we can find its tangent at &lt;code&gt;x=a&lt;/code&gt;. The equation of the tangent line L(x) is: &lt;code&gt;L(x)=f(a)+f′(a)(x−a)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Take a look at the following graph of a function and its tangent line:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9dc2jkaj309r05tq2w.jpg&#34; alt=&#34;tangent line&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From this graph we can see that near &lt;code&gt;x=a&lt;/code&gt;, the tangent line and the function have nearly the same graph. On occasion we will use the tangent line, &lt;code&gt;L(x)&lt;/code&gt;, as an approximation to the function, &lt;code&gt;f(x)&lt;/code&gt;, near &lt;code&gt;x=a&lt;/code&gt;. In these cases we call the tangent line the linear approximation to the function at &lt;code&gt;x=a&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quadratic Approximation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Same like linear approximation but this time we are dealing with a curve but we &lt;strong&gt;cannot&lt;/strong&gt; find the point near to &lt;strong&gt;0&lt;/strong&gt; by using the tangent line.&lt;/p&gt;

&lt;p&gt;Instead, we use a &lt;strong&gt;parabola&lt;/strong&gt; (&lt;em&gt;which is a curve where any point is at an equal distance from a fixed point or a fixed straight line&lt;/em&gt;), like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9dhnyppj30ay075mxe.jpg&#34; alt=&#34;quadratic function&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And in order to fit a good parabola, both parabola and quadratic function should have same value, same first derivative, AND second derivative, &amp;hellip; the formula will be (&lt;em&gt;just out of curiosity&lt;/em&gt;): &lt;code&gt;Qa(x) = f(a) + f&#39;(a)(x-a) + f&#39;&#39;(a)(x-a)2/2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Now we should be ready to do the comparison in details.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;comparison-between-the-methods&#34;&gt;Comparison between the methods&lt;/h2&gt;

&lt;h3 id=&#34;1-newton-s-method-newton-cg&#34;&gt;&lt;strong&gt;1. Newton’s Method(newton-cg):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Recall the motivation for gradient descent step at x: we minimize the quadratic function (i.e. Cost Function).&lt;/p&gt;

&lt;p&gt;Newton’s method uses in a sense a &lt;strong&gt;better&lt;/strong&gt; quadratic function minimisation. A better because it uses the quadratic approximation (i.e. first AND &lt;em&gt;second&lt;/em&gt; partial derivatives).&lt;/p&gt;

&lt;p&gt;You can imagine it as a twisted Gradient Descent with The Hessian (&lt;em&gt;The Hessian is a square matrix of second-order partial derivatives of order nxn&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Moreover, the geometric interpretation of Newton&amp;rsquo;s method is that at each iteration one approximates &lt;code&gt;f(x)&lt;/code&gt; by a quadratic function around &lt;code&gt;xn&lt;/code&gt;, and then takes a step towards the maximum/minimum of that quadratic function (in higher dimensions, this may also be a saddle point). Note that if &lt;code&gt;f(x)&lt;/code&gt; happens to be a quadratic function, then the exact extremum is found in one step.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It’s computationally &lt;strong&gt;expensive&lt;/strong&gt; because of The Hessian Matrix (i.e. second partial derivatives calculations).&lt;/li&gt;
&lt;li&gt;It attracts to &lt;strong&gt;Saddle Points&lt;/strong&gt; which are common in multivariable optimization (i.e. a point its partial derivatives disagree over whether this input should be a maximum or a minimum point!).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;2-limited-memory-broyden-fletcher-goldfarb-shanno-algorithm-lbfgs&#34;&gt;&lt;strong&gt;2. Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm(lbfgs):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In a nutshell, it is analogue of the Newton’s Method but here the Hessian matrix is &lt;strong&gt;approximated&lt;/strong&gt; using updates specified by gradient evaluations (or approximate gradient evaluations). In other words, using an estimation to the inverse Hessian matrix.&lt;/p&gt;

&lt;p&gt;The term Limited-memory simply means it stores only a few vectors that represent the approximation implicitly.&lt;/p&gt;

&lt;p&gt;If I dare say that when dataset is &lt;strong&gt;small&lt;/strong&gt;, L-BFGS relatively performs the best compared to other methods especially it saves a lot of memory, however there are some “*serious*” drawbacks such that if it is unsafeguarded, it may not converge to anything.&lt;/p&gt;

&lt;h3 id=&#34;3-a-library-for-large-linear-classification-liblinear&#34;&gt;&lt;strong&gt;3. A Library for Large Linear Classification(liblinear):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;It’s a linear classification that supports logistic regression and linear support vector machines (&lt;em&gt;A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics i.e feature value&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;The solver uses a coordinate descent (CD) algorithm that solves optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;LIBLINEAR&lt;/code&gt; is the winner of ICML 2008 large-scale learning challenge. It applies &lt;em&gt;Automatic parameter selection&lt;/em&gt; (a.k.a L1 Regularization) and it’s recommended when you have high dimension dataset (&lt;em&gt;recommended for solving large-scale classification problems&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It may get stuck at a &lt;em&gt;non-stationary point&lt;/em&gt; (i.e. non-optima) if the level curves of a function are not smooth.&lt;/li&gt;
&lt;li&gt;Also cannot run in parallel.&lt;/li&gt;
&lt;li&gt;It cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a “one-vs-rest” fashion so separate binary classifiers are trained for all classes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Side note: According to Scikit Documentation: The “liblinear” solver is used by default for historical reasons.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;4-stochastic-average-gradient-sag&#34;&gt;&lt;strong&gt;4. Stochastic Average Gradient(sag):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;SAG method optimizes the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method&amp;rsquo;s iteration cost is independent of the number of terms in the sum. However, by &lt;strong&gt;incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate&lt;/strong&gt; than black-box SG methods.&lt;/p&gt;

&lt;p&gt;It is &lt;strong&gt;faster&lt;/strong&gt; than other solvers for &lt;em&gt;large&lt;/em&gt; datasets, when both the number of samples and the number of features are large.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It only supports L2 penalization.&lt;/li&gt;
&lt;li&gt;Its memory cost of &lt;code&gt;O(N)&lt;/code&gt;, which can make it impractical for large N (&lt;em&gt;because it remembers the most recently computed values for approx. all gradients&lt;/em&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;5-saga-saga&#34;&gt;&lt;strong&gt;5. SAGA(saga):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The SAGA solver is a &lt;em&gt;variant&lt;/em&gt; of SAG that also supports the non-smooth &lt;em&gt;penalty=l1&lt;/em&gt; option (i.e. L1 Regularization). This is therefore the solver of choice for &lt;strong&gt;sparse&lt;/strong&gt; multinomial logistic regression and it’s also suitable &lt;strong&gt;very Large&lt;/strong&gt; dataset.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Side note: According to Scikit Documentation: The SAGA solver is often the best choice.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The following table is taken from &lt;a href=&#34;http://scikit-learn.org/stable/modules/linear_model.html&#34; target=&#34;_blank&#34;&gt;Scikit Documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9g3deuhj30ik04ejs7.jpg&#34; alt=&#34;Solver Comparison&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
