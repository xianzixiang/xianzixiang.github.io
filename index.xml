<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zixiang&#39;s Personal Page</title>
    <link>https://faithio.cn/</link>
      <atom:link href="https://faithio.cn/index.xml" rel="self" type="application/rss+xml" />
    <description>Zixiang&#39;s Personal Page</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sat, 25 Apr 2020 17:27:27 -0400</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Zixiang&#39;s Personal Page</title>
      <link>https://faithio.cn/</link>
    </image>
    
    <item>
      <title>Semi Supervised Learning</title>
      <link>https://faithio.cn/post/semi-supervised-learning/</link>
      <pubDate>Sat, 25 Apr 2020 17:27:27 -0400</pubDate>
      <guid>https://faithio.cn/post/semi-supervised-learning/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;./image-20200425172854388.png&#34; alt=&#34;image-20200425172854388&#34; /&gt;&lt;/p&gt;

&lt;p&gt;éœ€è¦åŸºäºå‡è®¾ï¼Œè€ƒè™‘ä½ çš„å‡è®¾åˆä¸åˆç†&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425173010036.png&#34; alt=&#34;image-20200425173010036&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;generative-model&#34;&gt;Generative Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425173140737.png&#34; alt=&#34;image-20200425173140737&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425173309028.png&#34; alt=&#34;image-20200425173309028&#34; /&gt;&lt;/p&gt;

&lt;p&gt;åˆå§‹åŒ–ä¸€ç³»åˆ—å‚æ•°ï¼Œå¯ä»¥randomä¹Ÿå¯ä»¥ä»å·²ç»è®­ç»ƒçš„æ•°æ®å¾—æ¥ã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425173406351.png&#34; alt=&#34;image-20200425173406351&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ¯æ¬¡å¢åŠ loglikelihood ç»“æŸåœ¨local minimum&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425173938442.png&#34; alt=&#34;image-20200425173938442&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;low-density-separation-assumption&#34;&gt;Low-density Separation Assumption&lt;/h3&gt;

&lt;p&gt;é€šè¿‡ä¸€å®šç­–ç•¥å°†pseudo-label data å¢åŠ åˆ°train setï¼Œä¹Ÿå¯ä»¥ç»™ä¸€å®šweightï¼Œconfidentæœ‰é«˜weight&lt;/p&gt;

&lt;p&gt;outputä¸€ä¸ªæ•°å­—å…¶å®ä¸ä¼šå½±å“ç»“æœ&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./Users/faith/academic-zixiang/content/en/post/semi-supervised learning/image-20200425174515783.png&#34; alt=&#34;image-20200425174515783&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;å¯¹äºç¥ç»ç½‘ç»œå“ªä¸ªæœ‰ç”¨&#34;&gt;å¯¹äºç¥ç»ç½‘ç»œå“ªä¸ªæœ‰ç”¨&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425174909139.png&#34; alt=&#34;image-20200425174909139&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;entropy-based-regularization&#34;&gt;Entropy-based Regularization&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425175155110.png&#34; alt=&#34;image-20200425175155110&#34; /&gt;&lt;/p&gt;

&lt;p&gt;entropyè¶Šå°è¶Šå¥½ï¼Œ labelled data cross entropyè¶Šå°è¶Šå¥½ï¼Œä¹Ÿå¯ä»¥åŠ ä¸Šweight&lt;/p&gt;

&lt;p&gt;çœ‹å€¾å‘äºå“ªè¾¹&lt;/p&gt;

&lt;h3 id=&#34;outlook-semi-supervised-svm&#34;&gt;Outlook: Semi-supervised SVM&lt;/h3&gt;

&lt;p&gt;ç©·ä¸¾æ‰€æœ‰unlabeled dataã€‚æ”¹ä¸€äº›label å¦‚æœobjective functionå¤§ï¼Œé‚£å°±æ”¹&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425175551180.png&#34; alt=&#34;image-20200425175551180&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;smoothness-assumption&#34;&gt;Smoothness Assumption&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;è¿‘æœ±è€…èµ¤ï¼Œè¿‘å¢¨è€…é»‘
â€œYou are known by the company you keepâ€&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425175912387.png&#34; alt=&#34;image-20200425175912387&#34; /&gt;&lt;/p&gt;

&lt;p&gt;x1 å’Œ x2 ä¹‹é—´æœ‰high density&lt;/p&gt;

&lt;p&gt;æ–‡æœ¬åˆ†ç±»&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425180246636.png&#34; alt=&#34;image-20200425180246636&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;cluster-and-then-label&#34;&gt;Cluster and then Label&lt;/h4&gt;

&lt;p&gt;åœ¨image ä¸Šå¯èƒ½ä¸ä¼šworkï¼Œå› ä¸ºpixelå¯èƒ½å·®ä¸å¤šï¼Œä½†æ˜¯å¹¶ä¸åƒ&lt;/p&gt;

&lt;p&gt;äººçš„å·¦å³ä¾§é¢/æ‰‹å†™&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425180337365.png&#34; alt=&#34;image-20200425180337365&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Deep Autoencoder call feature, call clustering.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;graph-based-approach&#34;&gt;Graph-based Approach&lt;/h4&gt;

&lt;p&gt;ç½‘é¡µçš„hyperlink/è®ºæ–‡çš„å¼•ç”¨å¯ä»¥åšåˆ†ç±»&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425180707408.png&#34; alt=&#34;image-20200425180707408&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425180950009.png&#34; alt=&#34;image-20200425180950009&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å› ä¸ºæœ‰å–expï¼Œæ‰€ä»¥ä¸‹é™å¾ˆå¿«ï¼Œsingularityæ‰ä¼šå¤§ã€‚éœ€è¦æœ‰è¿™æ ·çš„æœºåˆ¶æ‰ä¸ä¼šè¿åˆ°è·¨æµ·æ²Ÿçš„link&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425181148486.png&#34; alt=&#34;image-20200425181148486&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;data éœ€è¦å¤Ÿå¤šï¼Œä¸ç„¶ä¼šæ–­æ‰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425181421207.png&#34; alt=&#34;image-20200425181421207&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ä¸¤ä¸¤æ‹¿å‡ºæ¥ï¼Œä¹˜ä»¥weights&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425181729818.png&#34; alt=&#34;image-20200425181729818&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425181922038.png&#34; alt=&#34;image-20200425181922038&#34; /&gt;&lt;/p&gt;

&lt;p&gt;è®©ç¥ç»ç½‘ç»œçš„labeled data å’ŒçœŸæ­£çš„label è¶Šæ¥è¿‘è¶Šå¥½ï¼Œè¿˜è¦ç¬¦åˆsmoothness assumptionçš„å‡è®¾ã€‚åšgradient descent&lt;/p&gt;

&lt;h3 id=&#34;better-representation&#34;&gt;Better Representation&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Ensemble Methods</title>
      <link>https://faithio.cn/post/ensemble-methods/</link>
      <pubDate>Sat, 25 Apr 2020 15:27:05 -0400</pubDate>
      <guid>https://faithio.cn/post/ensemble-methods/</guid>
      <description>

&lt;p&gt;è®²è®²Ensemble Methodsçš„Boosting/Bagging/Stacking&lt;/p&gt;

&lt;p&gt;åœ¨Kaggleæ¯”èµ›æœ€åéƒ½ä¼šç”¨åˆ°Ensemble Methodsæ¥æé«˜performanceã€‚&lt;/p&gt;

&lt;p&gt;ç®€å•æ¥è¯´å°±æ˜¯ï¼šEnsemble of various models into one more effective model&lt;/p&gt;

&lt;p&gt;ä¸€èˆ¬æ¥è¯´model &amp;rsquo;s learning error æ¥è‡ªäº variance, noise, and bias.&lt;/p&gt;

&lt;p&gt;ensembleçš„å‡ºç°å°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸‰ä¸ªé—®é¢˜ï¼Œè®©model æ›´åŠ robustã€‚&lt;/p&gt;

&lt;h3 id=&#34;bagging-to-decrease-the-model-s-variance&#34;&gt;&lt;strong&gt;Bagging&lt;/strong&gt; to decrease the modelâ€™s variance;&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;ä¸»è¦è§£å†³overfitingé—®é¢˜ï¼Œè®©error surfaceæ›´åŠ å¹³ç¼“ã€‚&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;å¯ä»¥å¹¶è¡Œè¿›è¡Œã€‚&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;é‡‡å–voting æˆ–è€…averageæ–¹æ³•é€‰å–modelï¼Œæ‰€ä»¥weightæ˜¯ç›¸åŒçš„&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ä¸€èˆ¬é‡‡ç”¨subsampleæ–¹æ³•ï¼Œåˆ†ä¸ºbootstraping and aggregatingã€‚ä½†æ˜¯åœ¨sampleçš„è¿‡ç¨‹ä¸­æ ·æœ¬æ˜¯æœ‰æ”¾å›çš„&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/960/1*DFHUbdz6EyOuMYP4pDnFlw.jpeg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Get some classifiers to evaluate
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import RidgeClassifier
from sklearn.svm import SVC
seed = 1075
np.random.seed(seed)
# Create classifiers
rf = RandomForestClassifier()
et = ExtraTreesClassifier()
knn = KNeighborsClassifier()
svc = SVC()
rg = RidgeClassifier()
clf_array = [rf, et, knn, svc, rg]
for clf in clf_array:
    vanilla_scores = cross_val_score(clf, X, y, cv=10, n_jobs=-1)
    bagging_clf = BaggingClassifier(clf, 
       max_samples=0.4, max_features=10, random_state=seed)
    bagging_scores = cross_val_score(bagging_clf, X, y, cv=10, 
       n_jobs=-1)
    
    print &amp;quot;Mean of: {1:.3f}, std: (+/-) {2:.3f [{0}]&amp;quot;  
                       .format(clf.__class__.__name__, 
                       vanilla_scores.mean(), vanilla_scores.std())
    print &amp;quot;Mean of: {1:.3f}, std: (+/-) {2:.3f} [Bagging {0}]\n&amp;quot;
                       .format(clf.__class__.__name__, 
                        bagging_scores.mean(), bagging_scores.std())
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;Mean of: 0.632, std: (+/-) 0.081 [RandomForestClassifier]
Mean of: 0.639, std: (+/-) 0.069 [Bagging RandomForestClassifier]

Mean of: 0.636, std: (+/-) 0.080 [ExtraTreesClassifier]
Mean of: 0.654, std: (+/-) 0.073 [Bagging ExtraTreesClassifier]

Mean of: 0.500, std: (+/-) 0.086 [KNeighborsClassifier]
Mean of: 0.535, std: (+/-) 0.111 [Bagging KNeighborsClassifier]

Mean of: 0.465, std: (+/-) 0.085 [SVC]
Mean of: 0.535, std: (+/-) 0.083 [Bagging SVC]

Mean of: 0.639, std: (+/-) 0.050 [RidgeClassifier]
Mean of: 0.597, std: (+/-) 0.045 [Bagging RidgeClassifier]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;åœ¨åšcross validation éªŒè¯çš„æ—¶å€™ï¼Œåªæœ‰ridge classifierçš„å‡†ç¡®ç‡ä¸‹é™äº†ï¼Œå…¶ä»–çš„æ–¹æ³•éƒ½èƒ½æé«˜accuracyã€‚&lt;/p&gt;

&lt;p&gt;é‚£ç°åœ¨bagged classifieréƒ½æ¯”åŸæ¥å¥½äº†ï¼Œæˆ‘ä»¬é€‰æ‹©å“ªä¸ªå‘¢ï¼Ÿ&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Voting&lt;/strong&gt;ï¼Œå¯ä»¥åšhard votingæˆ–è€…soft votingï¼ˆvote by weightï¼‰ã€&lt;/p&gt;

&lt;p&gt;ä»¥ä¸‹æ˜¯hard voting&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/1820/1*tE0yuFerQPDCMzhobqv1Dw.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å®é™…æƒ…å†µï¼Œweightæ˜¯æ¯”è¾ƒéš¾è°ƒæ•´çš„ã€‚&lt;/p&gt;

&lt;h3 id=&#34;boosting-to-decreasing-the-model-s-bias&#34;&gt;&lt;strong&gt;Boosting&lt;/strong&gt; to decreasing the modelâ€™s bias&lt;/h3&gt;

&lt;p&gt;é›†åˆweak classifierï¼Œé™ä½error rateã€‚ç›¸å½“äºsequentialï¼Œåªèƒ½ä¸€ä¸ªä¸ªè¿›è¡Œã€‚&lt;/p&gt;

&lt;p&gt;ä»£è¡¨modelï¼šAdaboost&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;find a weak classifer with less then 50% accuracy.&lt;/li&gt;
&lt;li&gt;æƒ©ç½šåˆ†ç±»ç»“æœï¼Œé”™è¯¯çš„å¢åŠ weightï¼Œæ­£ç¡®çš„é™ä½weight&lt;/li&gt;
&lt;li&gt;ç»§ç»­train&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6n8rh5ysj30rh0judt5.jpg&#34; alt=&#34;image-20200425155448039&#34; /&gt;&lt;/p&gt;

&lt;p&gt;åˆå¯ä»¥æ¼”åŒ–ä¸ºGradient Boosting&lt;/p&gt;

&lt;h3 id=&#34;stacking-to-increasing-the-predictive-force-of-the-classifier&#34;&gt;Stacking to increasing the predictive force of the classifier.&lt;/h3&gt;

&lt;p&gt;stacking ä¸»è¦å°±æ˜¯å°†å¤šä¸ªè®­ç»ƒå¥½çš„æ¯”è¾ƒè´Ÿè´£æ¨¡å‹çš„output ä½œä¸ºLogistics Regression1çš„inputã€‚&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;å…¶å®ä»”ç»†æ€è€ƒä¸‹æœ‰ç‚¹åƒç¥ç»ç½‘ç»œçš„fine tunedã€‚&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6msmnen7j30ql0ilti5.jpg&#34; alt=&#34;image-20200425153917987&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://miro.medium.com/max/946/0*GHYCJIjkkrP5ZgPh.png&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;p&gt;åœ¨Kaggleä¸­æœ€å¹¿æ³›è¢«ä½¿ç”¨å°±æ˜¯stacking&lt;/p&gt;

&lt;p&gt;å› ä¸ºå¯ä»¥å¹¶è¡Œè®­ç»ƒï¼Œå¤šä¸ªä¸åŒmodelä¹‹é—´weightè®¾ç½®å¥½å¯ä»¥æ¶ˆé™¤variance&lt;/p&gt;

&lt;p&gt;ä¸‹é¢æ˜¯ä¸€ä¸ªæ¯”èµ›ç¬¬ä¸‰åé‡‡ç”¨æ–¹æ³•&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;My final submission is ensemble of resnet34 x 5, inception-v3 and se-resnext50&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Model&lt;/th&gt;
&lt;th&gt;GPUs&lt;/th&gt;
&lt;th&gt;Image size&lt;/th&gt;
&lt;th&gt;Training Epochs&lt;/th&gt;
&lt;th&gt;Training Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;resnet34&lt;/td&gt;
&lt;td&gt;1x TitanX&lt;/td&gt;
&lt;td&gt;512&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;16 hours&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;inception-v3&lt;/td&gt;
&lt;td&gt;3x TitanX&lt;/td&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;1day 15 hours&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;se-resnext50&lt;/td&gt;
&lt;td&gt;2x TitanX&lt;/td&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;2days 15 hours&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;ä»€ä¹ˆæ˜¯tta&#34;&gt;ä»€ä¹ˆæ˜¯TTA&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Test Time Augmentation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;citedï¼š&lt;a href=&#34;https://towardsdatascience.com/augmentation-for-image-classification-24ffcbc38833&#34; target=&#34;_blank&#34;&gt;https://towardsdatascience.com/augmentation-for-image-classification-24ffcbc38833&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;å› ä¸ºå¯¹æ¯”åº¦ï¼Œæˆªå–ç­‰åŸå› å¯èƒ½misclassifiedï¼Œå¯ä»¥ä½¿ç”¨TTA&lt;/p&gt;

&lt;p&gt;To mitigate errors such as these we use TTA wherein we predict class for the original test image along with 4 random tranforms of the same image. We then take an average of the predictions to determine which class the image belongs to.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;The intuition behind this is that even if the test image is not too easy to make a prediction, the transformations change it such that the model has higher chances of capturing the dog/cat shape and predicting accordingly.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ç®€å•æ¥è¯´å°±æ˜¯åŒä¸€ç§imageï¼Œé‡‡ç”¨ä¸åŒtransformï¼Œç„¶åå–average predictionã€‚å½“åŒä¸€ä¸ªå›¾ç‰‡æœ‰ä¸åŒçš„å˜æ¢ï¼Œmodelæ›´å®¹æ˜“learnåˆ°featureã€‚&lt;/p&gt;

&lt;p&gt;å½“ç„¶ç°åœ¨GAN èƒ½ç”Ÿæˆæ›´å¤šæ›´å¤æ‚çš„å›¾ç‰‡äº†å‚è€ƒï¼š&lt;a href=&#34;https://arxiv.org/pdf/1712.04621.pdf&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/pdf/1712.04621.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CrossEntropy vs KL Divergence</title>
      <link>https://faithio.cn/post/crossentropy-vs-kl-divergence/</link>
      <pubDate>Sat, 25 Apr 2020 13:41:37 -0400</pubDate>
      <guid>https://faithio.cn/post/crossentropy-vs-kl-divergence/</guid>
      <description>&lt;p&gt;Both of Cross-entropy and KL divergence are tools to measure the distance between two probability distribution. What is the difference?&lt;/p&gt;

&lt;p&gt;ğ»(ğ‘ƒ,ğ‘„)=âˆ’âˆ‘ğ‘¥ğ‘ƒ(ğ‘¥)logğ‘„(ğ‘¥)H(P,Q)=âˆ’âˆ‘xP(x)logâ¡Q(x)&lt;/p&gt;

&lt;p&gt;ğ¾ğ¿(ğ‘ƒ|ğ‘„)=âˆ‘ğ‘¥ğ‘ƒ(ğ‘¥)logğ‘ƒ(ğ‘¥)ğ‘„(ğ‘¥)KL(P|Q)=âˆ‘xP(x)logâ¡P(x)Q(x)&lt;/p&gt;

&lt;p&gt;Moreover, minimization of KL is equivalent to minimization of Cross-Entropy.&lt;/p&gt;

&lt;p&gt;I want to know them instinctively.&lt;/p&gt;

&lt;p&gt;You will need some conditions to claim the equivalence between minimizing cross entropy and minimizing KL divergence. I will put your question under the context of classification problems using cross entropy as loss functions.&lt;/p&gt;

&lt;p&gt;Let us first recall that entropy is used to measure the uncertainty of a system, which is defined as&lt;/p&gt;

&lt;p&gt;ğ‘†(ğ‘£)=âˆ’âˆ‘ğ‘–ğ‘(ğ‘£ğ‘–)logğ‘(ğ‘£ğ‘–),S(v)=âˆ’âˆ‘ip(vi)logâ¡p(vi),&lt;/p&gt;

&lt;p&gt;for&lt;/p&gt;

&lt;p&gt;For instance, the event A &lt;code&gt;I will die eventually&lt;/code&gt; is almost certain (maybe we can solve the aging problem for word &lt;code&gt;almost&lt;/code&gt;), therefore it has low entropy which requires only the information of &lt;code&gt;the aging problem cannot be solved&lt;/code&gt; to make it certain. However, the event B &lt;code&gt;The president will die in 50 years&lt;/code&gt; is much more uncertain than A, thus it needs more information to remove the uncertainties.&lt;/p&gt;

&lt;p&gt;Now look at the definition of KL divergence between events A and B&lt;/p&gt;

&lt;p&gt;ğ·ğ¾ğ¿(ğ´âˆ¥ğµ)=âˆ‘ğ‘–ğ‘ğ´(ğ‘£ğ‘–)logğ‘ğ´(ğ‘£ğ‘–)âˆ’ğ‘ğ´(ğ‘£ğ‘–)logğ‘ğµ(ğ‘£ğ‘–),DKL(Aâˆ¥B)=âˆ‘ipA(vi)logâ¡pA(vi)âˆ’pA(vi)logâ¡pB(vi),&lt;/p&gt;

&lt;p&gt;where the first term of the right hand side is the entropy of event A, the second term can be interpreted as the expectation of event B in terms of event A. And the&lt;/p&gt;

&lt;p&gt;To relate cross entropy to entropy and KL divergence, we formalize the cross entropy in terms of events A and B as&lt;/p&gt;

&lt;p&gt;ğ»(ğ´,ğµ)=âˆ’âˆ‘ğ‘–ğ‘ğ´(ğ‘£ğ‘–)logğ‘ğµ(ğ‘£ğ‘–).H(A,B)=âˆ’âˆ‘ipA(vi)logâ¡pB(vi).&lt;/p&gt;

&lt;p&gt;From the definitions, we can easily seeğ»(ğ´,ğµ)=ğ·ğ¾ğ¿(ğ´âˆ¥ğµ)+ğ‘†ğ´.H(A,B)=DKL(Aâˆ¥B)+SA.     .&lt;/p&gt;

&lt;p&gt;A further question follows naturally as how the entropy can be a constant. In a machine learning task, we start with a dataset (denoted as ğ‘ƒ(îˆ°)P(D)) which represent the problem to be solved, and the learning purpose is to make the model estimated distribution (denoted as ğ‘ƒ(ğ‘šğ‘œğ‘‘ğ‘’ğ‘™)P(model)) as close as possible to true distribution of the problem (denoted as ğ‘ƒ(ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„)P(truth)). ğ‘ƒ(ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„)P(truth) is unknown and represented by ğ‘ƒ(îˆ°)P(D). Therefore in an ideal world, we expect&lt;/p&gt;

&lt;p&gt;ğ‘ƒ(ğ‘šğ‘œğ‘‘ğ‘’ğ‘™)â‰ˆğ‘ƒ(îˆ°)â‰ˆğ‘ƒ(ğ‘¡ğ‘Ÿğ‘¢ğ‘¡â„)P(model)â‰ˆP(D)â‰ˆP(truth)&lt;/p&gt;

&lt;p&gt;and minimize . And luckily, in practiceîˆ°Dis given, which means its entropyğ‘†(ğ·)S(D)is fixed as a constant.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GNN</title>
      <link>https://faithio.cn/post/gnn/</link>
      <pubDate>Sun, 19 Apr 2020 11:01:12 -0400</pubDate>
      <guid>https://faithio.cn/post/gnn/</guid>
      <description>

&lt;p&gt;Why use GNN?&lt;/p&gt;

&lt;p&gt;Classificationé—®é¢˜ï¼Œä¸¢ä¸€ä¸ªæ²¡æœ‰è§è¿‡çš„å›¾ç‰‡è¿›å»&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419110815992.png&#34; alt=&#34;image-20200419110815992&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ€ä¹ˆçŸ¥é“node å’Œ edgeçš„ feature&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419111309255.png&#34; alt=&#34;image-20200419111309255&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419111421761.png&#34; alt=&#34;image-20200419111421761&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419111957452.png&#34; alt=&#34;image-20200419111957452&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;nn4g&#34;&gt;NN4G&lt;/h2&gt;

&lt;h3 id=&#34;aggregation&#34;&gt;Aggregation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419112239215.png&#34; alt=&#34;image-20200419112239215&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;readout&#34;&gt;Readout&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419112338429.png&#34; alt=&#34;image-20200419112338429&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;dcnn&#34;&gt;DCNN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419123056579.png&#34; alt=&#34;image-20200419123056579&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419123202986.png&#34; alt=&#34;image-20200419123202986&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;dgc&#34;&gt;DGC&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419123248714.png&#34; alt=&#34;image-20200419123248714&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;monet&#34;&gt;MoNet&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419123434831.png&#34; alt=&#34;image-20200419123434831&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;gat&#34;&gt;GAT&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419123656391.png&#34; alt=&#34;image-20200419123656391&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;gin&#34;&gt;GIN&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419123931772.png&#34; alt=&#34;image-20200419123931772&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;dgl.ai&lt;/p&gt;

&lt;h3 id=&#34;graph-signal-processing-and-spectral-based-gnn&#34;&gt;Graph Signal Processing and Spectral-based GNN&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419124355835.png&#34; alt=&#34;image-20200419124355835&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419124601870.png&#34; alt=&#34;image-20200419124601870&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gradient Descent</title>
      <link>https://faithio.cn/post/gradient-descent/</link>
      <pubDate>Fri, 17 Apr 2020 10:56:33 -0400</pubDate>
      <guid>https://faithio.cn/post/gradient-descent/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;./image-20200417105657139.png&#34; alt=&#34;image-20200417105657139&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;adaptive-learning-rates&#34;&gt;Adaptive Learning Rates&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417105946810.png&#34; alt=&#34;image-20200417105946810&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418150703110.png&#34; alt=&#34;image-20200418150703110&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418150959371.png&#34; alt=&#34;image-20200418150959371&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418151157717.png&#34; alt=&#34;image-20200418151157717&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418152046091.png&#34; alt=&#34;image-20200418152046091&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418152507803.png&#34; alt=&#34;image-20200418152507803&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ä¸å¢åŠ è¿ç®—é‡çš„æƒ…å†µä¸‹ï¼Œç®—äºŒæ¬¡å¾®åˆ†ï¼Œå…¶å®æ˜¯é‡‡æ ·&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;stochastic-gradient-descent&#34;&gt;Stochastic Gradient Descent&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418153333563.png&#34; alt=&#34;image-20200418153333563&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418153757128.png&#34; alt=&#34;image-20200418153757128&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;feature-scaling&#34;&gt;Feature scaling&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418153931567.png&#34; alt=&#34;image-20200418153931567&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418154059652.png&#34; alt=&#34;image-20200418154059652&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;x1å¯¹æ¢¯åº¦å½±å“å¾ˆå°ï¼Œupdateæ…¢ï¼Œ error surface ä¸ä¸€æ ·ï¼Œæ­£åœ†é€Ÿåº¦æ›´å¿«&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418154819560.png&#34; alt=&#34;image-20200418154819560&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;gradient-descent-theory&#34;&gt;Gradient Descent Theory&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418155057008.png&#34; alt=&#34;image-20200418155057008&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418160236495.png&#34; alt=&#34;image-20200418160236495&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418160725921.png&#34; alt=&#34;image-20200418160725921&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418162014678.png&#34; alt=&#34;image-20200418162014678&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418162027542.png&#34; alt=&#34;image-20200418162027542&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418162040817.png&#34; alt=&#34;image-20200418162040817&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418163732667.png&#34; alt=&#34;image-20200418163732667&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;limitation-of-gradient-descent&#34;&gt;Limitation of Gradient Descent&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418163958360.png&#34; alt=&#34;image-20200418163958360&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;new-optimizers-for-deep-learning&#34;&gt;New Optimizers for deep learning&lt;/h3&gt;

&lt;p&gt;Exponential moving average (EMA)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419143941320.png&#34; alt=&#34;image-20200419143941320&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419144246649.png&#34; alt=&#34;image-20200419144246649&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419144940742.png&#34; alt=&#34;image-20200419144940742&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ‰¾åˆ°ä¸€ç»„å‚æ•°ä½¿å¾—Loss functionçš„å’Œæœ€å°&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419145221197.png&#34; alt=&#34;image-20200419145221197&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419145305433.png&#34; alt=&#34;image-20200419145305433&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419150407821.png&#34; alt=&#34;image-20200419150407821&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419150605065.png&#34; alt=&#34;image-20200419150605065&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419150809955.png&#34; alt=&#34;image-20200419150809955&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419150903147.png&#34; alt=&#34;image-20200419150903147&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;optimizer-real-application&#34;&gt;Optimizer: Real Application&lt;/h3&gt;

&lt;p&gt;Bert: QA/æ–‡ç« ç”Ÿæˆ/é˜…è¯»ç†è§£&lt;/p&gt;

&lt;p&gt;transformerï¼š ç¿»è¯‘&lt;/p&gt;

&lt;p&gt;Tacotronï¼šè¯­éŸ³ç”Ÿæˆ&lt;/p&gt;

&lt;p&gt;Big-Gan&lt;/p&gt;

&lt;p&gt;MEMOï¼šåœ¨ä¸åŒåˆ†ç±»å­¦ä¹ å…±åŒçš„åˆ†ç±»&lt;/p&gt;

&lt;p&gt;éƒ½æ˜¯Adam&lt;/p&gt;

&lt;p&gt;YOLOï¼šå½±åƒä¾¦æµ‹&lt;/p&gt;

&lt;p&gt;Mask R-CNN&lt;/p&gt;

&lt;p&gt;ResNet&lt;/p&gt;

&lt;p&gt;éƒ½æ˜¯ï¼ˆSGDMï¼‰&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/&#34; target=&#34;_blank&#34;&gt;https://shaoanlu.wordpress.com/2017/05/29/sgd-all-which-one-is-the-best-optimizer-dogs-vs-cats-toy-experiment/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419151753171.png&#34; alt=&#34;image-20200419151753171&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SGDM æ¯”è¾ƒç¨³å®šï¼Œä¸ä¼šæœ‰å¾ˆå¤§è½å·®ï¼Œcovergeå¾ˆå¥½ã€‚&lt;/p&gt;

&lt;p&gt;Ttrainingå’Œtestingçš„functionæœ‰å·®å¼‚ã€‚&lt;/p&gt;

&lt;h3 id=&#34;swats&#34;&gt;SWATS&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419152122414.png&#34; alt=&#34;image-20200419152122414&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ä»€ä¹ˆåˆ‡æ¢ç‚¹ä¸å¤ªåˆé€‚ï¼Œæ²¡æœ‰è¯æ˜ã€‚Adamæ˜¯adaptiveï¼Œä¼šè¢«åˆ†æ¯å½±å“&lt;/p&gt;

&lt;p&gt;åˆ‡æ¢çš„æ–¹æ³•ä¸ç§‘å­¦&lt;/p&gt;

&lt;p&gt;Adamçš„é—®é¢˜&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419152729164.png&#34; alt=&#34;image-20200419152729164&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419152825824.png&#34; alt=&#34;image-20200419152825824&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;amsgrad&#34;&gt;AMSGrad&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419153100167.png&#34; alt=&#34;image-20200419153100167&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419153538368.png&#34; alt=&#34;image-20200419153538368&#34; /&gt;&lt;/p&gt;

&lt;p&gt;adaptive learning rate å¯ä»¥åŠ¨æ€è°ƒæ•´ï¼Œéœ€è¦å¤§æ­¥èµ°å¤§æ­¥ï¼Œå¹³ç¼“çš„åœ°æ–¹å¯ä»¥å¤§æ­¥èµ°è¿‡å»ã€‚&lt;/p&gt;

&lt;h3 id=&#34;lr-range-test&#34;&gt;LR range test&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419153736973.png&#34; alt=&#34;image-20200419153736973&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;cyclical-lr&#34;&gt;Cyclical LR&lt;/h3&gt;

&lt;p&gt;learning rate å¤§çš„æ—¶å€™å°±æ˜¯exploringï¼Œå°çš„æ—¶å€™fine-tuning, step sizeå¦‚ä½•è®¾è®¡ï¼Ÿ&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419153834405.png&#34; alt=&#34;image-20200419153834405&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;does-adam-need-warm-up&#34;&gt;Does Adam need warm-up?&lt;/h3&gt;

&lt;h3 id=&#34;æ€»ç»“&#34;&gt;æ€»ç»“&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419155036254.png&#34; alt=&#34;image-20200419155036254&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419155053058.png&#34; alt=&#34;image-20200419155053058&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Transformer</title>
      <link>https://faithio.cn/post/transformer/</link>
      <pubDate>Wed, 15 Apr 2020 23:45:52 -0400</pubDate>
      <guid>https://faithio.cn/post/transformer/</guid>
      <description>

&lt;p&gt;bert - semi-supervisored learning&lt;/p&gt;

&lt;p&gt;RNN - Hard to parallel&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415235158012.png&#34; alt=&#34;image-20200415235158012&#34; /&gt;&lt;/p&gt;

&lt;p&gt;CNN can parallel, ä½†æ˜¯éœ€è¦å å¾ˆé•¿æ—¶é—´çš„å’¨è¯¢&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415235355040.png&#34; alt=&#34;image-20200415235355040&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å¯ä»¥å®Œå…¨æ›¿ä»£RNNï¼Œä¹Ÿæ˜¯sequence2sequenceçš„æ¨¡å‹&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415235607503.png&#34; alt=&#34;image-20200415235607503&#34; /&gt;&lt;/p&gt;

&lt;p&gt;é™¤ä»¥dimensionsæ˜¯ä¸ºäº†å‡å°‘varianceï¼Œç›¸ä¹˜ä¼šäº§ç”Ÿæ›´å¤šerror&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415235927070.png&#34; alt=&#34;image-20200415235927070&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416000112495.png&#34; alt=&#34;image-20200416000112495&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416000314547.png&#34; alt=&#34;image-20200416000314547&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416000357860.png&#34; alt=&#34;image-20200416000357860&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416000525138.png&#34; alt=&#34;image-20200416000525138&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416000630467.png&#34; alt=&#34;image-20200416000630467&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416000722766.png&#34; alt=&#34;image-20200416000722766&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416000820901.png&#34; alt=&#34;image-20200416000820901&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;summary&#34;&gt;Summary&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416000935054.png&#34; alt=&#34;image-20200416000935054&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;å˜å½¢&#34;&gt;å˜å½¢&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416001102390.png&#34; alt=&#34;image-20200416001102390&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416001147596.png&#34; alt=&#34;image-20200416001147596&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ä½ç½®å¹¶ä¸é‡è¦ï¼ˆå¤©æ¶¯è‹¥æ¯”é‚»ï¼‰&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416001522212.png&#34; alt=&#34;image-20200416001522212&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;seq2seq&#34;&gt;seq2seq&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416001730057.png&#34; alt=&#34;image-20200416001730057&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416002059224.png&#34; alt=&#34;image-20200416002059224&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416002532251.png&#34; alt=&#34;image-20200416002532251&#34; /&gt;&lt;/p&gt;

&lt;p&gt;mask: åªä¼šattend äº§ç”Ÿå‡ºæ¥çš„sequenceã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416002650474.png&#34; alt=&#34;image-20200416002650474&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416003133580.png&#34; alt=&#34;image-20200416003133580&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200416003205757.png&#34; alt=&#34;image-20200416003205757&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Embedding</title>
      <link>https://faithio.cn/post/wordembeddingelmobert/</link>
      <pubDate>Wed, 15 Apr 2020 18:48:25 -0400</pubDate>
      <guid>https://faithio.cn/post/wordembeddingelmobert/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;./image-20200415184935540.png&#34; alt=&#34;image-20200415184935540&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Generating Word Vector is unsupervised.&lt;/p&gt;

&lt;p&gt;How about auto-encoder?&lt;/p&gt;

&lt;h3 id=&#34;count-based&#34;&gt;Count based&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415185358996.png&#34; alt=&#34;image-20200415185358996&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;prediction-based&#34;&gt;Prediction-based&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415185725800.png&#34; alt=&#34;image-20200415185725800&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415190433735.png&#34; alt=&#34;image-20200415190433735&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415200628276.png&#34; alt=&#34;image-20200415200628276&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425210733980.png&#34; alt=&#34;image-20200425210733980&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å°±ä¹Ÿæ˜¯one of encodingï¼Œoutput æ¥è¿‘&lt;/p&gt;

&lt;h3 id=&#34;å˜å½¢&#34;&gt;å˜å½¢&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415203213211.png&#34; alt=&#34;image-20200415203213211&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ä¸æ˜¯deep ï¼Œåªæ˜¯ä¸€ä¸ªlinear hidden layerã€‚&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;æœ‰å¾ˆå¤štipï¼Œå¾ˆå¤šäººå…¶å®ä¹Ÿåšè¿‡ï¼Œå¹¶ä¸”ç”¨deepã€‚ä¸ç”¨deepå¯ä»¥åšå¾ˆå¤šè¿ç®—é‡&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415204214466.png&#34; alt=&#34;image-20200415204214466&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415204329240.png&#34; alt=&#34;image-20200415204329240&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415204642846.png&#34; alt=&#34;image-20200415204642846&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;unsupervised-learning-neighbor-embedding&#34;&gt;Unsupervised Learning: Neighbor Embedding&lt;/h2&gt;

&lt;p&gt;TSNE&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415205127380.png&#34; alt=&#34;image-20200415205127380&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415205325147.png&#34; alt=&#34;image-20200415205325147&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415205414215.png&#34; alt=&#34;image-20200415205414215&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415205531081.png&#34; alt=&#34;image-20200415205531081&#34; /&gt;&lt;/p&gt;

&lt;p&gt;x&amp;ndash;&amp;gt;zæ²¡æœ‰å¾ˆå¥½çš„ä¾æ®&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;ELMO/BERT/GPT&lt;/p&gt;

&lt;p&gt;å¦‚æœåŒä¸€ä¸ªè¯æ±‡æœ‰å¤šç§senses&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425212956279.png&#34; alt=&#34;image-20200425212956279&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ä¸åŒæ„æ€çš„tokenåŒæ ·çš„typeä¹Ÿæœ‰ä¸åŒçš„embedding&lt;/p&gt;

&lt;p&gt;è¿‡å»æ˜¯æŸ¥å­—å…¸ï¼Œç”¨ä¸åŒçš„embedding&lt;/p&gt;

&lt;p&gt;ç„¶åæ¯ä¸ªword token has its own embedding&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425213447780.png&#34; alt=&#34;image-20200425213447780&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;elmo&#34;&gt;ELMO&lt;/h3&gt;

&lt;p&gt;Embeddings from Language Model&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425213741845.png&#34; alt=&#34;image-20200425213741845&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ä¸åŒè¯­ä¹‰ä¸‹çš„hidden layerçš„input ä½œä¸ºContextualized embedding&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425213936075.png&#34; alt=&#34;image-20200425213936075&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ä¸åŒçš„äººç‰©å–çš„hidderlayerä¸ä¸€æ ·&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425214953693.png&#34; alt=&#34;image-20200425214953693&#34; /&gt;&lt;/p&gt;

&lt;p&gt;SRL:&lt;/p&gt;

&lt;p&gt;Coref: ä»£åè¯æ‰¾åˆ°äººç‰©æ‰¾å‡º&lt;/p&gt;

&lt;p&gt;SNLI:&lt;/p&gt;

&lt;p&gt;SQuAD: QAé—®é¢˜&lt;/p&gt;

&lt;p&gt;SST-5: Semantic classification&lt;/p&gt;

&lt;h3 id=&#34;bidirectional-encoder-representations-from-transformers-bert&#34;&gt;Bidirectional Encoder Representations from Transformers (BERT)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425215119903.png&#34; alt=&#34;image-20200425215119903&#34; /&gt;&lt;/p&gt;

&lt;p&gt;è®­ç»ƒbertçš„æ—¶å€™ï¼Œä¸­æ–‡æœ€å¥½ç”¨å­—æ›´å¥½ï¼Œinput wordï¼Œä¼šäº§å‡ºä¸€å †embedding&lt;/p&gt;

&lt;p&gt;ä¸­æ–‡çš„å­—ï¼Œå¸¸ç”¨çš„åªæœ‰4k   one of encoding å°å¾ˆå¤šï¼Œä½†æ˜¯è¯éƒ½æ˜¯æ— ç©·&lt;/p&gt;

&lt;h4 id=&#34;training&#34;&gt;training&lt;/h4&gt;

&lt;p&gt;ç¬¬ä¸€ç§&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425215628350.png&#34; alt=&#34;image-20200425215628350&#34; /&gt;&lt;/p&gt;

&lt;p&gt;éšæœºæŒ–ç©º15%mask ç„¶åé¢„æµ‹ã€‚ä¸¤ä¸ªè¯å¡«åœ¨è¿™é‡Œæ²¡æœ‰è¿å’Œæ„Ÿå°±è¡¨ç¤ºæœ‰ç±»ä¼¼çš„embedding&lt;/p&gt;

&lt;p&gt;ç¬¬äºŒç§æ–¹æ³•ï¼š&lt;/p&gt;

&lt;p&gt;SEP: åˆ†ç•Œã€‚&lt;/p&gt;

&lt;p&gt;CLSï¼šclassificationã€‚åœ¨è¿™å¼€å¤´è¡¨ç¤ºè¦åšåˆ†ç±»ã€‚ä¸ºä»€ä¹ˆæ”¾å¼€å¤´ï¼Ÿ&lt;/p&gt;

&lt;p&gt;å¦‚æœæ˜¯RNNç»“æ„ï¼Œæ”¾åœ¨å°¾éƒ¨æ¯”è¾ƒåˆç†ï¼Œå› ä¸ºå°¾éƒ¨æ‰ä¼šè¾“å‡ºã€‚ä½†æ˜¯bertå†…éƒ¨æ˜¯transformerï¼Œä¸¤ä¸ªç›¸é‚»æˆ–è€…å¾ˆè¿œçš„æ²¡æœ‰å·®åˆ«ã€‚å¯¹äºæ”¾åœ¨å“ªé‡ŒåŒºåˆ«ä¸å¤§&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./Users/faith/academic-zixiang/content/en/post/WordEmbedding:ELMO:BERT/image-20200425215937528.png&#34; alt=&#34;image-20200425215937528&#34; /&gt;&lt;/p&gt;

&lt;p&gt;è¿™ä¸¤ä¸ªæ–¹æ³•éœ€è¦åŒæ—¶ä½¿ç”¨ï¼Œæ•ˆæœæœ€å¥½&lt;/p&gt;

&lt;p&gt;åœ¨CLSå‰é¢æ”¾ä¸€ä¸ªç±»å‹ï¼Œ24å±‚æˆ–è€…48å±‚ï¼Œæ–‡æœ¬åˆ†ç±»ï¼Œæƒ…æ„Ÿåˆ†ç±»&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425220550264.png&#34; alt=&#34;image-20200425220550264&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Slot fillingï¼ŒæŒ–è¯å¡«ç©º&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425221015668.png&#34; alt=&#34;image-20200425221015668&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Natural Language Inferenceï¼Œå›ç­”å¯¹é”™&lt;/p&gt;

&lt;p&gt;å…¶å®æ˜¯åˆ†ç±»ï¼Œåªæœ‰ä¸‰ç±»&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425221136235.png&#34; alt=&#34;image-20200425221136235&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Extraction-based Question Answering é˜…è¯»ç†è§£&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425221419766.png&#34; alt=&#34;image-20200425221419766&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ€ä¹ˆtrainå‘¢ï¼Ÿ&lt;/p&gt;

&lt;p&gt;æ©™è‰²çš„sï¼Œè“è‰²çš„æ˜¯eï¼Œæ¯æ¬¡éƒ½åšç±»ä¼¼attentionçš„åŠ¨ä½œã€‚è¿™äº›è¯æ±‡æ¥è‡ªäºæ–‡ç« ã€‚å–æœ€é«˜çš„æ¦‚ç‡&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425221745810.png&#34; alt=&#34;image-20200425221745810&#34; /&gt;&lt;/p&gt;

&lt;p&gt;S=3ï¼Œe=2 å°±æ˜¯æ­¤é¢˜æ— è§£&lt;/p&gt;

&lt;h3 id=&#34;enhanced-representation-through-knowledge-integration-ernie&#34;&gt;Enhanced Representation through Knowledge Integration (ERNIE)&lt;/h3&gt;

&lt;p&gt;ä¸ºä¸­æ–‡è®¾è®¡çš„&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425222151270.png&#34; alt=&#34;image-20200425222151270&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;bert trained byå…¶ä»–è¯­è¨€æ–‡æœ¬åˆ†ç±»ï¼Œä½†æ˜¯å¯ä»¥å¿«é€Ÿå­¦ä¼šä¸­æ–‡çš„åˆ†ç±»&lt;/p&gt;

&lt;h3 id=&#34;generative-pre-training-gpt&#34;&gt;Generative Pre-Training (GPT)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425224702668.png&#34; alt=&#34;image-20200425224702668&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425224918497.png&#34; alt=&#34;image-20200425224918497&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200425225049069.png&#34; alt=&#34;image-20200425225049069&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ML 2020</title>
      <link>https://faithio.cn/post/ml-2020/</link>
      <pubDate>Wed, 15 Apr 2020 16:42:22 -0400</pubDate>
      <guid>https://faithio.cn/post/ml-2020/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;./image-20200415164228259.png&#34; alt=&#34;image-20200415164228259&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415164428366.png&#34; alt=&#34;image-20200415164428366&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415164600406.png&#34; alt=&#34;image-20200415164600406&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415164621730.png&#34; alt=&#34;image-20200415164621730&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415164725366.png&#34; alt=&#34;image-20200415164725366&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415164955799.png&#34; alt=&#34;image-20200415164955799&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415165052826.png&#34; alt=&#34;image-20200415165052826&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200415165508904.png&#34; alt=&#34;image-20200415165508904&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Auto Encoder</title>
      <link>https://faithio.cn/post/auto-encoder/</link>
      <pubDate>Thu, 09 Apr 2020 22:19:05 -0400</pubDate>
      <guid>https://faithio.cn/post/auto-encoder/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;./image-20200409223241587.png&#34; alt=&#34;image-20200409223241587&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;pre-training&#34;&gt;Pre-training&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409221924650.png&#34; alt=&#34;image-20200409221924650&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;unlabled dataæœ‰å¤§é‡ï¼Œå¯ä»¥å…ˆè®­ç»ƒå¥½ï¼Œæœ€åå†ç”¨labled data train ä¸€ä¸‹å°±å¥½&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409222120187.png&#34; alt=&#34;image-20200409222120187&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;cnn&#34;&gt;CNN&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409222335967.png&#34; alt=&#34;image-20200409222335967&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409222517794.png&#34; alt=&#34;image-20200409222517794&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409222810907.png&#34; alt=&#34;image-20200409222810907&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Structured Learning</title>
      <link>https://faithio.cn/post/structured-learning/</link>
      <pubDate>Thu, 09 Apr 2020 17:16:09 -0400</pubDate>
      <guid>https://faithio.cn/post/structured-learning/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;./image-20200409171641927.png&#34; alt=&#34;image-20200409171641927&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;hmm&#34;&gt;HMM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409171835224.png&#34; alt=&#34;image-20200409171835224&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;step1&#34;&gt;Step1&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409172113243.png&#34; alt=&#34;image-20200409172113243&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;step2&#34;&gt;Step2&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409172211239.png&#34; alt=&#34;image-20200409172211239&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409172318205.png&#34; alt=&#34;image-20200409172318205&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409172424343.png&#34; alt=&#34;image-20200409172424343&#34; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409172500491.png&#34; alt=&#34;image-20200409172500491&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409172645029.png&#34; alt=&#34;image-20200409172645029&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409173134454.png&#34; alt=&#34;image-20200409173134454&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409173226554.png&#34; alt=&#34;image-20200409173226554&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409173423567.png&#34; alt=&#34;image-20200409173423567&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409173952413.png&#34; alt=&#34;image-20200409173952413&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RNN</title>
      <link>https://faithio.cn/post/rnn/</link>
      <pubDate>Wed, 08 Apr 2020 23:32:41 -0400</pubDate>
      <guid>https://faithio.cn/post/rnn/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN backprop.ecm.mp4/&#34; target=&#34;_blank&#34;&gt;http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN%20backprop.ecm.mp4/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409001545106.png&#34; alt=&#34;image-20200409001545106&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409001558634.png&#34; alt=&#34;image-20200409001558634&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409001619909.png&#34; alt=&#34;image-20200409001619909&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409001648125.png&#34; alt=&#34;image-20200409001648125&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408233652075.png&#34; alt=&#34;image-20200408233652075&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Gate function uses sigmoid, but not ReLu&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408233931947.png&#34; alt=&#34;image-20200408233931947&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408234357433.png&#34; alt=&#34;image-20200408234357433&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408234617041.png&#34; alt=&#34;image-20200408234617041&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408234854424.png&#34; alt=&#34;image-20200408234854424&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408235319626.png&#34; alt=&#34;image-20200408235319626&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408235531482.png&#34; alt=&#34;image-20200408235531482&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408235710883.png&#34; alt=&#34;image-20200408235710883&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408235855214.png&#34; alt=&#34;image-20200408235855214&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408235835005.png&#34; alt=&#34;image-20200408235835005&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409000422582.png&#34; alt=&#34;image-20200409000422582&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409000655276.png&#34; alt=&#34;image-20200409000655276&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;rnn-çš„big-problems&#34;&gt;RNN çš„big problems&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409000918432.png&#34; alt=&#34;image-20200409000918432&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409001231798.png&#34; alt=&#34;image-20200409001231798&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409001835408.png&#34; alt=&#34;image-20200409001835408&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;RNNä¸å¥½è®­ç»ƒä¸æ˜¯å› ä¸ºactivation functionï¼Œè€Œæ˜¯æ¥è‡ªtime sequenceï¼ŒåŒæ ·çš„weightï¼Œåœ¨ä¸åŒçš„æ—¶é—´ç‚¹åå¤ä½¿ç”¨&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;lstm&#34;&gt;LSTM&lt;/h4&gt;

&lt;p&gt;å¯ä»¥è®©RNNçš„error surface ä¸é‚£ä¹ˆå´å²–ï¼Œå¯ä»¥æŠŠå¹³å¦çš„åœ°æ–¹æ‹¿æ‰ï¼Œgradient vanishing but not gradient explodeã€‚&lt;/p&gt;

&lt;p&gt;learning rate å°çš„æ—¶å€™è®­ç»ƒï¼Œæ²¡æœ‰å¹³å°è®­ç»ƒã€‚&lt;/p&gt;

&lt;p&gt;ä¸ºä»€ä¹ˆhandling gradient vanishingï¼Ÿ&lt;/p&gt;

&lt;p&gt;RNNï¼šmemoryçš„å€¼æ¯æ¬¡éƒ½ä¼šè¢«æ´—æ‰&lt;/p&gt;

&lt;p&gt;LSTMï¼šmemory ä¹˜ä»¥forget gateå†åŠ èµ·inputçš„å€¼ï¼Œ memoryå’Œinputç›¸åŠ çš„ã€‚åªè¦forget gate openå°±ä¸ä¼šæ¸…é™¤memoryï¼Œæ‰€ä»¥ä¸ä¼šæœ‰gradient vanishingã€‚&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;å»ºè®®ä¸è¦ç»™forget gateå¾ˆå¤§çš„biasï¼Œç¡®ä¿å®ƒå¤šæ•°æƒ…å†µå¼€å¯ã€‚&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409002919268.png&#34; alt=&#34;image-20200409002919268&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;å‚æ•°æ¯”è¾ƒå°‘ï¼ŒLSTMå¦‚æœoverfitingï¼Œå¯ä»¥å°è¯•GRUã€‚&lt;/p&gt;

&lt;p&gt;GRUï¼š input gate/ forget gateè”åŠ¨ï¼Œinput æ‰“å¼€çš„æ—¶å€™ï¼Œforgetå…³é—­ï¼Œæ´—æ‰memoryã€‚è¦æ¸…æ‰memoryæ‰èƒ½æ”¾æ–°å€¼ã€‚&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409003240369.png&#34; alt=&#34;image-20200409003240369&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;å¦‚æœç”¨identity matrix åˆå§‹åŒ–ï¼Œå°±æ˜¯ReLu ä½œä¸ºactivation functionï¼Œæ•ˆæœå¾ˆå¥½ã€‚åŠæ‰“LSTMï¼&lt;/p&gt;

&lt;p&gt;å¦‚æœæ˜¯randomï¼Œå°±ç”¨sigmoid ä½œä¸ºactivation functionã€‚&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409003706220.png&#34; alt=&#34;image-20200409003706220&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409003903477.png&#34; alt=&#34;image-20200409003903477&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409004110329.png&#34; alt=&#34;image-20200409004110329&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409004228085.png&#34; alt=&#34;image-20200409004228085&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409004508687.png&#34; alt=&#34;image-20200409004508687&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409004618217.png&#34; alt=&#34;image-20200409004618217&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409092820297.png&#34; alt=&#34;image-20200409092820297&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409093035412.png&#34; alt=&#34;image-20200409093035412&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;attention-based-model-memory&#34;&gt;Attention-based model(Memory)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409102156594.png&#34; alt=&#34;image-20200409102156594&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409102305686.png&#34; alt=&#34;image-20200409102305686&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409115636418.png&#34; alt=&#34;image-20200409115636418&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409120107169.png&#34; alt=&#34;image-20200409120107169&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409163453089.png&#34; alt=&#34;image-20200409163453089&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409164153093.png&#34; alt=&#34;image-20200409164153093&#34; /&gt;&lt;/p&gt;

&lt;p&gt;RNN output &amp;ndash;&amp;gt; HMM/CRF(input)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409164833992.png&#34; alt=&#34;image-20200409164833992&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200409165025527.png&#34; alt=&#34;image-20200409165025527&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CNN</title>
      <link>https://faithio.cn/post/cnn/</link>
      <pubDate>Wed, 08 Apr 2020 22:30:14 -0400</pubDate>
      <guid>https://faithio.cn/post/cnn/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;./image-20200408223055298.png&#34; alt=&#34;image-20200408223055298&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223108550.png&#34; alt=&#34;image-20200408223108550&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223121952.png&#34; alt=&#34;image-20200408223121952&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223133146.png&#34; alt=&#34;image-20200408223133146&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223143761.png&#34; alt=&#34;image-20200408223143761&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223154238.png&#34; alt=&#34;image-20200408223154238&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223205482.png&#34; alt=&#34;image-20200408223205482&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223218039.png&#34; alt=&#34;image-20200408223218039&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223228871.png&#34; alt=&#34;image-20200408223228871&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223238009.png&#34; alt=&#34;image-20200408223238009&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223247511.png&#34; alt=&#34;image-20200408223247511&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223303544.png&#34; alt=&#34;image-20200408223303544&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223314184.png&#34; alt=&#34;image-20200408223314184&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200408223324970.png&#34; alt=&#34;image-20200408223324970&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;dropout linear å¥½ &amp;ndash;&amp;gt;ReLU/maxout å¥½&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;##ResNet&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200423183015473.png&#34; alt=&#34;image-20200423183015473&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200423182923388.png&#34; alt=&#34;image-20200423182923388&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classification</title>
      <link>https://faithio.cn/post/classification/</link>
      <pubDate>Tue, 07 Apr 2020 11:23:28 -0400</pubDate>
      <guid>https://faithio.cn/post/classification/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;./image-20200407112526341.png&#34; alt=&#34;image-20200407112526341&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200407112551869.png&#34; alt=&#34;image-20200407112551869&#34; /&gt;
$$
\begin{array}{l}P(Y | X)=\frac{P(X | Y) P(Y)}{P(X)} \propto(P(X | Y) \cdot P(Y) \
M A P \ P(Y=0 | X) \Rightarrow P(Y=1 | X)\end{array}
$$&lt;/p&gt;

&lt;h2 id=&#34;classification&#34;&gt;Classification&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418205909645.png&#34; alt=&#34;image-20200418205909645&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418210055887.png&#34; alt=&#34;image-20200418210055887&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418210413842.png&#34; alt=&#34;image-20200418210413842&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418211630205.png&#34; alt=&#34;image-20200418211630205&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418211819580.png&#34; alt=&#34;image-20200418211819580&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418212423769.png&#34; alt=&#34;image-20200418212423769&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;æ”¹è¿›&#34;&gt;æ”¹è¿›&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418212408745.png&#34; alt=&#34;image-20200418212408745&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ä¸åŒçš„classå¯ä»¥shareå‚æ•°ï¼Œ&lt;strong&gt;å¦‚æœå‚æ•°è¶Šå¤šï¼Œvarianceä¼šè¶Šå¤§ï¼Œå°±ä¼šå¯¼è‡´overfiting&lt;/strong&gt;ã€‚Featureå’Œcovarianceçš„å¹³æ–¹æˆæ­£æ¯”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418212749041.png&#34; alt=&#34;image-20200418212749041&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418213117186.png&#34; alt=&#34;image-20200418213117186&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;å½“Gaussian å…±ç”¨covarianceçš„æ—¶å€™ï¼Œboundaryå°±æ˜¯linearçš„ï¼Œæ‰€ä»¥ä¹Ÿå¯ä»¥æˆä¸ºlinear modelã€‚ä½†æ˜¯å¦‚æœä¸å…±ç”¨covarianceå°±ä¸æ˜¯linearçš„&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ç®€å•çš„æ¨¡å‹ï¼Œå‚æ•°å°‘ï¼Œbiaså¤§ï¼Œvarianceå°&lt;/p&gt;

&lt;p&gt;å¤æ‚çš„æ¨¡å‹ï¼Œå‚æ•°å¤šï¼Œbiaså°ï¼Œvarianceå¤§&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418214124978.png&#34; alt=&#34;image-20200418214124978&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;covarianceé™¤äº†å¯¹è§’çº¿éƒ½æ˜¯é›¶ï¼Œå±æ€§ä¸å±æ€§ä¹‹é—´æ˜¯independent&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418214356057.png&#34; alt=&#34;image-20200418214356057&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418214503762.png&#34; alt=&#34;image-20200418214503762&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418214552654.png&#34; alt=&#34;image-20200418214552654&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418214802448.png&#34; alt=&#34;image-20200418214802448&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418215129639.png&#34; alt=&#34;image-20200418215129639&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;logistic-regression&#34;&gt;Logistic Regression&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418215555308.png&#34; alt=&#34;image-20200418215555308&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418215808021.png&#34; alt=&#34;image-20200418215808021&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418220022300.png&#34; alt=&#34;image-20200418220022300&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Cross entropy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418220335222.png&#34; alt=&#34;image-20200418220335222&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ä¸ºä»€ä¹ˆé€»è¾‘å›å½’ä¸ç”¨square erroræ¥å®šä¹‰loss functionå‘¢ï¼Ÿ&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418220637419.png&#34; alt=&#34;image-20200418220637419&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418220754427.png&#34; alt=&#34;image-20200418220754427&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418220943512.png&#34; alt=&#34;image-20200418220943512&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418221021157.png&#34; alt=&#34;image-20200418221021157&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;å¯ä»¥é€šè¿‡è§£least square errorçš„æ–¹å¼ï¼Œæ‰¾æœ€ä½³è§£ï¼Œç„¶ååˆå§‹åŒ–&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;why-logistic-regression-cannot-use-square-error&#34;&gt;Why Logistic Regression cannot use square error&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418221807965.png&#34; alt=&#34;image-20200418221807965&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418221825935.png&#34; alt=&#34;image-20200418221825935&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418221947036.png&#34; alt=&#34;image-20200418221947036&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;discriminative-vs-generative&#34;&gt;Discriminative vs Generative&lt;/h3&gt;

&lt;p&gt;Gaussian å±äºGenerativeï¼Œä½†æ˜¯covarianceshareçš„è¯ï¼Œmodelæ˜¯ä¸€æ ·çš„&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418222434264.png&#34; alt=&#34;image-20200418222434264&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ‰¾å‡ºçš„parameter w bæ˜¯ä¸ä¸€æ ·çš„&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418223045516.png&#34; alt=&#34;image-20200418223045516&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418223100856.png&#34; alt=&#34;image-20200418223100856&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418223119457.png&#34; alt=&#34;image-20200418223119457&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418223427729.png&#34; alt=&#34;image-20200418223427729&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å¦‚æœå½“å‰labelæœ‰é—®é¢˜ï¼Œdataæ˜¯æœ‰noiseçš„ï¼Œgenerative modelåšäº†ä¸€äº›å¼ºçš„å‡è®¾å°±å¯ä»¥æ’é™¤å¹²æ‰°ã€‚&lt;/p&gt;

&lt;p&gt;prior å’Œ class-dependent probabilityæ¥è‡ªä¸åŒçš„åˆ†å¸ƒï¼Œè¯­éŸ³è¾¨è¯†ï¼Œprioræ˜¯ä»ç½‘ç»œä¸Šæ”¶é›†çš„æ–‡æœ¬ï¼ˆæŸä¸€å¥è¯è¯´å‡ºçš„å‡ ç‡ï¼‰ï¼Œclass-dependentæ‰æ˜¯å£°éŸ³å’Œæ–‡æœ¬çš„ç»“åˆã€‚&lt;/p&gt;

&lt;p&gt;æ‰€ä»¥æ•´ä¸ªç³»ç»Ÿæ˜¯generativeçš„ç³»ç»Ÿ&lt;/p&gt;

&lt;h3 id=&#34;multilass&#34;&gt;Multilass&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418224131427.png&#34; alt=&#34;image-20200418224131427&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å› ä¸ºæœ‰exponential å¤§çš„å€¼å’Œå°çš„å€¼ä¼šè¢«æ‹‰çš„æ›´å¼€ï¼Œä¸€å®šæ˜¯æ­£çš„ã€‚total sum æ˜¯ï¼Œå› ä¸ºåšäº†normalizationã€‚ï¼ˆsoftmaxæ˜¯å¼ºåŒ–ï¼‰&lt;/p&gt;

&lt;p&gt;Softmax outputç”¨æ¥ç»Ÿè®¡posterior probabilityã€‚æœ‰ä¸‰ä¸ªclassï¼Œ gaussianï¼Œå…±ç”¨covariance matrixï¼Œå°±ä¼šå¾—åˆ°softmaxã€‚ï¼ˆMaximum entropyå’ŒLogistic regressionä¸€æ ·çš„ï¼‰&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418224827186.png&#34; alt=&#34;image-20200418224827186&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;xoré—®é¢˜&#34;&gt;XORé—®é¢˜&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418224940733.png&#34; alt=&#34;image-20200418224940733&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418225134560.png&#34; alt=&#34;image-20200418225134560&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200418225644674.png&#34; alt=&#34;image-20200418225644674&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SVM</title>
      <link>https://faithio.cn/post/svm/</link>
      <pubDate>Tue, 07 Apr 2020 11:21:59 -0400</pubDate>
      <guid>https://faithio.cn/post/svm/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;./image-20200407112231032.png&#34; alt=&#34;image-20200407112231032&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generative Adversarial Network</title>
      <link>https://faithio.cn/post/generative-adversarial-network/</link>
      <pubDate>Mon, 06 Apr 2020 10:40:49 -0400</pubDate>
      <guid>https://faithio.cn/post/generative-adversarial-network/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;./gan.html&#34;&gt;click here for link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;gan&#34;&gt;GAN&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200406104115087.png&#34; alt=&#34;image-20200406104115087&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200406104139692.png&#34; alt=&#34;image-20200406104139692&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200406104202074.png&#34; alt=&#34;image-20200406104202074&#34; /&gt;&lt;/p&gt;

&lt;p&gt;$$
\mathcal{KL}(p|q) = E_p[\log \frac{p}{q}] \
&amp;mdash;-&lt;br /&gt;
P_g \rightarrow \theta_g &lt;br /&gt;
\text{  MLE: } &lt;br /&gt;
\theta&lt;em&gt;g  = \arg \max&lt;/em&gt;{\theta&lt;em&gt;g} \sum&lt;/em&gt;{i=1}^N \log P_g(x&lt;em&gt;i) &lt;br /&gt;
= \arg \min \mathcal{KL}(P&lt;/em&gt;{data}|P_g)
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning</title>
      <link>https://faithio.cn/post/deep-learning/</link>
      <pubDate>Tue, 24 Mar 2020 21:26:35 -0400</pubDate>
      <guid>https://faithio.cn/post/deep-learning/</guid>
      <description>

&lt;h2 id=&#34;neural-networks-basics&#34;&gt;Neural Networks Basics&lt;/h2&gt;

&lt;h3 id=&#34;logistic-regression-as-a-neural-network&#34;&gt;Logistic Regression as a Neural Network&lt;/h3&gt;

&lt;p&gt;$$
\hat{y}=\sigma\left(w^{T} x+b\right), \text { where } \sigma(z)=\frac{1}{1+e^{-z}} \&lt;/p&gt;

&lt;p&gt;\text { Given }\left{\left(x^{(1)}, y^{(1)}\right), \ldots,\left(x^{(m)}, y^{(m)}\right)\right}, \text { want } \hat{y}^{(i)} \approx y^{(i)}
$$&lt;/p&gt;

&lt;p&gt;Loss(error) function:&lt;/p&gt;

&lt;p&gt;Don&amp;rsquo;t use this, non-convex
$$
\ell(\hat{y}, y)=\frac{1}{2}(\hat{y}-y)^{2}
$$
The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.
$$
J(w, b)=\frac{1}{m} \sum&lt;em&gt;{i=1}^{m} \mathcal{L}\left(\hat{y}^{(i)}, y^{(i)}\right)=-\frac{1}{m} \sum&lt;/em&gt;{i=1}^{m} y^{(i)} \log \widehat{y}^{(i)}+\left(1-y^{(i)}\right) \log \left(1-\hat{y}^{(i)}\right)
$$&lt;/p&gt;

&lt;p&gt;$$
\text { Want to find } w, b \text { that minimize } J(w, b)
$$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200324214403439.png&#34; alt=&#34;image-20200324214403439&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200324215144592.png&#34; alt=&#34;image-20200324215144592&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200324220303010.png&#34; alt=&#34;image-20200324220303010&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;gradient-desent&#34;&gt;Gradient Desent&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200324221310916.png&#34; alt=&#34;image-20200324221310916&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200324232529252.png&#34; alt=&#34;image-20200324232529252&#34; /&gt;&lt;/p&gt;

&lt;p&gt;$$
\begin{array}{l}\text { Step } 1: \frac{d L}{d a} \ L=-(y \times \log (a)+(1-y) \times \log (1-a)) \ \frac{d L}{d a}=-y \times \frac{1}{a}-(1-y) \times \frac{1}{1-a} \times-1\end{array}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\dfrac{d}{dx} \sigma(x) &amp;amp;= \dfrac{d}{dx} \left[ \dfrac{1}{1 + e^{-x}} \right] &lt;br /&gt;
&amp;amp;= \dfrac{d}{dx} \left( 1 + \mathrm{e}^{-x} \right)^{-1} &lt;br /&gt;
&amp;amp;= -(1 + e^{-x})^{-2}(-e^{-x}) &lt;br /&gt;
&amp;amp;= \dfrac{e^{-x}}{\left(1 + e^{-x}\right)^2} &lt;br /&gt;
&amp;amp;= \dfrac{1}{1 + e^{-x}\ } \cdot \dfrac{e^{-x}}{1 + e^{-x}}  &lt;br /&gt;
&amp;amp;= \dfrac{1}{1 + e^{-x}\ } \cdot \dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}}  &lt;br /&gt;
&amp;amp;= \dfrac{1}{1 + e^{-x}\ } \cdot \left( \dfrac{1 + e^{-x}}{1 + e^{-x}} - \dfrac{1}{1 + e^{-x}} \right) &lt;br /&gt;
&amp;amp;= \dfrac{1}{1 + e^{-x}\ } \cdot \left( 1 - \dfrac{1}{1 + e^{-x}} \right) &lt;br /&gt;
&amp;amp;= \sigma(x) \cdot (1 - \sigma(x))
\end{align}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{array}{l}\text { In the previous video, Andrew refers to } d z=a(1-a) \ \text { Note that Andrew is using &amp;ldquo;dz&amp;rdquo; as a shorthand to refer to } \frac{d a}{d z}=a(1-a) \text { . } \ \text { To clarify, earlier in this week&amp;rsquo;s videos, Andrew used the name &amp;ldquo;dz&amp;rdquo; to refer to a different derivative: } \frac{d L}{d z}=a-y . \ \text { Recall that the relationship between } \frac{d L}{d z} \text { and } \frac{d a}{d z} \text { is: } \ \frac{d L}{d z}=\frac{d L}{d a} \times \frac{d a}{d z} \ \frac{d L}{d z}=\frac{a-y}{a(1-a)} \times a(1-a)=a-y\end{array}
$$&lt;/p&gt;

&lt;h3 id=&#34;vectorization&#34;&gt;Vectorization&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200324232349586.png&#34; alt=&#34;image-20200324232349586&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200324233831522.png&#34; alt=&#34;image-20200324233831522&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200324234028558.png&#34; alt=&#34;image-20200324234028558&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325000709367.png&#34; alt=&#34;image-20200325000709367&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325000944011.png&#34; alt=&#34;image-20200325000944011&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;code&#34;&gt;Code&lt;/h3&gt;

&lt;p&gt;For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $&lt;em&gt;$ num_px $&lt;/em&gt;$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Exercise:&lt;/strong&gt; Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $&lt;em&gt;$ num_px $&lt;/em&gt;$ 3, 1).&lt;/p&gt;

&lt;p&gt;A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$&lt;em&gt;$c$&lt;/em&gt;$d, a) is to use:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;X_flatten = X.reshape(X.shape[0], -1).T      # X.T is the transpose of X
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.&lt;/p&gt;

&lt;p&gt;One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).&lt;/p&gt;

&lt;!-- During the training of your model, you&#39;re going to multiply weights and add biases to some initial inputs in order to observe neuron activations. Then you backpropogate with the gradients to train the model. But, it is extremely important for each feature to have a similar range such that our gradients don&#39;t explode. You will see that more in detail later in the lectures. !--&gt; 

&lt;p&gt;Let&amp;rsquo;s standardize our dataset.&lt;/p&gt;

&lt;h3 id=&#34;neural-network&#34;&gt;Neural Network&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325130254559.png&#34; alt=&#34;image-20200325130254559&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;shallow-neural-networks&#34;&gt;Shallow neural networks&lt;/h2&gt;

&lt;h3 id=&#34;representation&#34;&gt;Representation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329195140415.png&#34; alt=&#34;image-20200329195140415&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329195430112.png&#34; alt=&#34;image-20200329195430112&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329200220168.png&#34; alt=&#34;image-20200329200220168&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329201139101.png&#34; alt=&#34;image-20200329201139101&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Xï¼šç«–æ–¹å‘ features&lt;/p&gt;

&lt;p&gt;æ¨ªæ–¹å‘ï¼štraining example&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329203208413.png&#34; alt=&#34;image-20200329203208413&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329203423150.png&#34; alt=&#34;image-20200329203423150&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329205526293.png&#34; alt=&#34;image-20200329205526293&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Activation functionï¼š&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Sigmoid: å¦‚æœoutputæ˜¯binary classificationï¼Œå¯ä»¥è€ƒè™‘sigmoid functionï¼Œä¸€èˆ¬nerver used&lt;/li&gt;
&lt;li&gt;tanh functionï¼š&lt;/li&gt;
&lt;li&gt;ReLU Rectified Linear Unit $a = \max(0, z)$ : by default, å¾ˆå¤šäººç”¨ï¼Œfasterï¼Œslope 1ï¼Œor 0&lt;/li&gt;
&lt;li&gt;Leaky ReLU&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;why-need-non-linear-activation-functions&#34;&gt;Why need non-linear activation functions&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329211018395.png&#34; alt=&#34;image-20200329211018395&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;derivatives-of-activation-functions&#34;&gt;Derivatives of activation functions&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329213350341.png&#34; alt=&#34;image-20200329213350341&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329213514765.png&#34; alt=&#34;image-20200329213514765&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200329213812889.png&#34; alt=&#34;image-20200329213812889&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;gradient-descent-for-neural-networks&#34;&gt;Gradient descent for Neural Networks&lt;/h3&gt;

&lt;p&gt;np.sum(keepdims=True): prevent rank one arrays&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200406144044096.png&#34; alt=&#34;image-20200406144044096&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200406144013392.png&#34; alt=&#34;image-20200406144013392&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;backpropagation-intuition&#34;&gt;Backpropagation intuition&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200406144511159.png&#34; alt=&#34;image-20200406144511159&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200406145730947.png&#34; alt=&#34;image-20200406145730947&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60&#34; target=&#34;_blank&#34;&gt;https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200406152406973.png&#34; alt=&#34;image-20200406152406973&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.coursera.org/learn/neural-networks-deep-learning/discussions/weeks/3/threads/a38VuhyMEei5zw6yFhWyOg&#34; target=&#34;_blank&#34;&gt;https://www.coursera.org/learn/neural-networks-deep-learning/discussions/weeks/3/threads/a38VuhyMEei5zw6yFhWyOg&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;random-initialization&#34;&gt;Random Initialization&lt;/h3&gt;

&lt;p&gt;LR can be initialized as 0, but not neural network&lt;/p&gt;

&lt;h2 id=&#34;supplement&#34;&gt;Supplement&lt;/h2&gt;

&lt;h3 id=&#34;åŸºç¡€&#34;&gt;åŸºç¡€&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR;
MachineLearning--&amp;gt;é¢‘ç‡/ç»Ÿè®¡æœºå™¨å­¦ä¹ ;
é¢‘ç‡/ç»Ÿè®¡æœºå™¨å­¦ä¹ --&amp;gt;æ­£åˆ™åŒ–/L1/L2
é¢‘ç‡/ç»Ÿè®¡æœºå™¨å­¦ä¹ --&amp;gt;æ ¸åŒ–
é¢‘ç‡/ç»Ÿè®¡æœºå™¨å­¦ä¹ --&amp;gt;é›†æˆåŒ–
é¢‘ç‡/ç»Ÿè®¡æœºå™¨å­¦ä¹ --&amp;gt;å±‚æ¬¡åŒ–
å±‚æ¬¡åŒ–--&amp;gt;MultiayerPercepton/MLP
å±‚æ¬¡åŒ–--&amp;gt;Autoencoder
å±‚æ¬¡åŒ–--&amp;gt;CNN
å±‚æ¬¡åŒ–--&amp;gt;RNN
MachineLearning--&amp;gt;è´å¶æ–¯æ´¾/PGM
è´å¶æ–¯æ´¾/PGM--&amp;gt;BayesianNetwork/æœ‰å‘å›¾
è´å¶æ–¯æ´¾/PGM--&amp;gt;MarkovNetwork/æ— å‘å›¾
è´å¶æ–¯æ´¾/PGM--&amp;gt;MixedNetwork/æœ‰å‘å›¾æ— å‘å›¾
BayesianNetwork/æœ‰å‘å›¾--&amp;gt;DeepDirectedNetwork
DeepDirectedNetwork--&amp;gt;SigmoidBeliefNetwork
DeepDirectedNetwork--&amp;gt;VAE
DeepDirectedNetwork--&amp;gt;GAN
MarkovNetwork/æ— å‘å›¾--&amp;gt;DeepBoltzmannNetwork
MixedNetwork/æœ‰å‘å›¾æ— å‘å›¾--&amp;gt;DeepBeliefNetwork
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;ç»Ÿè®¡æœºå™¨å­¦ä¹ &#34;&gt;ç»Ÿè®¡æœºå™¨å­¦ä¹ &lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;æ­£åˆ™åŒ–ï¼šLoss Function + regularizer(L1/L2)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;æ ¸åŒ–: Kernel SVM&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;é›†æˆåŒ–: AdaBoost, RandomForest&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;å±‚æ¬¡åŒ–: Neural Network/Deep Neural Network&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;MLP(Mutilayer Perceptron)&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Autoencoder&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CNN&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;RNN&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;è´å¶æ–¯æ´¾&#34;&gt;è´å¶æ–¯æ´¾&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;BayesianNetwork $\Rightarrow$ Deep Directed Network

&lt;ul&gt;
&lt;li&gt;Sigmoid Belief Network&lt;/li&gt;
&lt;li&gt;Variational Autoencoder(VAE)&lt;/li&gt;
&lt;li&gt;GAN&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Markov Network  $\Rightarrow$ Deep Boltzmann Network&lt;/li&gt;
&lt;li&gt;Mixed Netowork $\Rightarrow$ Deep Belief Network&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;ä¸Šè¿°éƒ½æ˜¯ï¼šDeep Generative Model&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ç‹­ä¹‰çš„DeepLearningï¼šDeep Neural Network&lt;/p&gt;

&lt;p&gt;å…¶å®åº”è¯¥åŒ…æ‹¬ï¼š&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Deep Neural Network&lt;/li&gt;
&lt;li&gt;Deep Generative Modelï¼ˆå½“å±‚æ¬¡éå¸¸å¤šï¼Œæ¨æ–­éå¸¸å›°éš¾ï¼‰&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;æ—¶é—´çº¿&#34;&gt;æ—¶é—´çº¿&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325141506141.png&#34; alt=&#34;image-20200325141506141&#34; /&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;æ·±åº¦å­¦ä¹ çš„ç†è®ºåœ¨2006å¹´å·²ç»æˆå‹ï¼Œç›´åˆ°ç°åœ¨ï¼Œç†è®ºå¹¶æ²¡æœ‰æ ¹æœ¬æ€§çªç ´ã€‚&lt;/li&gt;
&lt;li&gt;ä¸ºä»€ä¹ˆtake off

&lt;ol&gt;
&lt;li&gt;data&lt;/li&gt;
&lt;li&gt;åˆ†å¸ƒå¼&lt;/li&gt;
&lt;li&gt;ç¡¬ä»¶ GPU&lt;/li&gt;
&lt;li&gt;æ•ˆæœ&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325142443790.png&#34; alt=&#34;image-20200325142443790&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;non-linear-problem&#34;&gt;Non-Linear Problem&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325144411294.png&#34; alt=&#34;image-20200325144411294&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;neural-network-1&#34;&gt;Neural Network&lt;/h4&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325144959585.png&#34; alt=&#34;image-20200325144959585&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325145230541.png&#34; alt=&#34;image-20200325145230541&#34; /&gt;&lt;/p&gt;

&lt;p&gt;ç¬¦åˆè¿ç®—-ã€‹å¤åˆè¡¨è¾¾å¼-ã€‰å¤åˆå‡½æ•°&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325150148814.png&#34; alt=&#34;image-20200325150148814&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;äººå·¥æ™ºèƒ½-ä¸¤å¤§é˜µè¥-ä¸‰å¤§ä¸»ä¹‰&#34;&gt;äººå·¥æ™ºèƒ½ ä¸¤å¤§é˜µè¥ ä¸‰å¤§ä¸»ä¹‰&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200325143542849.png&#34; alt=&#34;image-20200325143542849&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;deep-learning&#34;&gt;Deep learning&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419103358781.png&#34; alt=&#34;image-20200419103358781&#34; /&gt;&lt;/p&gt;

&lt;p&gt;HMM-GMM æ¯ä¸ªmodelå°±æ˜¯ä¸€ä¸ªstateï¼Œä½†æ˜¯DNNå°±æ˜¯ä¸€ä¸ªå¤§çš„modelã€‚æ‰€æœ‰çš„stateå…±ç”¨ä¸€ä¸ªDNNã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419103911142.png&#34; alt=&#34;image-20200419103911142&#34; /&gt;&lt;/p&gt;

&lt;p&gt;æ›´åŠ å¤šå±‚çš„DNNï¼Œå…¶å®ä¼šæ›´å°‘neuronsï¼Œæœ€ç»ˆæ˜¯æ›´å°‘çš„å‚æ•°ï¼Œä¸å®¹æ˜“overfiting&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419104310591.png&#34; alt=&#34;image-20200419104310591&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å¤šå±‚å…¶å®å°±æ˜¯å°†ç©ºé—´å¯¹æŠ˜äº†ã€‚&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419104517053.png&#34; alt=&#34;image-20200419104517053&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419105241175.png&#34; alt=&#34;image-20200419105241175&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419105811266.png&#34; alt=&#34;image-20200419105811266&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200419105822655.png&#34; alt=&#34;image-20200419105822655&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HMM</title>
      <link>https://faithio.cn/post/hmm/</link>
      <pubDate>Tue, 24 Mar 2020 16:45:50 -0400</pubDate>
      <guid>https://faithio.cn/post/hmm/</guid>
      <description>

&lt;p&gt;[pdf]&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;./hmm.html&#34;&gt;click here for link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;æ¦‚ç‡å›¾æ¨¡å‹&#34;&gt;æ¦‚ç‡å›¾æ¨¡å‹&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;inference-&amp;gt; $P(Z|X)$-&amp;gt;ç§¯åˆ†é—®é¢˜ï¼ˆMCMCï¼‰&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;GMMï¼šæ ·æœ¬ä¹‹é—´æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒ&lt;/p&gt;

&lt;h3 id=&#34;hmm-dynamic-model&#34;&gt;HMM: Dynamic Model&lt;/h3&gt;

&lt;p&gt;y: System state éšå˜é‡&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;state  ç¦»æ•£ï¼šHMM&lt;/li&gt;
&lt;li&gt;state çº¿æ€§: Kalman Filter&lt;/li&gt;
&lt;li&gt;state éçº¿æ€§: Particle Filter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
\lambda = (\underbrace{\pi}{åˆå§‹prob dist}, \underbrace{A}{çŠ¶æ€è½¬ç§»çŸ©é˜µ}, \underbrace{B}_{å‘å°„çŸ©é˜µemission}) &lt;br /&gt;
çŠ¶æ€å˜é‡i: i_1, i_2, \cdots i_t \cdots \rightarrow Q={q_1, q_2, \cdots, q_m} &lt;br /&gt;
è§‚æµ‹å˜é‡o: o_1, o_2, \cdots o_t \cdots \rightarrow V={v_1, v_2, \cdots, v&lt;em&gt;m} &lt;br /&gt;
A=\left[a&lt;/em&gt;{i j}\right], a&lt;em&gt;{i j}=P\left(i&lt;/em&gt;{t+1}=q_{i} | i&lt;em&gt;t=q&lt;/em&gt;{i}\right)  &lt;br /&gt;
B=\left[b&lt;em&gt;{j k} \right], b&lt;/em&gt;{j k}=P\left(Q&lt;em&gt;{t}=v&lt;/em&gt;{k} | i&lt;em&gt;t=q&lt;/em&gt;{j}\right)
$$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;transition å’Œ emission probability æ˜¯independent&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ä¸¤ä¸ªå‡è®¾ï¼š&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;é½æ¬¡Markov å‡è®¾&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;$$
P(i_{t+1}|i&lt;em&gt;t, t&lt;/em&gt;{t-1}, \cdots, t_1, o&lt;em&gt;t, o&lt;/em&gt;{t-1}, \dots, o&lt;em&gt;1) = p(i&lt;/em&gt;{t+1}|i_t)  &lt;br /&gt;
$$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;è§‚å¯Ÿç‹¬ç«‹å‡è®¾
$$
P(o_{t}|i&lt;em&gt;t, t&lt;/em&gt;{t-1}, \cdots, t_1, o&lt;em&gt;t, o&lt;/em&gt;{t-1}, \dots, o&lt;em&gt;1) = p(o&lt;/em&gt;{t}|i_t)  &lt;br /&gt;
$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ä¸‰ä¸ªé—®é¢˜ï¼š&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Evaluation: $P(O| \lambda) \Rightarrow$ å‰å‘åå‘ Forward-backward&lt;/li&gt;
&lt;li&gt;learning $\lambda=\arg \max P(O|\lambda)$ EM algorithm\baum welch&lt;/li&gt;
&lt;li&gt;Decoding $\lambda=\arg \max_{i} P(I|O)$

&lt;ol&gt;
&lt;li&gt;é¢„æµ‹ï¼š$P(i_{t+1}|o_1, o_2, \cdots, o_t)$&lt;/li&gt;
&lt;li&gt;æ»¤æ³¢ï¼š$P(i_{t}|o_1, o_2, \cdots, o_t)$&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;hmm-evaluation&#34;&gt;HMM-Evaluation&lt;/h3&gt;

&lt;p&gt;$$
\begin{aligned}
 Give &amp;amp;\lambda, æ±‚ P(O|\lambda) &lt;br /&gt;
P(O | \lambda) &amp;amp;=\sum&lt;em&gt;{1} P(I, O | \lambda)=\sum&lt;/em&gt;{1} P(O | I, \lambda) \cdot P(I | \lambda) \&lt;/p&gt;

&lt;p&gt;P(I | \lambda)&amp;amp;=P\left(i&lt;em&gt;{1}, i&lt;/em&gt;{2}, \cdots,  i&lt;em&gt;{T} | \lambda\right)=\underbrace{P\left(i&lt;/em&gt;{T} | i, i&lt;em&gt;{2},\cdots,i&lt;/em&gt;{T-1}, \lambda\right)}_{P(i&lt;em&gt;T|i&lt;/em&gt;{T-1})=a&lt;em&gt;{i&lt;/em&gt;{T-1}, i_T}} P(i_1, i&lt;em&gt;2, \cdots, i&lt;/em&gt;{T-1}|\lambda) = a&lt;em&gt;{i&lt;/em&gt;{T-1}, i&lt;em&gt;T} \cdot a&lt;/em&gt;{i&lt;em&gt;{T-2}, i&lt;/em&gt;{T-1}} \cdots a&lt;em&gt;{i&lt;/em&gt;{1}, i_{2}} \cdot \pi(i_1) \&lt;/p&gt;

&lt;p&gt;&amp;amp;\pi\text{ æ˜¯åˆå§‹åˆ†å¸ƒ} &lt;br /&gt;
&amp;amp;= \pi\left(a&lt;em&gt;{i}\right) \cdot \prod&lt;/em&gt;{t=2}^{T} a&lt;em&gt;{i&lt;/em&gt;{t-1}, i&lt;em&gt;t} &lt;br /&gt;
&amp;mdash;-&lt;br /&gt;
p(O | I, \lambda) &amp;amp; =\prod&lt;/em&gt;{t=1}^{T} b_{i&lt;em&gt;t}\left(O&lt;/em&gt;{t}\right)
\end{aligned}
$$&lt;/p&gt;

&lt;h3 id=&#34;ppt&#34;&gt;PPT&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;1.jpg&#34; alt=&#34;image-20200324164641801&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NLP</title>
      <link>https://faithio.cn/post/nlp/</link>
      <pubDate>Tue, 24 Mar 2020 12:46:09 -0400</pubDate>
      <guid>https://faithio.cn/post/nlp/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5hyy8e3dj30iz0dxtdq.jpg&#34; alt=&#34;image-20200324124629145&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5ijwtti0j30j50db42h.jpg&#34; alt=&#34;image-20200324130638081&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;è¯­éŸ³è¯†åˆ«&#34;&gt;è¯­éŸ³è¯†åˆ«&lt;/h2&gt;

&lt;h3 id=&#34;hmm&#34;&gt;HMM&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5npq72s0j30gr0cq41w.jpg&#34; alt=&#34;image-20200324160513581&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5nt18l2uj30it0cwjv0.jpg&#34; alt=&#34;image-20200324160824714&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5nw4kccfj30jc0efgqe.jpg&#34; alt=&#34;image-20200324161121196&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5nz90vi1j30i60dbdji.jpg&#34; alt=&#34;image-20200324161422230&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5o4zk81bj30ik0dudkc.jpg&#34; alt=&#34;image-20200324161953002&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;method-1-tandem&#34;&gt;Method 1:Tandem&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5o5qhqshj30g109gta7.jpg&#34; alt=&#34;image-20200324162036664&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;method-2-dnn-hmm-hybrid&#34;&gt;Method 2: DNN-HMM Hybrid&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5ock6amnj30h70cjgoj.jpg&#34; alt=&#34;image-20200324162710364&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gd5ohregnxj30jh0eiq75.jpg&#34; alt=&#34;image-20200324163209107&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Markov Chain Monte Carlo</title>
      <link>https://faithio.cn/post/markov-chain-monte-carlo/</link>
      <pubDate>Tue, 24 Mar 2020 01:38:20 -0400</pubDate>
      <guid>https://faithio.cn/post/markov-chain-monte-carlo/</guid>
      <description>

&lt;h3 id=&#34;æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·-standard-distributions&#34;&gt;æ¦‚ç‡åˆ†å¸ƒé‡‡æ · &lt;strong&gt;Standard distributions&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;1.jpg&#34; alt=&#34;image-20200324020445663&#34; /&gt;&lt;/p&gt;

&lt;p&gt;PDFå¦‚æœå¾ˆå¤æ‚ï¼ŒCDFæ±‚ä¸å‡ºæ¥ã€‚&lt;/p&gt;

&lt;h3 id=&#34;rejection-sampling&#34;&gt;Rejection Sampling&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;2.jpg&#34; alt=&#34;image-20200324021231280&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;importance-sampling&#34;&gt;Importance Sampling&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;3.jpg&#34; alt=&#34;image-20200324022419484&#34; /&gt;&lt;/p&gt;

&lt;p&gt;å› ä¸ºå¤§éƒ¨åˆ†é‡‡æ ·å¾—åˆ°çš„æ ·æœ¬é‡è¦æ€§å¾ˆä½ï¼Œåä¹‹ä»…æœ‰å°‘é‡çš„æ ·æœ¬é‡è¦æ€§éå¸¸å¤§ã€‚&lt;/p&gt;

&lt;p&gt;Sampling Importance Resampling&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;4.jpg&#34; alt=&#34;image-20200324023300041&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;monte-carlo-method&#34;&gt;Monte Carlo Method&lt;/h3&gt;

&lt;p&gt;åŸºäºéšæœºé‡‡æ ·çš„è¿‘ä¼¼æ–¹æ³•&lt;/p&gt;

&lt;p&gt;Markov Chain: æ—¶é—´çŠ¶æ€éƒ½æ˜¯ç¦»æ•£çš„ã€‚ç‰¹æ®Šçš„éšæœºè¿‡ç¨‹&lt;/p&gt;

&lt;p&gt;é½æ¬¡ï¼ˆä¸€é˜¶ï¼‰Markov Chainï¼šæœªæ¥åªä¾èµ–äºå½“å‰ï¼Œå’Œè¿‡å»æ²¡æœ‰å…³ç³» ${x&lt;em&gt;t}$ Pä¸ºè½¬ç§»çŸ©é˜µ$[P&lt;/em&gt;{ij}]$
$$
P(X_{t+1} = x| x_1, x_2, \cdots, x&lt;em&gt;t) = P(X&lt;/em&gt;{t+1}|x_t)
$$&lt;/p&gt;

&lt;p&gt;$$
P&lt;em&gt;{i j} \quad P&lt;/em&gt;{i j}=P\left(X&lt;em&gt;{i+1}=j | X&lt;/em&gt;{t=i}\right)
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR;
x1--&amp;gt;x2;
x2--&amp;gt;x3;
x3--&amp;gt;xt;
xt--&amp;gt;xt+1;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å¹³ç¨³åˆ†å¸ƒ&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;5.jpg&#34; alt=&#34;image-20200324115614383&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gaussian Mixture Model</title>
      <link>https://faithio.cn/post/gaussian-mixture-model/</link>
      <pubDate>Mon, 23 Mar 2020 23:42:44 -0400</pubDate>
      <guid>https://faithio.cn/post/gaussian-mixture-model/</guid>
      <description>

&lt;p&gt;æ··åˆæ¨¡å‹éƒ½æ˜¯ç”Ÿæˆæ¨¡å‹ï¼ŒNä¸ªæ ·æœ¬ï¼Œå…ˆç”Ÿæˆx1ï¼Œx2&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;1.jpg&#34; alt=&#34;image-20200323235058567&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;2.jpg&#34; alt=&#34;image-20200323235323227&#34; /&gt;
$$
\begin{aligned}
p(x) &amp;amp;= \sum&lt;em&gt;z P(x,z) &lt;br /&gt;
&amp;amp;= \sum&lt;/em&gt;{k=1}^z P(x, z=C&lt;em&gt;k) &lt;br /&gt;
&amp;amp;= \sum&lt;/em&gt;{k=1}^z p(z=C_k) \cdot p(x|z=C&lt;em&gt;k) &lt;br /&gt;
&amp;amp;= \sum&lt;/em&gt;{k=1}^z p_k \mathcal{N}(x| \mu_k, \Sigma_k)
\end{aligned}
$$
&lt;img src=&#34;3.jpg&#34; alt=&#34;image-20200324001130297&#34; /&gt;
$$
\log (\Delta + \Delta + \Delta ) \rightarrow \text{è¿åŠ ç¬¦å·å¾ˆéš¾æ±‚å¯¼ï¼Œä»¤å¯¼æ•°ä¸º0ï¼Œè¿ä¹˜æ¯”è¾ƒæ–¹ä¾¿} &lt;br /&gt;
\text{å•å˜é‡çš„é«˜æ–¯åˆ†å¸ƒå¯ä»¥ç›´æ¥ç”¨MLEæ±‚å‡ºæ¥}
$$&lt;/p&gt;

&lt;h3 id=&#34;ç”¨emæ±‚è§£&#34;&gt;ç”¨EMæ±‚è§£&lt;/h3&gt;

&lt;p&gt;$$
\text{ E-Step: } P\left(z | x, \theta^{(t)}\right) \rightarrow E_{z|x, \theta^{(t)}}[\log P(x, z | \theta)] \&lt;/p&gt;

&lt;p&gt;\text{ M-Step: } \theta^{(t+1)}=\arg \max &lt;em&gt;{\theta} \underbrace{ E&lt;/em&gt;{z|x, \theta^{(t)}}[\log P(x, z | \theta)]}_{Q(\theta, \theta^{(t)})} \&lt;/p&gt;

&lt;p&gt;\text{E-step:} \&lt;/p&gt;

&lt;p&gt;Q(\theta, \theta^{(t)}) = \int_z \log P(X, Z|\theta) P(Z|X, \theta^{(t)}) dz &lt;br /&gt;
 \text{ç‹¬ç«‹åŒåˆ†å¸ƒ} &lt;br /&gt;
 = \sum&lt;em&gt;z \log \prod&lt;/em&gt;{i=1}^N P(x_i, z_i|\theta) \cdot \sum&lt;em&gt;z \log \prod&lt;/em&gt;{i=1}^N    P(z_i|x&lt;em&gt;i, \theta^{(t)})&lt;br /&gt;
 =\sum&lt;/em&gt;{z_{1} z&lt;em&gt;2 \cdots z&lt;/em&gt;{N}} \sum&lt;em&gt;{i=1}^{N} \log P\left(x&lt;/em&gt;{i} z&lt;em&gt;{i} | \theta\right) \cdot \prod&lt;/em&gt;{i=1}^{N} p\left(z&lt;em&gt;{i} | x&lt;/em&gt;{i}, \theta^{(t)}\right) \&lt;/p&gt;

&lt;p&gt;=\sum&lt;em&gt;{z&lt;/em&gt;{1} z&lt;em&gt;2 \cdots z&lt;/em&gt;{N}} \left[\log P\left(x&lt;em&gt;{1}, z&lt;/em&gt;{1} | \theta\right)+\log P\left(x&lt;em&gt;{2}, z&lt;/em&gt;{2} | \theta\right)+\cdots+\log P\left(x&lt;em&gt;{N}, z&lt;/em&gt;{N} | \theta\right)\right] \cdot \prod&lt;em&gt;{i=1}^{N} p\left(z&lt;/em&gt;{i} | x&lt;em&gt;{i}, \theta^{(t)}\right) &lt;br /&gt;
 = \sum&lt;/em&gt;{z&lt;em&gt;{1}} \log p\left(x&lt;/em&gt;{1}, z&lt;em&gt;{1} | \theta\right) \cdot p\left(z&lt;/em&gt;{1} | x&lt;em&gt;{1}, \theta\right) + \cdots + \sum&lt;/em&gt;{z&lt;em&gt;{N}} \log p\left(x&lt;/em&gt;{N}, z&lt;em&gt;{N} | \theta\right) \cdot p\left(z&lt;/em&gt;{N} | x&lt;em&gt;{N}, \theta\right) &lt;br /&gt;
 =\sum&lt;/em&gt;{i=1}^{N} \sum_{z&lt;em&gt;i} \log p\left(x&lt;/em&gt;{i}, z&lt;em&gt;{i} | \theta\right) \cdot p\left(z&lt;/em&gt;{i} | x_{i}, \theta^{(i)}\right) \&lt;/p&gt;

&lt;p&gt;=\sum&lt;em&gt;{i=1}^{N} \sum&lt;/em&gt;{z&lt;em&gt;{i}} \log p&lt;/em&gt;{z&lt;em&gt;{i}} \mathcal{N}\left(x&lt;/em&gt;{i} | \mu&lt;em&gt;{z&lt;/em&gt;{i}} \Sigma&lt;em&gt;{z&lt;/em&gt;{i}}\right) \cdot \frac{p&lt;em&gt;{z&lt;/em&gt;{i}} \cdot \mathcal{N}\left(x&lt;em&gt;{i} | \mu&lt;/em&gt;{i}, \Sigma&lt;em&gt;{i}\right)}{\sum&lt;/em&gt;{k=1}^{K} p&lt;em&gt;{x} \mathcal{N}\left(x&lt;/em&gt;{i} | \mu_{k}, \Sigma&lt;em&gt;k\right)} &lt;br /&gt;
 &amp;mdash; &lt;br /&gt;
 \begin{aligned} P(x, z) &amp;amp;=P(z) \cdot p(x | z) \ &amp;amp;=p&lt;/em&gt;{z} \cdot N\left(x | \mu&lt;em&gt;{z}, z&lt;/em&gt;{z}\right) \ p(z | x)
 &amp;amp;=\frac{p(x, z)}{p(x)}=\frac{p&lt;em&gt;{z} \cdot \mathcal{N}\left(x | \mu&lt;/em&gt;{z} \Sigma&lt;em&gt;{z}\right)}{\sum&lt;/em&gt;{k=1}^{K} p&lt;em&gt;{k} \cdot \mathcal{N}\left(x | \mu&lt;/em&gt;{k}, \Sigma_{k}\right)} \end{aligned}
$$&lt;/p&gt;

&lt;p&gt;$$
\text{å–ç¬¬ä¸€é¡¹}
\sum&lt;em&gt;{z&lt;/em&gt;{1} z&lt;em&gt;2 \cdots z&lt;/em&gt;{N}} \log  p\left(x&lt;em&gt;{1}, z&lt;/em&gt;{1} |\theta\right)   \cdot   \underbrace{\prod&lt;em&gt;{i=1}^{N} p\left(z&lt;/em&gt;{i} | x&lt;em&gt;{i}, \theta^{(t)}\right)}&lt;/em&gt;{
p\left(z_1,\left|x&lt;em&gt;1, \theta^{(t)}\right) \cdot \prod&lt;/em&gt;{i=2}^{N} p\left(z&lt;em&gt;{i} | x&lt;/em&gt;{i}, \theta^{(t)}\right)\right.
}   \&lt;/p&gt;

&lt;p&gt;=\sum&lt;em&gt;{z&lt;/em&gt;{1}} \log p\left(x&lt;em&gt;{1}, z&lt;/em&gt;{1} | \theta\right) \cdot p\left(z&lt;em&gt;{1} | x&lt;/em&gt;{1}, \theta\right) \sum_{z_2 \cdots z&lt;em&gt;N} \prod&lt;/em&gt;{i=2}^{N} p\left(z&lt;em&gt;{i} | x&lt;/em&gt;{i}, \theta^{(t)}\right) &lt;br /&gt;
= \sum&lt;em&gt;{z&lt;/em&gt;{1}} \log p\left(x&lt;em&gt;{1}, z&lt;/em&gt;{1} | \theta\right) \cdot p\left(z&lt;em&gt;{1} | x&lt;/em&gt;{1}, \theta\right) &lt;br /&gt;
&amp;mdash;-  &lt;br /&gt;
 \prod&lt;em&gt;{i=2}^{N} p\left(z&lt;/em&gt;{i} | x&lt;em&gt;{i}, \theta^{(t)}\right) =  \prod&lt;/em&gt;{i=2}^{N} P(z_2|x_2)P(z_3|x_3)\cdots P(z_N|x&lt;em&gt;N) &lt;br /&gt;
 = \sum&lt;/em&gt;{z_2}P(z_2|x&lt;em&gt;2)\cdot \sum&lt;/em&gt;{z_3}P(z_3|x&lt;em&gt;3)\cdot \cdots \sum&lt;/em&gt;{z_N}P(z_N|x_N) &lt;br /&gt;
 = 1
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;E-Step:
$$
 \text{ M-Step: } \theta^{(t+1)}=\arg \max &lt;em&gt;{\theta} \underbrace{ E&lt;/em&gt;{z|x, \theta^{(t)}}[\log P(x, z | \theta)]}_{Q(\theta, \theta^{(t)})} &lt;br /&gt;
\begin{aligned}
Q(\theta, \theta^{(t)}) &amp;amp;= \int&lt;em&gt;z \log P(X, Z|\theta) P(Z|X, \theta^{(t)}) dz &lt;br /&gt;
&amp;amp;=\sum&lt;/em&gt;{i=1}^{N} \sum&lt;em&gt;{z&lt;/em&gt;{i}} \log p&lt;em&gt;{z&lt;/em&gt;{i}} \mathcal{N}\left(x&lt;em&gt;{i} | \mu&lt;/em&gt;{z&lt;em&gt;{i}} \Sigma&lt;/em&gt;{z&lt;em&gt;{i}}\right) \cdot  \underbrace{  \frac{p&lt;/em&gt;{z&lt;em&gt;{i}} \cdot \mathcal{N}\left(x&lt;/em&gt;{i} | \mu&lt;em&gt;{i}, \Sigma&lt;/em&gt;{i}\right)}{\sum&lt;em&gt;{k=1}^{K} p&lt;/em&gt;{x} \mathcal{N}\left(x&lt;em&gt;{i} | \mu&lt;/em&gt;{k}, \Sigma&lt;em&gt;k\right)} }&lt;/em&gt;{P(z_i|x_i, \theta^{(t)}) \rightarrow \theta^{(t)} \text{ is a constant.}} \&lt;/p&gt;

&lt;p&gt;&amp;amp;= \int&lt;em&gt;z \log P(X, Z|\theta) P(Z|X, \theta^{(t)}) dz &lt;br /&gt;
&amp;amp;=\sum&lt;/em&gt;{i=1}^{N} \sum&lt;em&gt;{z&lt;/em&gt;{i}} \log [p&lt;em&gt;{z&lt;/em&gt;{i}} \mathcal{N}\left(x&lt;em&gt;{i} | \mu&lt;/em&gt;{z&lt;em&gt;{i}} \Sigma&lt;/em&gt;{z_{i}}\right)] \cdot P(z_i|x&lt;em&gt;i, \theta^{(t)}) &lt;br /&gt;
&amp;amp;= \sum&lt;/em&gt;{z&lt;em&gt;{i}} \sum&lt;/em&gt;{i=1}^{N} \log [p&lt;em&gt;{z&lt;/em&gt;{i}} \mathcal{N}\left(x&lt;em&gt;{i} | \mu&lt;/em&gt;{z&lt;em&gt;{i}} \Sigma&lt;/em&gt;{z_{i}}\right)] \cdot P(z_i|x&lt;em&gt;i, \theta^{(t)}) &lt;br /&gt;
&amp;amp;= \sum&lt;/em&gt;{k=1}^{k} \sum&lt;em&gt;{i=1}^{N} \log [p&lt;/em&gt;{k} \cdot \mathcal{N}\left(x&lt;em&gt;{i} | \mu&lt;/em&gt;{k}, \Sigma&lt;em&gt;{k}\right)] \cdot p\left(z&lt;/em&gt;{i}=C&lt;em&gt;{k} | x&lt;/em&gt;{i}, \theta^{(t)}\right) \&lt;/p&gt;

&lt;p&gt;&amp;amp;= \sum&lt;em&gt;{k=1}^{k} \sum&lt;/em&gt;{i=1}^{N} [\log p&lt;em&gt;{k} + \log \mathcal{N}\left(x&lt;/em&gt;{i} | \mu&lt;em&gt;{k}, \Sigma&lt;/em&gt;{k}\right)] \cdot p\left(z&lt;em&gt;{i}=C&lt;/em&gt;{k} | x_{i}, \theta^{(t)}\right) &lt;br /&gt;
\end{aligned}
$$
æ±‚ $p_k^{t+1} = (p_1^{t+1}, p_2^{t+1}, \cdots, p&lt;em&gt;k^{t+1})$
$$
\left{
\begin{array}{l}
p&lt;/em&gt;{k}^{(k+1)}=\operatorname{argmax}&lt;em&gt;{p&lt;/em&gt;{k}} \sum&lt;em&gt;{k=1}^{k} \sum&lt;/em&gt;{i=1}^{N} \log p&lt;em&gt;{k} \cdot p\left(z&lt;/em&gt;{i} = C&lt;em&gt;k | x&lt;/em&gt;{i}, \theta^{(t)}\right) &lt;br /&gt;
s.t. \sum&lt;em&gt;{k=1}^{k} p&lt;/em&gt;{k}=1
\end{array}
\right.
$$
ç”¨æ‹‰æ ¼æœ—æ—¥ä¹˜å­æ³•ï¼š
$$
\mathcal{L}(p, \lambda)=\sum&lt;em&gt;{k=1}^{K} \sum&lt;/em&gt;{i=1}^{N} \log p&lt;em&gt;{k} \cdot p\left(z&lt;/em&gt;{i}=C&lt;em&gt;{k} | x&lt;/em&gt;{i}, \theta^{(t)}\right)+\lambda(\sum_{k=1}^k - 1)
$$&lt;/p&gt;

&lt;p&gt;$$
\frac{\partial \mathcal{L}}{\partial p&lt;em&gt;{k}}= \sum&lt;/em&gt;{i=1}^{N} \frac{1}{p&lt;em&gt;{k}} \cdot p\left(z&lt;/em&gt;{i}=C&lt;em&gt;k | x&lt;/em&gt;{i} \cdot \theta^{(k)}\right)+\lambda \triangleq 0 &lt;br /&gt;
\rightarrow \sum&lt;em&gt;{i=1}^{N}  \cdot p\left(z&lt;/em&gt;{i}=C&lt;em&gt;k | x&lt;/em&gt;{i} \cdot \theta^{(k)}\right)+p_k \lambda = 0 \&lt;/p&gt;

&lt;p&gt;\Rightarrow^{(k=1,\cdots , K)} \sum&lt;em&gt;{i=1}^{N} \underbrace{\sum&lt;/em&gt;{i=1}^K  \cdot p\left(z_{i}=C&lt;em&gt;k | x&lt;/em&gt;{i} \cdot  \theta^{(k)}\right)}&lt;em&gt;1 + \underbrace{\sum&lt;/em&gt;{i=1}^K p_k}_1 \lambda &lt;br /&gt;
\Rightarrow N+ \lambda = 0 &lt;br /&gt;
\Rightarrow  \lambda = -N &lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;$$
p&lt;em&gt;{k}^{(t+1)}=\frac{1}{N} \sum&lt;/em&gt;{i=1}^{N} p\left(z&lt;em&gt;{i}=C&lt;/em&gt;{k} | x_{i}, \theta^{(t)}\right) &lt;br /&gt;
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Expectation Maximization</title>
      <link>https://faithio.cn/post/expectation-maximization/</link>
      <pubDate>Thu, 19 Mar 2020 23:55:34 -0400</pubDate>
      <guid>https://faithio.cn/post/expectation-maximization/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;./em.html&#34;&gt;click here for link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;$MLE: P(X|\theta)$&lt;/p&gt;

&lt;p&gt;$$
\theta_{MLE}=\arg \max _{\theta} \log P(X | \theta) \&lt;/p&gt;

&lt;p&gt;\theta^{(t+1)}=\arg \max &lt;em&gt;{\theta} \int&lt;/em&gt;{z} \log p(x, z | \theta) \cdot p\left(z | x, \theta^{(t)}\right) d z
&lt;br /&gt;
= \arg \max &lt;em&gt;{\theta} E&lt;/em&gt;{\mathbf{Z} | x \theta^{t}}[\log P(x, z | \theta)]  \&lt;/p&gt;

&lt;p&gt;\text{ E-Step: } P\left(z | x, \theta^{(t)}\right) \rightarrow E_{z|x, \theta^{(t)}}[\log P(x, z | \theta)] \&lt;/p&gt;

&lt;p&gt;\text{ M-Step: } \theta^{(t+1)}=\arg \max &lt;em&gt;{\theta} E&lt;/em&gt;{z|x, \theta^{(t)}}[\log P(x, z | \theta)]
$$&lt;/p&gt;

&lt;p&gt;éœ€è¦è¯æ˜ä¸€å®šä¼šæ”¶æ•›Convergence
$$
\theta^{(t)} \rightarrow \theta^{(t+1)} &lt;br /&gt;
\log p\left(x | \theta^{(t)}\right) \leqslant \log p\left(x | \theta^{(t+1)}\right) &lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;$$
\frac{P(x, z | \theta)}{P(z | x)}  \&lt;/p&gt;

&lt;p&gt;\log P(x | \theta)=\log P(x, z | \theta)-\log P(z | x, \theta) \&lt;/p&gt;

&lt;p&gt;\begin{aligned}
left &amp;amp;=\int&lt;em&gt;{z} p\left(z | x, \theta^{(t)}\right) \cdot \log p(x | \theta) d z
\ &amp;amp;=\log P(x | \theta) \underbrace{ \int&lt;/em&gt;{z} P\left(z | x, \theta^{(t)}\right) d z }_1
\ &amp;amp;= \log P(x | \theta)
\end{aligned}  \&lt;/p&gt;

&lt;p&gt;right
=\underbrace{\int&lt;em&gt;{\mathbb{Z}} p\left(z | x, \theta^{(t)}\right) \cdot \log P(x, z | \theta) d z}&lt;/em&gt;{Q\left(\theta, \theta^{(t)}\right)}
-\underbrace{\int&lt;em&gt;{\mathbb{Z}} p\left(z | x, \theta^{(t)}\right) \cdot \log p(z | x, \theta) d z}&lt;/em&gt;{H\left(\theta, \theta^{(t)}\right)}\&lt;/p&gt;

&lt;p&gt;Q\left(\theta^{(t+1)}, \theta^{(t)}\right) \geqslant Q\left(\theta^{(t)}, \theta^{(t)}\right) \&lt;/p&gt;

&lt;p&gt;\begin{aligned}
&amp;amp; H\left(\theta^{(t+1)} \cdot \theta^{(t)}\right)-H\left(\theta^{(t)} \cdot \theta^{(t)}\right)
\ &amp;amp;=\int&lt;em&gt;{z} p\left(z | x, \theta^{(t)}\right) \cdot \log p\left(z | x \theta^{(t+1)}\right) d z
\ &amp;amp;-\int&lt;/em&gt;{z} p\left(z | x, \theta^{(t)}\right) \cdot \log p\left(z | x, \theta^{(t)}\right)  d z
\&amp;amp; = \int_{z} P\left(z | x, \theta^{(t)}\right) \cdot \log \frac{p\left(z | x, \theta^{(t+1)}\right)}{p\left(z | x, \theta^{(t)}\right)} d z &lt;br /&gt;
\ &amp;amp; = -\mathcal{KL}\left(P(z | x, \theta^{(t)}) | p\left(z | x, \theta^{(t+1)}\right) \right)
\ &amp;amp;  \leqslant 0&lt;/p&gt;

&lt;p&gt;\end{aligned} \&lt;/p&gt;

&lt;p&gt;\begin{aligned}
&amp;amp; E[\ln x] \leqslant \log E[x]
\ \leqslant \log \underbrace{\int_{z} p\left(z|x, \theta^{(t+ 1)}\right) d z}_1=\log 1=0&lt;/p&gt;

&lt;p&gt;\end{aligned}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;$$
p(x, z)=p(z | x) \cdot p(x) &lt;br /&gt;
\log p(x) = \log \frac{p(x, z)}{p(z | x) }  =  \log p(x, z) - \log p(z | x)&lt;br /&gt;
add \quad \theta &lt;br /&gt;
&amp;mdash;&lt;br /&gt;
\log P(x | \theta)=\log P(x, z | \theta)-\log P(z | x, \theta) &lt;br /&gt;
  =\log \frac{P(x, z | \theta)}{q(z)}-\log \frac{P(z | x, \theta)}{q(z)}  \quad q(z) \neq 0 &lt;br /&gt;
  \text{å·¦å³ä¸¤è¾¹å¯¹äº} q(z) \text{æ±‚æœŸæœ›} &lt;br /&gt;
  left = \int&lt;em&gt;{z} q(z) \cdot \log p(x | \theta) d z=\log p(x | \theta) \cdot \underbrace{\int&lt;/em&gt;{z} q(z) d z}&lt;em&gt;{1}=\log p(x | \theta) &lt;br /&gt;
  rigth = \underbrace{ \int&lt;/em&gt;{z} q(z) \log \frac{p(x, z | \theta)}{q(z)} d z}&lt;em&gt;{ELBO: evidence lower bound} \underbrace{ -
  \int&lt;/em&gt;{z} q(z) \log \frac{p(z | x, \theta)}{q(z)} d z}_{\mathcal{KL}(q(z) | p(z | x, \theta)}  &lt;br /&gt;
  \log p(x | \theta)=ELBO+ KL(q|p) &lt;br /&gt;
  \log p(x | \theta) \geqslant ELBO &lt;br /&gt;
  \text{maximize ELBO, then posterior maximize, æœ€å¤§åŒ–ELBO(æœŸæœ›)ï¼Œ ç„¶åæ›´æ–°åéªŒæ¦‚ç‡çš„å‚æ•°} \theta &lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;$$
\hat{\theta}=\arg \max _{\theta} E L B O &lt;br /&gt;
= \arg \max _{\theta} \int q(z) \log \frac{p (x, z|\theta)}{q(z)} d z&lt;br /&gt;
å½“\log p(x | \theta) \geqslant ELBO å–ç­‰äºå·(KL=0)ï¼Œq(z) = p(z|x, \theta^{(t)})  &lt;br /&gt;
=\arg \max _{\theta} \int p\left(z | x, \theta^{(t)}\right) \log \frac{p(x, z | \theta)}{p\left(z | x,\theta^{(t)}\right)} d z &lt;br /&gt;
=\arg \max &lt;em&gt;{\theta} \int p\left(z | x, \theta^{(t)}\right) \left[
\log p(x, z | \theta) - \underbrace{ \log p\left(z | x,\theta^{(t)}\right)}&lt;/em&gt;{ä¸\thetaæ— å…³ï¼Œå·²ç»å˜æˆå¸¸æ•°}
\right]dz &lt;br /&gt;
= \arg \max &lt;em&gt;{\theta} \int&lt;/em&gt;{z} p\left(z | x, \theta^{(t)}\right) \cdot \log p(x, z | \theta)  d z&lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;1.jpg&#34; alt=&#34;image-20200323222822401&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;æ–°çš„è§’åº¦æ¨å¯¼em&#34;&gt;æ–°çš„è§’åº¦æ¨å¯¼EM&lt;/h3&gt;

&lt;p&gt;$$
\begin{aligned}
\log P(x|\theta) &amp;amp; = \log \int&lt;em&gt;z P(x, z| \theta) &lt;br /&gt;
 &amp;amp; = \log \int&lt;/em&gt;{z} \frac{P(x, z | \theta)}{q(z)} \cdot q(z) d z &lt;br /&gt;
 &amp;amp; = \log E&lt;em&gt;{q(z)}\left[\frac{p(x, z | \theta)}{q(z)}\right] &lt;br /&gt;
 &amp;amp; \text{use jensen inequalty} &lt;br /&gt;
 &amp;amp; \geqslant E&lt;/em&gt;{q(z)}\left[\log \frac{p(x, z | \theta)}{q(z)}\right] \rightarrow ELBO &lt;br /&gt;
 &amp;amp; \Leftrightarrow \frac{P(x, z | \theta)}{q(z)}=C  &lt;br /&gt;
 q(z) &amp;amp; =\frac{1}{c} p(x, z | \theta) &lt;br /&gt;
 1  =\int&lt;em&gt;z q(z) d z &amp;amp;=\int&lt;/em&gt;{z} \frac{1}{c} p(x, z | \theta) d z &lt;br /&gt;
 &amp;amp; =\frac{1}{c} \int_{x} p(x, z | \theta) d z &lt;br /&gt;
 1 &amp;amp; =\frac{1}{c} P(x | \theta) &lt;br /&gt;
 c &amp;amp; =P(x | \theta)
 \end{aligned}
$$&lt;/p&gt;

&lt;p&gt;$$
q(z)=\frac{1}{p(x | \theta)} \cdot p(x, z | \theta)=p(z | x, \theta)
$$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;2.jpg&#34; alt=&#34;image-20200323224250592&#34; /&gt;&lt;/p&gt;

&lt;p&gt;$$
t \in[0.1] &lt;br /&gt;
c= ta+ (1-t)b &lt;br /&gt;
f&amp;copy; =f(ta+ (1-t)b) \geqslant tf(a) + (1-t)f(b)
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{array}{l}x(t)=x&lt;em&gt;{1}+t\left(x&lt;/em&gt;{2}-x&lt;em&gt;{1}\right) \ y(t)=f\left(x&lt;/em&gt;{1}\right)+t\left(f\left(x&lt;em&gt;{2}\right)-f\left(x&lt;/em&gt;{1}\right)\right) \ t \in \mathbb{R}\end{array}
&lt;br /&gt;
\begin{array}{l}\text { Combining like terms and replacing } t \text { with }(1-t) \text { (which is fine since } t \text { is an arbitrary parameter; } \ \text { furthermore } 0 \leq 1-t \leq 1 \Longleftrightarrow 0 \leq t \leq 1 \text { ), we can write the secant line as the set of } \ \text { points } \ \qquad \begin{array}{l}x(t)=t x&lt;em&gt;{1}+(1-t) x&lt;/em&gt;{2} \ y(t)=t f\left(x&lt;em&gt;{1}\right)+(1-t) f\left(x&lt;/em&gt;{2}\right) \ t \in \mathbb{R}\end{array}\end{array}&lt;br /&gt;
$$
&lt;a href=&#34;http://www.gtmath.com/2016/03/convexity-and-jensens-inequality.html&#34; target=&#34;_blank&#34;&gt;http://www.gtmath.com/2016/03/convexity-and-jensens-inequality.html&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;å¹¿ä¹‰em&#34;&gt;å¹¿ä¹‰EM&lt;/h3&gt;

&lt;p&gt;æ¦‚ç‡ç”Ÿæˆæ¨¡å‹é—®é¢˜ï¼Œé€šè¿‡zç”Ÿæˆxï¼Œç„¶åå†é€šè¿‡ç§¯åˆ†æŠŠzæ¶ˆæ‰ã€‚å‚æ•°learningé—®é¢˜&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR;
z--&amp;gt;x;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$$
\hat{\theta}
= \arg \max _{\theta}  \log P(X|\theta)&lt;br /&gt;
 = \arg \max &lt;em&gt;{\theta}  \log \prod&lt;/em&gt;{i=1}^N P(x_i|\theta)
$$&lt;/p&gt;

&lt;p&gt;$$
\log P(x | \theta)=E L B O+K L(q | p) &lt;br /&gt;
\left{\begin{array}{l}E L B O=E_{q(z)}\left[\log \frac{p(x, z | \theta)}{q(z)}\right] \ KL(q | p)=\int q(z) \cdot \log \frac{q(z)}{p(z | x, \theta)} d z\end{array}\right. \&lt;/p&gt;

&lt;p&gt;\text{E-step: å›ºå®š }\theta \rightarrow \hat{q}=\arg \min _{q} K L(q | p)=\arg \max_q \mathcal{L}(q, \theta) &lt;br /&gt;
\text{M-step: å›ºå®š }\hat{q} \rightarrow \theta=\arg \max _{\theta} \mathcal{L}(\hat{q}, \theta)
$$&lt;/p&gt;

&lt;p&gt;$$
\left{\begin{array}{l}
\text { E-stes: } q^{(t+1)}=\arg \max _{q} \mathcal{L}\left(q, \theta^{(t)}\right)
\  \text { M-step: } \theta^{(t+1)}=\arg \max _{\theta} \mathcal{L}(q^{t+1}, \theta)
\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;æ‰€ä»¥EMä¹Ÿå«MMç®—æ³•ï¼Œè½®æµè¿­ä»£q and $\theta$&lt;/p&gt;

&lt;p&gt;SMOï¼šåæ ‡ä¸Šå‡æ³•/æ¢¯åº¦ä¸Šå‡æ³•&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;3.jpg&#34; alt=&#34;image-20200323233326072&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Mæ­¥å’ŒEæ­¥å…¶å®å¯ä»¥äº¤æ¢&lt;/p&gt;

&lt;p&gt;E-Step å…¶å®æ˜¯æ±‚åéªŒæ¦‚ç‡ï¼Œå‡å¦‚intractable  ç„¶åç”¨VIæ±‚å‡ºåéªŒ  å°±å«VBEM/VEMï¼Œå¦‚æœç”¨MCæ±‚åéªŒé‚£å°±æ˜¯MCEM&lt;/p&gt;

&lt;p&gt;VI &amp;lt;=&amp;gt; VB&lt;/p&gt;

&lt;p&gt;æ—¢ç„¶æ˜¯ä¼˜åŒ–é—®é¢˜ï¼Œèƒ½å¦ç”¨æ¢¯åº¦çš„æ–¹æ³•æ±‚è§£å‘¢ï¼Ÿ&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Foundation of Machine Learning</title>
      <link>https://faithio.cn/post/foundation-of-machine-learning/</link>
      <pubDate>Thu, 19 Mar 2020 19:15:53 -0400</pubDate>
      <guid>https://faithio.cn/post/foundation-of-machine-learning/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;./ml.html&#34;&gt;click here for link&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;common&#34;&gt;Common&lt;/h3&gt;

&lt;p&gt;X: data&lt;/p&gt;

&lt;p&gt;$X=\left(x_1, x_2 \cdots x&lt;em&gt;N\right)&lt;/em&gt;{N \times p}^{T}$
$$
\theta = \left(\begin{array}{cccc}x&lt;em&gt;{0} &amp;amp; x&lt;/em&gt;{a} &amp;amp; \ldots &amp;amp; x&lt;em&gt;{1 x} \ x&lt;/em&gt;{1} &amp;amp; x&lt;em&gt;{12} &amp;amp; \ldots &amp;amp; x&lt;/em&gt;{21} \ \vdots &amp;amp; &amp;amp; &amp;amp; x&lt;em&gt;{n y} \ x&lt;/em&gt;{m} &amp;amp; x_{n x} &amp;amp; \ldots &amp;amp; \end{array}\right)
$$&lt;/p&gt;

&lt;h3 id=&#34;é¢‘ç‡-ç»Ÿè®¡æœºå™¨å­¦ä¹ &#34;&gt;é¢‘ç‡ï¼ˆç»Ÿè®¡æœºå™¨å­¦ä¹ ï¼‰&lt;/h3&gt;

&lt;p&gt;ä¼˜åŒ–é—®é¢˜
$$
x \sim p(x | \theta)
$$
$\theta$ æœªçŸ¥å¸¸é‡ï¼Œ Xï¼šr v
$$
MLEï¼š\theta_{M L E}=\arg \max _\theta \log P(X | \theta)
$$&lt;/p&gt;

&lt;h3 id=&#34;è´å¶æ–¯-æ¦‚ç‡å›¾æ¨¡å‹&#34;&gt;è´å¶æ–¯(æ¦‚ç‡å›¾æ¨¡å‹)&lt;/h3&gt;

&lt;p&gt;æ±‚ç§¯åˆ†ï¼ˆMCMCï¼‰&lt;/p&gt;

&lt;p&gt;$\theta$ r vï¼Œ
$$
\theta \sim p(\theta)
$$&lt;/p&gt;

&lt;p&gt;$$
P(\theta | x)=\frac{P(x | \theta) \cdot P(\theta)}{P(x)} \propto  P(x | \theta) \cdot P(\theta)  &lt;br /&gt;
P(x) = \int_{\theta} P(x | \theta) \cdot P(\theta) d \theta
$$&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://pic4.zhimg.com/80/v2-1186de08c2ac2ef05b30cd88b5260707_720w.jpg&#34; alt=&#34;img&#34; /&gt;
$$
MAP: \theta_{MAP}=\arg \max _{\theta} P(\theta | x)=\arg \max _{\theta} P(x | \theta) \cdot P(\theta)
$$&lt;/p&gt;

&lt;h3 id=&#34;bias-and-variance&#34;&gt;Bias and Variance&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./1.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;avoid-overfiting&#34;&gt;Avoid overfiting&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;./IMG_1742.PNG&#34; alt=&#34;IMG_1742&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./IMG_1741.PNG&#34; alt=&#34;IMG_1741&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./IMG_1740.PNG&#34; alt=&#34;IMG_1740&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./IMG_1739.PNG&#34; alt=&#34;IMG_1739&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;book&#34;&gt;book&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;æèˆª ç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼ˆé¢‘ç‡æ´¾ï¼‰æ„ŸKæœ´å†³é€»ï¼Œ æ”¯æEéšæ¡&lt;/li&gt;
&lt;li&gt;å‘¨å¿—å â€œè¥¿ç“œä¹¦â€&lt;/li&gt;
&lt;li&gt;PRML å›åˆ†ç¥æ ¸ç¨€ å›¾æ··è¿‘é‡‡è¿  é¡ºç»„ - è´å¶æ–¯&lt;/li&gt;
&lt;li&gt;MLAPP  è´å¶æ–¯&lt;/li&gt;
&lt;li&gt;ESL  é¢‘ç‡æ´¾&lt;/li&gt;
&lt;li&gt;Deep Learning åœ£ç»/å¼ å¿—åç¿»è¯‘&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;é«˜æ–¯åˆ†å¸ƒ&#34;&gt;é«˜æ–¯åˆ†å¸ƒ&lt;/h3&gt;

&lt;p&gt;é©¬æ°è·ç¦»/æ¬§æ°è·ç¦»&lt;/p&gt;

&lt;p&gt;$X^TAX$ : æ¬§æ°è·ç¦»ï¼Œä¸­é—´æ˜¯å•ä½çŸ©é˜µï¼Œé©¬æ°è·ç¦»ï¼Œä¸­é—´æ˜¯åæ–¹å·®çŸ©é˜µï¼ˆå…¶å®æ˜¯äºŒæ¬¡å‹ï¼‰&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Approximate Inference</title>
      <link>https://faithio.cn/post/approximate-inference/</link>
      <pubDate>Thu, 19 Mar 2020 18:21:57 -0400</pubDate>
      <guid>https://faithio.cn/post/approximate-inference/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;./Approximate.pdf&#34;&gt;Approximate Inference.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;./Approximate.html&#34;&gt;Approximate Inference.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Exponential Family</title>
      <link>https://faithio.cn/post/the-exponential-family/</link>
      <pubDate>Thu, 19 Mar 2020 18:21:43 -0400</pubDate>
      <guid>https://faithio.cn/post/the-exponential-family/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;./Exponential.pdf&#34;&gt;Exponential.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;../../fomula/Exponential.html&#34;&gt;Exponential.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Probabilistic Graphical Model</title>
      <link>https://faithio.cn/post/probabilistic-graphical-model/</link>
      <pubDate>Thu, 19 Mar 2020 17:56:38 -0400</pubDate>
      <guid>https://faithio.cn/post/probabilistic-graphical-model/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;./probabilistic.pdf&#34;&gt;pdf&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD;
æ¦‚ç‡å›¾--&amp;gt;Representation-è¡¨ç¤º;
æ¦‚ç‡å›¾--&amp;gt;Inference-æ¨æ–­;
æ¦‚ç‡å›¾--&amp;gt;Learning-å­¦ä¹ ;
Representation-è¡¨ç¤º--&amp;gt;æœ‰å‘å›¾BayesianNetwork;
Representation-è¡¨ç¤º--&amp;gt;é«˜æ–¯å›¾-è¿ç»­;
Representation-è¡¨ç¤º--&amp;gt;æ— å‘å›¾MarkovNetwork;
é«˜æ–¯å›¾-è¿ç»­--&amp;gt;GaussianBN;
é«˜æ–¯å›¾-è¿ç»­--&amp;gt;GaussianMN;
Inference-æ¨æ–­--&amp;gt;ç²¾ç¡®æ¨æ–­;
Inference-æ¨æ–­--&amp;gt;ApproximateInference;
ApproximateInference--&amp;gt;DeterministicApproximation(Variantional Inference);
ApproximateInference--&amp;gt;StochasticcApproximation(MCMC);
Learning-å­¦ä¹ --&amp;gt;å‚æ•°å­¦ä¹ ;
Learning-å­¦ä¹ --&amp;gt;ç»“æ„å­¦ä¹ ;
å‚æ•°å­¦ä¹ --&amp;gt;å®Œå¤‡æ•°æ®;
å‚æ•°å­¦ä¹ --&amp;gt;éšå˜é‡;
éšå˜é‡--&amp;gt;EM;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$$
\begin{array}{l}
\text{Sum Rule: } P(x_1)=\int P\left(x_1, x&lt;em&gt;2\right) d x&lt;/em&gt;{2}
\
\text{Product Rule: } P\left(x&lt;em&gt;1, x&lt;/em&gt;{2}\right)=P\left(x_{1}\right) \cdot P\left(x_2 | x&lt;em&gt;1\right)=P\left(x&lt;/em&gt;{2}\right) \cdot P\left(x&lt;em&gt;{1} | x&lt;/em&gt;{2}\right)&lt;/p&gt;

&lt;p&gt;\end{array}
$$&lt;/p&gt;

&lt;p&gt;Chain Rule:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://wikimedia.org/api/rest_v1/media/math/render/svg/1386ec6778f1816c3fa6e9de68f89cee2e938066&#34; alt=&#34;{\displaystyle {\begin{aligned}\mathrm {P} (X_{4},X_{3},X_{2},X_{1})&amp;amp;=\mathrm {P} (X_{4}\mid X_{3},X_{2},X_{1})\cdot \mathrm {P} (X_{3},X_{2},X_{1})\\&amp;amp;=\mathrm {P} (X_{4}\mid X_{3},X_{2},X_{1})\cdot \mathrm {P} (X_{3}\mid X_{2},X_{1})\cdot \mathrm {P} (X_{2},X_{1})\\&amp;amp;=\mathrm {P} (X_{4}\mid X_{3},X_{2},X_{1})\cdot \mathrm {P} (X_{3}\mid X_{2},X_{1})\cdot \mathrm {P} (X_{2}\mid X_{1})\cdot \mathrm {P} (X_{1})\end{aligned}}}&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./1.jpg&#34; alt=&#34;image-20200319195228032&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;bayesian-network&#34;&gt;Bayesian Network&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;æ‹“æ‰‘æ’åºæ„å»ºå›¾&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD;
a--&amp;gt;b;
a--&amp;gt;c;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$
   Chain Rule: P\left(x&lt;em&gt;{1}, x&lt;/em&gt;{2}, \cdots, x&lt;em&gt;{p}\right)=P\left(x&lt;/em&gt;{1}\right) \cdot \prod&lt;em&gt;{i=2}^{p} p\left(x&lt;/em&gt;{i} | x_{1:i-1}\right)
   &lt;br /&gt;
   P(a, b, c) = P(a)P(b|a)(c|a) \rightarrow å› å­åˆ†è§£&lt;br /&gt;
   P(a, b, c) = P(a)P(b|a)(c|a,b) \rightarrow Chain rule&lt;br /&gt;
   P(c | a)=P(c | a, b) \Rightarrow c \perp b | a &lt;br /&gt;
   p(c | a) \cdot p(b | a)=p(c|a,b) \cdot p(b | a)=p( b, c | a) &lt;br /&gt;
   p(c | a) \cdot p(b | a)=p(b, c | a)
   $$&lt;/p&gt;

&lt;p&gt;Tail to tail, è‹¥aè¢«è§‚æµ‹ï¼Œåˆ™è·¯å¾„è¢«å µå¡$tail \rightarrow head$&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;   graph LR;
   a--&amp;gt;b;
   b--&amp;gt;c;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;head to tail
$$
P(a,b, c) = P(a)P(b|a)P(c|b) &lt;br /&gt;
P(a, b, c) = P(a) P(b|a)  P(c|a,b)  &lt;br /&gt;
P(c|b) = P(c|a,b)
$$&lt;/p&gt;

&lt;p&gt;$$
a \perp c | b
$$
è‹¥bè¢«è§‚æµ‹ï¼Œåˆ™è·¯å¾„è¢«é˜»å¡ï¼ˆindependentï¼‰&lt;/p&gt;

&lt;hr /&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;   graph TD;
   a--&amp;gt;c;
   b--&amp;gt;c;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;head to head&lt;/p&gt;

&lt;p&gt;é»˜è®¤æƒ…å†µä¸‹ï¼Œ$a \perp b$ï¼Œè·¯å¾„é˜»å¡çš„&lt;/p&gt;

&lt;p&gt;è‹¥cè¢«è§‚æµ‹ï¼Œåˆ™è·¯å¾„æ˜¯é€šçš„
$$
P(a, b, c)=P(a) \cdot P(b) \cdot P(c | a, b) &lt;br /&gt;
P(a, b, c) = P(a)\cdot P(b|a) \cdot (c|a,b)  &lt;br /&gt;
P(b) = P(b|a)  &lt;br /&gt;
$$&lt;/p&gt;

&lt;h3 id=&#34;inference&#34;&gt;Inference&lt;/h3&gt;

&lt;p&gt;$$
\begin{aligned} \text { sum rule } &amp;amp; p(X)=\sum_{Y} p(X, Y) \ \text { product rule } &amp;amp; p(X, Y)=p(Y | X) p(X) \end{aligned}
$$
æ±‚æ¦‚ç‡ï¼š $
P(x)=P\left(x_0, x_1, \cdots, x_p\right)
$&lt;/p&gt;

&lt;p&gt;è¾¹ç¼˜æ¦‚ç‡&lt;em&gt;marginal&lt;/em&gt; probabilityï¼š
$$
P\left(x&lt;em&gt;{i}\right)=\sum&lt;/em&gt;{x&lt;em&gt;1} \cdot \sum&lt;/em&gt;{x&lt;em&gt;{i-1}} \sum&lt;/em&gt;{x&lt;em&gt;{i+1}} \ldots \sum&lt;/em&gt;{x_{p}} p(x)
$$
æ¡ä»¶æ¦‚ç‡conditional probabilityï¼š
$$
P\left(x_A | x_B\right) \quad x=x_A \cup x_B
$$
MAP Inference:
$$
\hat{z}=\arg \max _{z} P(z | x) \propto \arg \max P(z, x)
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph LR;
Inference--&amp;gt;ç²¾ç¡®æ¨è®º;
Inference--&amp;gt;è¿‘ä¼¼æ¨æ–­;
ç²¾ç¡®æ¨è®º--&amp;gt;variableElimination/VE;
ç²¾ç¡®æ¨è®º--&amp;gt;BeliefPropagation/SumProductAlgorithmæ ‘ç»“æ„;
ç²¾ç¡®æ¨è®º--&amp;gt;JunctionTreeAlgorithmæ™®é€šå›¾ç»“æ„BasedBP;
è¿‘ä¼¼æ¨æ–­--&amp;gt;LoopBeliefPropagationæœ‰ç¯å›¾BasedBP;
è¿‘ä¼¼æ¨æ–­--&amp;gt;MenteCarloInference:ImportanceSampling,MCMC;
è¿‘ä¼¼æ¨æ–­--&amp;gt;VariationalInference
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;./2.jpg&#34; alt=&#34;image-20200319214741677&#34; /&gt;&lt;/p&gt;

&lt;h4 id=&#34;variable-elimination-ä¹˜æ³•åˆ†é…å¾‹&#34;&gt;Variable Elimination-ä¹˜æ³•åˆ†é…å¾‹&lt;/h4&gt;

&lt;p&gt;$$
M A P \quad \tilde{X}_{A}=\arg \max &lt;em&gt;{X} P\left(x&lt;/em&gt;{A} | x&lt;em&gt;{B}\right)=\arg \max P\left(x&lt;/em&gt;{A}, x_{B}\right)
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;   graph LR;
   a--&amp;gt;b;
   b--&amp;gt;c;
   c--&amp;gt;d;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;å‡è®¾a,b,c,då‡æ˜¯ç¦»æ•£çš„äºŒå€¼r,v {0,1}&lt;/p&gt;

&lt;p&gt;$$
p(d) =  \sum_{a, b, c} p(a, b, c, d) \&lt;/p&gt;

&lt;p&gt;=
\sum_{a, b, c} p(a) \cdot p(b | a) \cdot p(c | b) \cdot p(d | c) \&lt;/p&gt;

&lt;p&gt;=  p(a=0) \cdot p(b=0 | a=0) \cdot p(c=0 | b=0) \cdot p(d=0|c=0) &lt;br /&gt;
+ p(a=1) \cdot p(b=0 | a=1) \cdot p(c=0 | b=0) \cdot p(d=0|c=0) \&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\cdots &lt;br /&gt;&lt;/li&gt;
&lt;li&gt;p(a=1) \cdot p(b=1 | a=1) \cdot p(c=1 | b=1) \cdot p(d=1|c=1) &lt;br /&gt;
= \sum&lt;em&gt;{b, c} p(c | b) \cdot p(d | c) \cdot \underbrace{\sum&lt;/em&gt;{a} p(a) \cdot p(b | a)}_{\phi_a(b)} &lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;= \sum&lt;em&gt;c p(d | c) \cdot \underbrace{\sum&lt;/em&gt;{b}  p(c | b)\cdot \phi&lt;em&gt;a(b) }&lt;/em&gt;{\phi_b&amp;copy;} &lt;br /&gt;
= \phi_c(d)
$$
ä¹˜æ³•å¯¹åŠ æ³•çš„åˆ†é…å¾‹$ab+cb = b(a+c)$&lt;/p&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Memoryless.é‡å¤è®¡ç®—&lt;/li&gt;
&lt;li&gt;Ordering  NP-hard&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;belief-propagation&#34;&gt;Belief Propagation&lt;/h4&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;   graph LR;
   a--&amp;gt;b;
   b--&amp;gt;c;
   c--&amp;gt;d;
   d--&amp;gt;e;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$$
\text{ Forward Algorithm} &lt;br /&gt;
P(a, b, c, d, e)=P(a) P(b | a) \cdot P(c | b) \cdot P(d | c) \cdot P(e|d) &lt;br /&gt;
P(e)=\sum&lt;em&gt;{a, b,c, d} P(a, b, c, d, e) &lt;br /&gt;
=\sum&lt;/em&gt;{d} p(e | d) \sum&lt;em&gt;{c} p(d | c) \underbrace{ \sum&lt;/em&gt;{b} p(c | b) \underbrace{ \sum&lt;em&gt;{a} p(b | a) p(a) }&lt;/em&gt;{m&lt;em&gt;{a\rightarrow b }(b)}}&lt;/em&gt;{m_{b \rightarrow c}&amp;copy;}
$$&lt;/p&gt;

&lt;p&gt;$$
p&amp;copy;=\sum&lt;em&gt;{a, b, d, e} p(a, b, c, d, e)  &lt;br /&gt;
 = \left(\sum&lt;/em&gt;{b} p(c | b) \cdot \sum&lt;em&gt;{a} p(b | a) \cdot p(a) \right) \left(\sum&lt;/em&gt;{d} p(d | c) \sum_{e} p(e | d)\right)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
 \text{ Forward-Backward Algorithm}
$$&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-mermaid&#34;&gt;graph TD;
a---b;
b---d;
b---c;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;$$
\begin{aligned}
&amp;amp; p(a, b, c, d)
\=&amp;amp; \frac{1}{z} \psi&lt;em&gt;{a}(a) \psi&lt;/em&gt;{b}(b) \cdot \psi&lt;em&gt;{c}&amp;copy; \cdot \varphi(d)
\ &amp;amp; \cdot \psi&lt;/em&gt;{a, b}(a, b) \cdot \psi&lt;em&gt;{b, c}(b, c) \cdot \psi&lt;/em&gt;{b, d}(b, d) \end{aligned}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Run Issue With Matplotlib in Mac OS X</title>
      <link>https://faithio.cn/post/run-issue-with-matplotlib-in-mac-os-x/</link>
      <pubDate>Mon, 09 Mar 2020 13:37:09 -0400</pubDate>
      <guid>https://faithio.cn/post/run-issue-with-matplotlib-in-mac-os-x/</guid>
      <description>&lt;p&gt;When I run some python code from github, it occur the following problem as screenshot.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gco76fc96qj31fo01paai.jpg&#34; alt=&#34;image-20200309133826149&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of &amp;lsquo;python&amp;rsquo; with &amp;lsquo;pythonw&amp;rsquo;. See &amp;lsquo;Working with Matplotlib on OSX&amp;rsquo; in the Matplotlib FAQ for more information.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Solution:(&lt;a href=&#34;https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python&#34; target=&#34;_blank&#34;&gt;https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Problem Cause&lt;/strong&gt; In mac os image rendering back end of matplotlib (what-is-a-backend to render using the API of Cocoa by default). There is Qt4Agg and GTKAgg and as a back-end is not the default. Set the back end of macosx that is differ compare with other windows or linux os.&lt;/p&gt;

&lt;p&gt;I resolve this issue following ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;I assume you have installed the pip matplotlib, there is a directory in you root called &lt;code&gt;~/.matplotlib&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Create a file &lt;code&gt;~/.matplotlib/matplotlibrc&lt;/code&gt; there and add the following code: &lt;code&gt;backend: TkAgg&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;From this &lt;a href=&#34;http://matplotlib.org/examples/index.html&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt; you can try different diagram.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stochastic Expectation Propagation</title>
      <link>https://faithio.cn/post/stochastic-expectation-propagation/</link>
      <pubDate>Sat, 29 Feb 2020 11:34:28 -0500</pubDate>
      <guid>https://faithio.cn/post/stochastic-expectation-propagation/</guid>
      <description>

&lt;h1 id=&#34;math-formula&#34;&gt;Math Formula&lt;/h1&gt;

&lt;h3 id=&#34;factor-graphs&#34;&gt;Factor graphs&lt;/h3&gt;

&lt;p&gt;Shows how a function of several variables can be factored into a product of simpler functions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;$$ f(x,y,z) = (x+y) \cdot (y + z) \cdot (x  +z) $$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Very useful for representing posteriors.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;$ 
$$
P(x1, ..., x_n) = P(x_1) \Pi P( x_i | x_{i-1} )
$$
$&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;$$ P(m|x1, &amp;hellip;, x_n) = P(m) \cdot \Pi P(x_i|m)$$&lt;/p&gt;

&lt;h4 id=&#34;modeling&#34;&gt;modeling&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;What graph should I use for this data?&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;inference&#34;&gt;Inference&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;Given the graph and data, what is the mean of x&lt;/li&gt;
&lt;li&gt;algorithm

&lt;ul&gt;
&lt;li&gt;Sampling&lt;/li&gt;
&lt;li&gt;Variable elimination&lt;/li&gt;
&lt;li&gt;Message-passing(Expectation Propagation, Variational Bayes)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;cutter-problem&#34;&gt;Cutter problem&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Want to estimate x given multiple y&amp;rsquo;s&lt;/li&gt;
&lt;li&gt;$$ p(x) = \mathcal{N}(x; 0, 100) $$&lt;/li&gt;
&lt;li&gt;$$ p(y_i|x) = (0.5)\mathcal{N}(y_i; x, 1) + (0.5)\mathcal{N} (y_i;0,10)$$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-&amp;gt; $ P(x|y1, &amp;hellip;, y_n) = P(x) \cdot \Pi P(y_i|x)$&lt;/p&gt;

&lt;p&gt;if we only have 2 points:&lt;/p&gt;

&lt;p&gt;$$ P(x) \cdot  P(y_1|x) \cdot P(y_2|x)  \rightarrow   p(y_i|x) = (0.5)\mathcal{N}(y_i; x, 1) + (0.5)\mathcal{N} (y_i;0,10)$$&lt;/p&gt;

&lt;p&gt;2 points have 4 Gaussians -&amp;gt; N points $$2^N$$ Gaussians&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gck12m03b5j30nf0f5diu.jpg&#34; alt=&#34;image-20200305220454120&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/00831rSTgy1gck1kw7d6ij30mw0h5mz1.jpg&#34; alt=&#34;image-20200305222232412&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://pic4.zhimg.com/80/v2-1186de08c2ac2ef05b30cd88b5260707_720w.jpg&#34; alt=&#34;img&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://zhuanlan.zhihu.com/p/75617364&#34; target=&#34;_blank&#34;&gt;https://zhuanlan.zhihu.com/p/75617364&lt;/a&gt;
$$
p(z | w)=\frac{p(w | z) p(z)}{p(w)}=\frac{p(w | z) p(z)}{\int_{z} p(w | z) p(z) d z}
$$
Because it extends belief propagation. Belief propagation passes the entire distribution is the message. While EP will only pass onto the distribution certain expectation distribution allows you to you get a very compact message.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;expectation-propagation&#34;&gt;Expectation Propagation&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Fits an exponential-family approximation to the posterior.&lt;/li&gt;
&lt;li&gt;Belief propagation is a special case&lt;/li&gt;
&lt;li&gt;Kalman filtering is a special case&lt;/li&gt;
&lt;li&gt;Does not always converge.

&lt;ol&gt;
&lt;li&gt;May get stuck due to improper distributions&lt;/li&gt;
&lt;li&gt;May oscillate due to loopy graph&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h3 id=&#34;agm&#34;&gt;AGM&lt;/h3&gt;

&lt;p&gt;$$
p(\mathbf{X} | \Theta)=\sum&lt;em&gt;{j=1}^{M} p&lt;/em&gt;{j} p\left(\mathbf{X} | \xi_{j}\right)
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;$\xi_j$ is the set of the parameters of component j.&lt;/li&gt;
&lt;li&gt;$ p_j$ are the mixing proptions which must be positive and sum to one.&lt;/li&gt;
&lt;li&gt;$\Theta = {p_1, \ldots, p_M, \xi_1, \ldots, \xi_M}$ is the complete set of parameters fully characterizing the mixture.&lt;/li&gt;
&lt;li&gt;$ M \geq 1$ is number of components in the mixture.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
p\left(X | \theta\right)=\prod&lt;em&gt;{d=1}^{D} \sqrt{\frac{2}{\pi}} \frac{1}{\left(\sigma&lt;/em&gt;{l&lt;em&gt;{d}}+\sigma&lt;/em&gt;{r&lt;em&gt;{d}}\right)} \times\left{\begin{array}{ll}\exp \left[-\frac{\left(X&lt;/em&gt;{d}-\mu&lt;em&gt;{d}\right)^{2}}{2 \sigma&lt;/em&gt;{l&lt;em&gt;{d} }^{2}}\right] &amp;amp; \text { if } X&lt;/em&gt;{d}&amp;lt;\mu&lt;em&gt;{d} \ \exp \left[-\frac{\left(X&lt;/em&gt;{d}-\mu&lt;em&gt;{d}\right)^{2}}{2 \sigma&lt;/em&gt;{r&lt;em&gt;{d}}^{2}}\right] &amp;amp; \text { if } X&lt;/em&gt;{d} \geq \mu&lt;em&gt;{d}\end{array}\right. &lt;br /&gt;
$$
- $\vec{\mu}=\left(\mu&lt;/em&gt;{1}, \ldots, \mu&lt;em&gt;{D}\right)$ is the mean
- $\vec{\sigma}&lt;/em&gt;{l}=\left(\vec{\sigma}&lt;em&gt;{l&lt;/em&gt;{1}}, \ldots, \vec{\sigma}&lt;em&gt;{l&lt;/em&gt;{D}}\right)$ is the left standard deviation
- $\vec{\sigma}&lt;em&gt;{r}=\left(\vec{\sigma}&lt;/em&gt;{r&lt;em&gt;{1}}, \ldots, \vec{\sigma}&lt;/em&gt;{r_{D}}\right)$ is the right standard deviation&lt;/p&gt;

&lt;p&gt;$$
\log P = \sum&lt;em&gt;{d=1}^{D} \log \sqrt{\frac{2}{\pi}} - \frac{1}{2}\log (\sqrt{v&lt;/em&gt;{l&lt;em&gt;{d}}} + \sqrt{v&lt;/em&gt;{r&lt;em&gt;{d}}}) -
\left{\begin{array}{ll}
\frac{\left(X&lt;/em&gt;{d}-\mu&lt;em&gt;{d}\right)^{2}}{2 v&lt;/em&gt;{l&lt;em&gt;{d} }}  &amp;amp; \text { if } X&lt;/em&gt;{d}&amp;lt;\mu&lt;em&gt;{d} &lt;br /&gt;
\frac{\left(X&lt;/em&gt;{d}-\mu&lt;em&gt;{d}\right)^{2}}{2 v&lt;/em&gt;{r&lt;em&gt;{d} }} &amp;amp; \text { if } X&lt;/em&gt;{d} \geq \mu&lt;em&gt;{d} &lt;br /&gt;
\end{array}\right. &lt;br /&gt;
\frac{\partial \log P}{\partial v&lt;/em&gt;{l&lt;em&gt;{d}}} = -\frac{1}{4}\frac{1}{v&lt;/em&gt;{l&lt;em&gt;{d}} + \sqrt{v&lt;/em&gt;{l&lt;em&gt;{d}} v&lt;/em&gt;{r&lt;em&gt;{d}} }} + \left{\begin{array}{ll}
\frac{\left(X&lt;/em&gt;{d}-\mu&lt;em&gt;{d}\right)^{2}}{2 v&lt;/em&gt;{l&lt;em&gt;{d}}^2}  &amp;amp; \text { if } X&lt;/em&gt;{d}&amp;lt;\mu&lt;em&gt;{d} &lt;br /&gt;
0 &amp;amp; \text { if } X&lt;/em&gt;{d} \geq \mu_{d} &lt;br /&gt;
\end{array}\right.
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;$$
p(X, \boldsymbol{\theta})=\prod&lt;em&gt;{i} f&lt;/em&gt;{i}(\boldsymbol{\theta}) = \prod_{i} p(x&lt;em&gt;i|\boldsymbol{\theta})&lt;br /&gt;
%p\left(\theta | X \right) = \frac{1}{p(X)} \prod&lt;/em&gt;{i} f_{i}(\boldsymbol{\theta}) &lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;$$
%p(X)= \int \prod&lt;em&gt;{i} f&lt;/em&gt;{i}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$&lt;/p&gt;

&lt;p&gt;Here, $p(\vec{X})$ is very intractable to calculate and we don&amp;rsquo;t know $ f_{i}(\boldsymbol{\theta}) $.&lt;/p&gt;

&lt;p&gt;Now we consider using  &lt;strong&gt;EP&lt;/strong&gt;. The approximation, $q\left(\theta_j \right)$ , of the posterior,  $p\left( \theta_j | \vec{X} \right)$ , is assumed to have same functional form.
$$
q(\theta_j)=\frac{1}{Z} \prod_i \widetilde{f}_i(\theta_j)
$$&lt;/p&gt;

&lt;p&gt;We hope that:
$$
\mathrm{KL}(p | q)=\mathrm{KL}\left(\frac{1}{p(X)} \prod&lt;em&gt;{i} f&lt;/em&gt;{i}(\boldsymbol{\theta}) | \frac{1}{Z} \prod&lt;em&gt;{i} \widetilde{f}&lt;/em&gt;{i}(\boldsymbol{\theta})\right)
$$&lt;/p&gt;

&lt;p&gt;In general, this minimization will be intractable because the KL divergence involves averaging with respect to the true distribution.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;But We can use EP:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;first choose a factor $\widetilde{f}_{j}$ to approximate.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Begin Loop, until the following steps are convergence.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;second compute the cavity distribution $q^{\backslash j}(\boldsymbol{\theta})$:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$
q^{\backslash j}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}_{j}(\boldsymbol{\theta})} \&lt;/p&gt;

&lt;p&gt;\hat{p} =
\frac{1}{Z&lt;em&gt;{j}} f&lt;/em&gt;{j}(\boldsymbol{\theta}) q^{\backslash j}(\boldsymbol{\theta})
$$&lt;/p&gt;

&lt;p&gt;Here $q^{\backslash j}(\boldsymbol{\theta})$ is called the cavity distribution. $\hat{p}$ is defined as a product of the &lt;em&gt;exact&lt;/em&gt; factor $f_i$ with the rest of the factors &lt;em&gt;approximated&lt;/em&gt;,
normalised to 1, and the cavity distribution needs to be computed in order to express $\hat{p}$.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;$$
q^{\text {new }}(\boldsymbol{\theta}) \propto \widetilde{f}&lt;em&gt;{j}(\boldsymbol{\theta}) \prod&lt;/em&gt;{i \neq j} \tilde{f}&lt;em&gt;{i}(\boldsymbol{\theta}) &lt;br /&gt;
p \propto \hat{p} = f&lt;/em&gt;{j}(\boldsymbol{\theta}) \prod&lt;em&gt;{i \neq j} \tilde{f}&lt;/em&gt;{i}(\boldsymbol{\theta})
$$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;Then compute the approximative distribution $q^{new}$&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$
 \arg \min \mathrm{KL}(\hat{p} | q^{new}(\theta)) = \arg \min \mathrm{KL}\left(\frac{f&lt;em&gt;{j}(\boldsymbol{\theta}) q^{\backslash j}(\boldsymbol{\theta})}{Z&lt;/em&gt;{j}} | q^{\mathrm{new}}(\boldsymbol{\theta})\right)
$$&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;More generally, it is straightforward to obtain the required expectations for any member of the exponential family, provided it can be normalized, because the expected statistics can be related to the derivatives of the normalization coefficient.
$$
\text{bishop:} &lt;br /&gt;
p(\mathbf{x} | \boldsymbol{\eta})=h(\mathbf{x}) g(\boldsymbol{\eta}) \exp \left{\boldsymbol{\eta}^{\mathrm{T}} \mathbf{u}(\mathbf{x})\right} &lt;br /&gt;
-\nabla \ln g(\boldsymbol{\eta})=\mathbb{E}[\mathbf{u}(\mathbf{x})] &lt;br /&gt;
$$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;Update the factor&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;$$
q^{\text {new }}(\boldsymbol{\theta})  \propto \hat{p} = \frac{1}{Z&lt;em&gt;{j}} f&lt;/em&gt;{j}(\boldsymbol{\theta}) q^{\backslash j}(\boldsymbol{\theta})
$$&lt;/p&gt;

&lt;p&gt;Then we easily obtain the formula for the approximation of $f&lt;em&gt;i$:
$$
f&lt;/em&gt;{i} \approx \tilde{f}&lt;em&gt;{i}=Z&lt;/em&gt;{i} \frac{q^{\text {new }}(\boldsymbol{\theta})}{q^{\backslash j}(\boldsymbol{\theta})}
$$
This division of distributions is from exponetial family, so does the result $\tilde{f}_{i}$. Now repeat it until parameter covergence.&lt;/p&gt;

&lt;p&gt;End Loop.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Evaluate the approximation to the model evidence&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;After the algorithm has converged to a set of factors $\left{\tilde{f}&lt;em&gt;{i}\right}$, the approximate posterior as well as the model evidence can be computed as following:
$$
p(X, \boldsymbol{\theta}) \simeq \prod&lt;/em&gt;{i} \tilde{f}&lt;em&gt;{i}(\boldsymbol{\theta}) &lt;br /&gt;
p(X) \simeq \int \prod&lt;/em&gt;{i} \tilde{f}_{i}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta}
$$&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;$$
p(\mathbf{X} | \boldsymbol{\theta})=(1-w) \mathcal{A}(\mathbf{X} | \boldsymbol{\theta}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{X} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})
$$&lt;/p&gt;

&lt;p&gt;where w is the proportion of background clutter. And the prior over $\mathbf{\theta}$(mean) is taken to be Asymmetric Gaussian.&lt;/p&gt;

&lt;p&gt;And
$$
p(\boldsymbol{\theta})= \mathcal{A}(\mathbf{X} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I&lt;em&gt;r})
$$
$$
p(X, \boldsymbol{\theta})=p(\boldsymbol{\theta}) \prod&lt;/em&gt;{n=1}^{N} p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right)
$$&lt;/p&gt;

&lt;h3 id=&#34;1-initialize-the-approximating-factors&#34;&gt;1. initialize the approximating factors&lt;/h3&gt;

&lt;p&gt;we select an approximating distribution from the exponential family to approximate the stochastic variables $\theta$
$$
q_0(\boldsymbol{\theta})&lt;/p&gt;

&lt;p&gt;= \mathcal{A}(\boldsymbol{\theta} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I_r})
$$&lt;/p&gt;

&lt;p&gt;$$
\widetilde{f}_{n}(\boldsymbol{\theta})=s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{\sigma&lt;/em&gt;{r&lt;em&gt;n}^2}, \mathbf{\sigma&lt;/em&gt;{l_n}^2} \right)
= s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)
$$&lt;/p&gt;

&lt;p&gt;$$
s&lt;em&gt;n = \prod&lt;/em&gt;{d=1}^{D} \sqrt{\frac{2}{\pi}} \frac{1}{\left(\sigma&lt;em&gt;{l&lt;/em&gt;{d}}+\sigma&lt;em&gt;{r&lt;/em&gt;{d}}\right)}
$$
While $\sigma_{l&lt;em&gt;n} \rightarrow \infty,
  \sigma&lt;/em&gt;{r_n} \rightarrow \infty
$ and $ \mu_n = 0 $.&lt;/p&gt;

&lt;p&gt;###2. initialize the posterior  approximation $q(\boldsymbol{\theta})$&lt;/p&gt;

&lt;p&gt;We chooses the parameter values a = 10, b = 100 and w = 0.5 and use $v$ denote $ \sigma^2$ as following, then $\mathbf{v_r} = \mathbf{v_l} = b = 100$&lt;/p&gt;

&lt;h3 id=&#34;3-until-all-mu-n-v-l-n-v-r-n-s-n-converge&#34;&gt;3. Until all $(\mu&lt;em&gt;n, v&lt;/em&gt;{l&lt;em&gt;n}, v&lt;/em&gt;{r_n}, s_n)$ converge:&lt;/h3&gt;

&lt;p&gt;$$
q^{\backslash n}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}_n(\boldsymbol{\theta})} = \frac{\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu}, \mathbf{v_r I}, \mathbf{v_l I})}{s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)} &lt;br /&gt;
\propto \left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;{\frac{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_l \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu})\right}}{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{l_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}}}  &amp;amp;&amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;{\frac{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_r \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu})\right}}{\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{r_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}}}  &amp;amp;&amp;amp; \text { if } X&amp;gt;\mu&lt;/p&gt;

&lt;p&gt;\end{array}\right. \&lt;/p&gt;

&lt;p&gt;= \left{\begin{array}{ll}&lt;br /&gt;
\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_l \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu}) + \frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{l_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n})\right}   &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\exp \left{-\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu})^{T}(v_r \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu}) +&lt;br /&gt;
\frac{1}{2}(\boldsymbol{X}-\mathbf{\mu&lt;em&gt;n})^{T}(v&lt;/em&gt;{r_n} \mathbf{I})^{-1}(\boldsymbol{X}-\mathbf{\mu_n}) \right} &amp;amp; \text { if } X&amp;gt;\mu
\end{array}\right.
$$
- Remove the current estimate $\widetilde{f}_j(\boldsymbol{\theta})$ from $q(\theta)$, then we has mean and inverse variance given by:
$$
\left{\begin{array}{ll}
\left({v_l}^{\backslash n}\right)^{-1}={v_l}^{-1}-{v&lt;em&gt;l}&lt;/em&gt;{n}^{-1} &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\left({v_r}^{\backslash n}\right)^{-1}={v_r}^{-1}-{v&lt;em&gt;r}&lt;/em&gt;{n}^{-1} &amp;amp; \text { if } X&amp;gt;\mu &lt;br /&gt;
\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\mathbf{\mu}^{\backslash n}= \mathbf{\mu}+&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}
{v_l}^{\backslash n} {v&lt;em&gt;l}&lt;/em&gt;{n}^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_{n}\right) &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;{v_r}^{\backslash n} {v&lt;em&gt;r}&lt;/em&gt;{n}^{-1}\left(\mathbf{\mu}-\mathbf{\mu}_{n}\right) &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right. &lt;br /&gt;
&amp;mdash;-&lt;br /&gt;
\begin{aligned}
{v^{\backslash{n}}}^{-1} &amp;amp;= v^{-1} - v_n^{-1} &lt;br /&gt;
{\mu}^{\backslash n} &amp;amp;= v^{\backslash{n}}(\mu v^{-1} - \mu_n v_n^{-1}) &lt;br /&gt;
&amp;amp;= v^{\backslash{n}}[\mu ({v^{\backslash{n}}}^{-1} + v_n^{-1}) - \mu_n v_n^{-1}] &lt;br /&gt;
&amp;amp;= \mu + v^{\backslash{n}} v_n^{-1} \mu - v^{\backslash{n}} v_n^{-1}  \mu_n &lt;br /&gt;
&amp;amp;= \mu + v^{\backslash{n}} v_n^{-1} (\mu -\mu&lt;em&gt;n)
\end{aligned}
$$
&amp;gt; Cavity Distribution:
&amp;gt; $$
&amp;gt; q^{\backslash n}(\boldsymbol{\theta})=\frac{q(\boldsymbol{\theta})}{\widetilde{f}&lt;/em&gt;{n}(\boldsymbol{\theta})}
&amp;gt; $$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Recompute $(\mu, v, Z)$ from $(\mathbf{\mu}^{\backslash n}, {v_l}^{\backslash n}, {v&lt;em&gt;r}^{\backslash n})$
$$
Z&lt;/em&gt;{n}=(1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r}\right)
$$
&amp;gt;$$
&amp;gt;\begin{aligned}
&amp;gt;Z&lt;/em&gt;{n} &amp;amp;=\int q^{\backslash n}(\boldsymbol{\theta}) f_{n}(\boldsymbol{\theta}) \mathrm{d} \boldsymbol{\theta} \
&amp;gt;&amp;amp;=\int q^{\backslash n}(\boldsymbol{\theta}) P(X|\mu) \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;
&amp;gt;&amp;amp;=\int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I}) \cdot { (1-w) \mathcal{A}(\mathbf{x_n} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{x_n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})} \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;
&amp;gt;&amp;amp;= (1-w)\int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I}) \mathcal{A}(\mathbf{x_n} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r}) \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;&amp;amp;+ w \int \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v_r^{\backslash n} \mathbf{I})
&amp;gt;\mathcal{A}(\mathbf{x_n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r})} \mathrm{d} \boldsymbol{\theta} &lt;br /&gt;
&amp;gt;&amp;amp;=(1-w) \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)
&amp;gt;\end{aligned}
&amp;gt;$$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;we  assumed that  $f&lt;em&gt;{0}(\boldsymbol{\theta})=p(\boldsymbol{\theta})$  and $
f&lt;/em&gt;{n}(\boldsymbol{\theta})=p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) = (1-w) \mathcal{A}(\mathbf{X} | \boldsymbol{\mu}, \mathbf{I_l}, \mathbf{I_r})+w \mathcal{A}(\mathbf{X} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r})
$, also $q(\boldsymbol{\theta})=\mathcal{A}(\boldsymbol{\theta} | \mathbf{m}, v_l \mathbf{I}, v&lt;em&gt;r \mathbf{I}) $
$$
\begin{aligned}
\rho&lt;/em&gt;{n} &amp;amp;=\frac{1}{Z&lt;em&gt;{n}}(1-w) \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)  &lt;br /&gt;
&amp;amp;= \frac{1}{Z&lt;/em&gt;{n}}(1-w)\cdot \frac{Z&lt;em&gt;n - w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)}{1-w} &lt;br /&gt;
&amp;amp;= 1 - \frac{w}{Z&lt;em&gt;n} \cdot \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I&lt;em&gt;r}\right)
\end{aligned}
$$
&amp;gt;Then our goal is to minimize:
&amp;gt;$$
&amp;gt;\mathrm{KL}\left(\frac{f&lt;/em&gt;{n}(\boldsymbol{\theta}) q^{\backslash n}(\boldsymbol{\theta})}{Z_{n}} | q^{\mathrm{new}}(\boldsymbol{\theta})\right)
&amp;gt;$$&lt;/p&gt;

&lt;p&gt;Basic rule for Asymmetric Gaussian:&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://stats.stackexchange.com/questions/27436/how-to-take-derivative-of-multivariate-normal-density&#34; target=&#34;_blank&#34;&gt;https://stats.stackexchange.com/questions/27436/how-to-take-derivative-of-multivariate-normal-density&lt;/a&gt;
$$
\nabla_{\boldsymbol{\mu}} \mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r})=&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r}) \cdot(\mathbf{x}-\boldsymbol{\mu}) \mathbf{v_l}^{-1}  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\mathcal{A}(\mathbf{x} | \boldsymbol{\mu}, \mathbf{v_l}, \mathbf{v_r}) \cdot(\mathbf{x}-\boldsymbol{\mu}) \mathbf{v_r}^{-1}  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right. &lt;br /&gt;
$$
So we compute the mean and variance:
$$
\begin{aligned}
\nabla&lt;em&gt;{\mathbf{\mu}^{\backslash n}} \ln Z&lt;/em&gt;{n} &amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \nabla&lt;/em&gt;{\mathbf{\mu}^{\backslash n}} Z_{n} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \nabla&lt;/em&gt;{\mathbf{\mu}^{\backslash n}} \int q^{\backslash n}(\boldsymbol{\theta})f_{n}(\boldsymbol{\theta}) d \boldsymbol{\theta} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \nabla&lt;/em&gt;{\mathbf{\mu}^{\backslash n}} \int q^{\backslash n}(\boldsymbol{\theta}) p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \int\left{\nabla&lt;/em&gt;{\mathbf{\mu}^{\backslash n}} q^{\backslash n}(\boldsymbol{\theta})\right} \cdot p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \int \frac{1}{v^{\backslash n}}\left(\boldsymbol{\theta}-\mathbf{\mu}^{\backslash n}\right) \cdot q^{\backslash n}(\boldsymbol{\theta})
\cdot p\left(\mathbf{x}&lt;/em&gt;{n} | \boldsymbol{\theta}\right) d \theta\&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \frac{1}{v^{\backslash n}} \cdot\left{\int \boldsymbol{\theta} \cdot q^{\backslash n}(\boldsymbol{\theta}) \cdot p\left(\mathbf{x}&lt;/em&gt;{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta}-\int \mathbf{\mu}^{\backslash n} \cdot q^{\backslash n}(\boldsymbol{\theta}) \cdot p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta}\right} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{v^{\backslash n}} \cdot\left{\mathbb{E}[\boldsymbol{\theta}]-\mathbf{\mu}^{\backslash n}\right} \&lt;/p&gt;

&lt;p&gt;&amp;amp;= \left{\mathbb{E}[\boldsymbol{\theta}]-\mathbf{\mu}^{\backslash n}\right} \cdot
\left{\begin{array}{ll}
\frac{1}{v&lt;em&gt;l^{\backslash n}} &amp;amp; \text { if } X&lt;/em&gt;{d}&amp;lt;\mu_{d} &lt;br /&gt;
\frac{1}{v&lt;em&gt;r^{\backslash n}}  &amp;amp; \text { if } X&lt;/em&gt;{d} \geqslant \mu_{d} &lt;br /&gt;
\end{array}\right.
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;$$
\text{According to the following}: &lt;br /&gt;
q^{\backslash n}(\boldsymbol{\theta})=\mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu^{\backslash n}}, v_l^{\backslash n} \mathbf{I}, v&lt;em&gt;r^{\backslash n} \mathbf{I}) &lt;br /&gt;
q^{\backslash n}(\boldsymbol{\theta}) \cdot p\left(\mathbf{x}&lt;/em&gt;{n} | \boldsymbol{\theta}\right)=Z_{n} \cdot q^{new}(\theta) &lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned} \mathbb{E}[\boldsymbol{\theta}]
&amp;amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \nabla&lt;em&gt;{\mathbf{\mu}^{\backslash n}} \ln Z&lt;/em&gt;{n} \
&amp;amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \frac{1}{Z&lt;em&gt;{n}} \nabla&lt;/em&gt;{\mathbf{\mu}^{\backslash n}} Z_n \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \frac{1}{Z&lt;em&gt;{n}} \nabla&lt;/em&gt;{\mathbf{\mu}^{\backslash n}} (1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)
\&lt;/p&gt;

&lt;p&gt;&amp;amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \frac{1}{Z&lt;em&gt;{n}}(1-w)
\nabla&lt;/em&gt;{\mathbf{\mu}^{\backslash n}}
\mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v_r^{\backslash n}+1\right) \mathbf{I}\right)&lt;/p&gt;

&lt;p&gt;%\cdot \frac{1}{v^{\backslash n}+1}\left(\mathbf{x}_{n}-\mathbf{\mu}^{\backslash n}\right)&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;&amp;amp;=\mathbf{\mu}^{\backslash n}+v^{\backslash n} \cdot \rho&lt;em&gt;{n} \cdot \frac{1}{v^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right)  \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\mathbf{\mu}^{\backslash n}+ \rho_{n} \cdot&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}
\frac{1}{v&lt;em&gt;l^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right) \cdot v_l^{\backslash n}&lt;/p&gt;

&lt;p&gt;&amp;amp; \text { if } X&lt;em&gt;{d}&amp;lt;\mu&lt;/em&gt;{d} &lt;br /&gt;
\frac{1}{v&lt;em&gt;r^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right)  \cdot v&lt;em&gt;l^{\backslash n}
&amp;amp; \text { if } X&lt;/em&gt;{d} \geqslant \mu_{d} &lt;br /&gt;
\end{array}\right.&lt;/p&gt;

&lt;p&gt;\&lt;/p&gt;

&lt;p&gt;\text{According to: }&amp;amp; \rho{n} = 1 - \frac{w}{Z&lt;em&gt;n} \cdot \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)    \&lt;/p&gt;

&lt;p&gt;&amp;amp;\text{Here we match first moment}:
\mathbb{E}[\boldsymbol{\theta}] = \mathbf{\mu^{new}}
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;Now we consider when:
$$
\text { if } X&lt;em&gt;{d}&amp;lt;\mu&lt;/em&gt;{d}
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned} \nabla_{v&lt;em&gt;l^{\backslash n}} \ln Z&lt;/em&gt;{n}
&amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \nabla&lt;/em&gt;{v&lt;em&gt;l^{\backslash n}} Z&lt;/em&gt;{n} \ &amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \nabla&lt;/em&gt;{v&lt;em&gt;l^{\backslash n}} \int q^{\backslash n}(\boldsymbol{\theta}) p\left(\mathbf{x}&lt;/em&gt;{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{Z&lt;em&gt;{n}} \cdot \int\left{\nabla&lt;/em&gt;{v&lt;em&gt;l^{\backslash n}} q^{\backslash n}(\boldsymbol{\theta})\right} p\left(\mathbf{x}&lt;/em&gt;{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{Z_{n}} \cdot \int\left{&lt;/p&gt;

&lt;p&gt;\frac{1}{2\left(v_l^{\backslash n}\right)^{2}}\left| \boldsymbol{\theta} - \mathbf{\mu}^{\backslash n}\right|^{2}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} }   }&lt;/p&gt;

&lt;p&gt;\right}&lt;/p&gt;

&lt;p&gt;q^{\backslash n}(\boldsymbol{\theta}) \cdot p\left(\mathbf{x}_{n} | \boldsymbol{\theta}\right) d \boldsymbol{\theta} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\int q^{\mathrm{new}}(\boldsymbol{\theta}) \cdot\left{\frac{1}{2\left(v_l^{\backslash n}\right)^{2}}\left(\mathbf{\mu}^{\backslash n}-\boldsymbol{\theta}\right)^{T}\left(\mathbf{\mu}^{\backslash n}-\boldsymbol{\theta}\right)-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }\right} d \boldsymbol{\theta} \&lt;/p&gt;

&lt;p&gt;&amp;amp;=\frac{1}{2\left(v_l^{\backslash n}\right)^{2}}\left{\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{T}\right]-2 \mathbb{E}[\boldsymbol{\theta}] \mathbf{\mu}^{\backslash n}+\left|\mathbf{\mu}^{\backslash n}\right|^{2}\right}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } } \end{aligned}
$$&lt;/p&gt;

&lt;p&gt;So we rearrange the above equation:
$$
\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{T}\right]=2\left(v&lt;em&gt;l^{\backslash n}\right)^{2} \cdot \nabla&lt;/em&gt;{v&lt;em&gt;l^{\backslash n}} \ln Z&lt;/em&gt;{n}+2 \mathbb{E}[\boldsymbol{\theta}] \mathbf{m}^{\backslash n}-\left|\mathbf{m}^{\backslash n}\right|^{2}+ \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v&lt;em&gt;r^{\backslash n} } }
$$
Also according to:
$$
Z&lt;/em&gt;{n}=(1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v&lt;em&gt;r^{\backslash n}+1\right) \mathbf{I}\right)+w \mathcal{A}\left(\mathbf{x}&lt;/em&gt;{n} | \mathbf{0}, a \mathbf{I_l}, a \mathbf{I_r}\right)
$$&lt;/p&gt;

&lt;p&gt;$$
\nabla_{v&lt;em&gt;l^{\backslash n}} \ln Z&lt;/em&gt;{n}  = (1-w) \mathcal{A}\left(\mathbf{x}_{n} | \mathbf{\mu}^{\backslash n},\left(v_l^{\backslash n}+1\right) \mathbf{I}, \left(v_r^{\backslash n}+1\right) \mathbf{I}\right) \cdot \&lt;/p&gt;

&lt;p&gt;\left(
\frac{1}{2\left(v_l^{\backslash n} + 1\right)^{2}}\left| \mathbf{x_n} - \mathbf{\mu}^{\backslash n}\right|^{2}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }&lt;/p&gt;

&lt;p&gt;\right)  \&lt;/p&gt;

&lt;p&gt;= \rho_n \cdot \left(
\frac{1}{2\left(v_l^{\backslash n} + 1\right)^{2}}\left| \mathbf{x_n} - \mathbf{\mu}^{\backslash n}\right|^{2}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }&lt;/p&gt;

&lt;p&gt;\right) &lt;br /&gt;
$$&lt;/p&gt;

&lt;p&gt;According to the bellow formula:
$$
v \mathbf{I}=\mathbb{E}\left[\boldsymbol{\theta} \boldsymbol{\theta}^{T}\right]-\mathbb{E}[\boldsymbol{\theta}] \mathbb{E}\left[\boldsymbol{\theta}^{T}\right]
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
v_l^{new} &amp;amp;=\frac{1}{D} \cdot\left{\mathbb{E}\left[\boldsymbol{\theta}^{T} \boldsymbol{\theta}\right]-\mathbb{E}\left[\boldsymbol{\theta}^{T}\right] \mathbb{E}[\boldsymbol{\theta}]\right}=\frac{1}{D} \cdot\left{\mathbb{E}\left[\boldsymbol{\theta}^{T} \boldsymbol{\theta}\right]-|\mathbb{E}[\boldsymbol{\theta}]|^{2}\right} &lt;br /&gt;
&amp;amp;=\frac{1}{D} \cdot\left{ 2\left(v&lt;em&gt;l^{\backslash n}\right)^{2} \cdot \nabla&lt;/em&gt;{v&lt;em&gt;l^{\backslash n}} \ln Z&lt;/em&gt;{n}+2 \mathbb{E}[\boldsymbol{\theta}] \mathbf{\mu}^{\backslash n}-\left|\mathbf{\mu}^{\backslash n}\right|^{2}&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } } - |\mathbb{E}[\boldsymbol{\theta}]|^{2} \right}&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&amp;amp;=\frac{1}{D} \cdot\left{ 2\left(v&lt;em&gt;l^{\backslash n}\right)^{2} \cdot \nabla&lt;/em&gt;{v&lt;em&gt;l^{\backslash n}} \ln Z&lt;/em&gt;{n}
- \left|\mathbb{E}[\boldsymbol{\theta}]-\mathbf{\mu}^{\backslash n}\right|^{2} + \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }
\right} \&lt;/p&gt;

&lt;p&gt;&amp;amp;= \frac{1}{D} \cdot\left{ 2\left(v&lt;em&gt;l^{\backslash n}\right)^{2} \cdot \nabla&lt;/em&gt;{v&lt;em&gt;l^{\backslash n}} \ln Z&lt;/em&gt;{n}
- \left|
\rho_n \cdot \frac{1}{v&lt;em&gt;l^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right) \cdot v_l^{\backslash n}
\right|^{2} + \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }
\right}&lt;/p&gt;

&lt;p&gt;\end{aligned}
$$
substitute $\nabla_{v&lt;em&gt;l^{\backslash n}} \ln Z&lt;/em&gt;{n}$:
$$
\begin{aligned}
v_l^{new} &amp;amp;=
\frac{1}{D} \cdot\left{ 2\left(v&lt;em&gt;l^{\backslash n}\right)^{2} \cdot \nabla&lt;/em&gt;{v&lt;em&gt;l^{\backslash n}} \ln Z&lt;/em&gt;{n}
- \left|
\rho_n \cdot \frac{1}{v&lt;em&gt;l^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right) \cdot v_l^{\backslash n}
\right|^{2} + \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }
\right} \&lt;/p&gt;

&lt;p&gt;&amp;amp;= \frac{1}{D} \cdot\left{ 2\left(v_l^{\backslash n}\right)^{2} \cdot&lt;/p&gt;

&lt;p&gt;\rho_n \cdot \left(
\frac{1}{2\left(v_l^{\backslash n} + 1\right)^{2}}\left| \mathbf{x_n} - \mathbf{\mu}^{\backslash n}\right|^{2}-\frac{D}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }&lt;/p&gt;

&lt;p&gt;\right)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;\left|
\rho_n \cdot \frac{1}{v&lt;em&gt;l^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right) \cdot v_l^{\backslash n}
\right|^{2} + \frac{D \cdot \left(v_l^{\backslash n}\right)^{2}}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }
\right} &lt;br /&gt;
&amp;amp;=\frac{\left(v_l^{\backslash n}\right)^{2} - 2 \left(v_l^{\backslash n}\right)^{2} \rho_n}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v&lt;em&gt;r^{\backslash n} } } + \rho&lt;/em&gt;{n}\left(1-\rho_{n}\right)
\frac{\left(v&lt;em&gt;l^{\backslash n}\right)^{2}\left|\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right|^{2}}{D\left(v_l^{\backslash n}+1\right)^{2}}&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;\end{aligned}
$$
So in conclusions: we have:&lt;/p&gt;

&lt;p&gt;$$
\mathbf{\mu^{new}}=\mathbf{\mu}^{\backslash n}+&lt;/p&gt;

&lt;p&gt;\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\rho_{n} \frac{v_l^{\backslash n}}{v&lt;em&gt;l^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\rho_{n} \frac{v_r^{\backslash n}}{v&lt;em&gt;r^{\backslash n}+1}\left(\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\left{\begin{array}{ll}
v_l^{new}= \frac{\left(v_l^{\backslash n}\right)^{2} - 2 \left(v_l^{\backslash n}\right)^{2} \rho_n}{4 v_l^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v&lt;em&gt;r^{\backslash n} } } + \rho&lt;/em&gt;{n}\left(1-\rho_{n}\right)
\frac{\left(v&lt;em&gt;l^{\backslash n}\right)^{2}\left|\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right|^{2}}{D\left(v_l^{\backslash n}+1\right)^{2}}&lt;/p&gt;

&lt;p&gt;&amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;v_r^{new}=
\frac{\left(v_r^{\backslash n}\right)^{2} - 2 \left(v_r^{\backslash n}\right)^{2} \rho_n}{4 v_r^{\backslash n} + \sqrt{ v_l^{\backslash n} \cdot v_r^{\backslash n} } }&lt;/p&gt;

&lt;p&gt;+\rho&lt;em&gt;{n}\left(1-\rho&lt;/em&gt;{n}\right) \frac{\left(v&lt;em&gt;r^{\backslash n}\right)^{2}\left|\mathbf{x}&lt;/em&gt;{n}-\mathbf{\mu}^{\backslash n}\right|^{2}}{D\left(v_r^{\backslash n}+1\right)^{2}}
&amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$
- Evaluate and store the new factor&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Update formula for $\hat{f}&lt;em&gt;i$
$$
\left{\begin{array}{ll}
\left({v&lt;/em&gt;{l_n}}\right)^{-1}={(v_l^{new})}^{-1}-({v&lt;em&gt;l}^{\backslash n})^{-1} &amp;amp; \text { if } X&amp;lt;\mu &lt;br /&gt;
\left({v&lt;/em&gt;{r_n}}\right)^{-1}={(v_r^{new})}^{-1}-({v_r}^{ \backslash n})^{-1} &amp;amp; \text { if } X&amp;gt;\mu &lt;br /&gt;
\end{array}\right.
$$&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;$$
\mathbf{\mu}_{n}=\mathbf{\mu}^{\backslash n}+
\left{\begin{array}{ll}&lt;/p&gt;

&lt;p&gt;\left(v_{n}+v^{\backslash n}\right)\left(v^{\backslash n}\right)^{-1}\left(\mathbf{\mu}^{\mathrm{new}}-\mathbf{\mu}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;lt;\mu \&lt;/p&gt;

&lt;p&gt;\left(v_{n}+v^{\backslash n}\right)\left(v^{\backslash n}\right)^{-1}\left(\mathbf{\mu}^{\mathrm{new}}-\mathbf{\mu}^{\backslash n}\right)  &amp;amp; \text { if } X&amp;gt;\mu \&lt;/p&gt;

&lt;p&gt;\end{array}\right.
$$&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
&amp;amp; \widetilde{f}&lt;em&gt;{n}(\boldsymbol{\theta})=Z&lt;/em&gt;{n} \frac{q^{\mathrm{new}}(\boldsymbol{\theta})}{q^{\backslash n}(\boldsymbol{\theta})} \&lt;/p&gt;

&lt;p&gt;\Rightarrow &amp;amp; Z_n q^{\mathrm{new}}(\boldsymbol{\theta}) = s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right) q^{\backslash n}(\boldsymbol{\theta}) =
s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)
\mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) \&lt;/p&gt;

&lt;p&gt;\Rightarrow &amp;amp; \int Z_n q^{\mathrm{new}}(\boldsymbol{\theta}) d\theta =
\int s_n \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}&lt;em&gt;n, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right)
\mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) d \theta \&lt;/p&gt;

&lt;p&gt;\Rightarrow &amp;amp; Z_n = s_n \int q^{\mathrm{new}}(\boldsymbol{\theta}) d\theta =&lt;/p&gt;

&lt;p&gt;\int s_n \mathcal{A}\left( \mathbf{\mu}&lt;em&gt;n - \boldsymbol{\theta} | 0, \mathbf{v&lt;/em&gt;{r&lt;em&gt;n} I}, \mathbf{v&lt;/em&gt;{l_n} I} \right) \mathcal{A}\left(\boldsymbol{\theta} | \mathbf{\mu}^{\backslash n}, \mathbf{v_r^{\backslash n} I}, \mathbf{v_l^{\backslash n} I} \right) d \theta \&lt;/p&gt;

&lt;p&gt;\Rightarrow &amp;amp; Z_n  = s_n \mathcal{A}\left(\mathbf{\mu}_n | \mathbf{\mu}^{\backslash n}, \mathbf{(v&lt;em&gt;r^{\backslash n}+ v&lt;/em&gt;{r_n}) I}, \mathbf{(v&lt;em&gt;l^{\backslash n}+v&lt;/em&gt;{r_n}) I} \right)
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;$$
s_n  = \frac{Z_n} {\mathcal{A}\left(\mathbf{\mu}_n | \mathbf{\mu}^{\backslash n}, \mathbf{(v&lt;em&gt;r^{\backslash n}+ v&lt;/em&gt;{r_n}) I}, \mathbf{(v&lt;em&gt;l^{\backslash n}+v&lt;/em&gt;{r_n}) I} \right)}
$$&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Evaluate the approximation to the model evidence - Posterior probability. (When $(\mathbf{\mu}_n, {v&lt;em&gt;l}&lt;/em&gt; n, {v_r}_n, S_n)$ unchanged )&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;$$
p(X) \simeq q(\boldsymbol{\theta})= \mathcal{A}(\boldsymbol{\theta} | \mathbf{\mu},  \mathbf{v_l},  \mathbf{v&lt;em&gt;r}) =
\prod&lt;/em&gt;{n=0}^{N} \widetilde{f}&lt;em&gt;{n}(\boldsymbol{\theta})=f&lt;/em&gt;{0}(\boldsymbol{\theta}) \prod&lt;em&gt;{n=1}^{N} \widetilde{f}&lt;/em&gt;{n}(\boldsymbol{\theta})&lt;/p&gt;

&lt;p&gt;=  \mathcal{A}(\boldsymbol{\theta} | \mathbf{0}, b \mathbf{I_l}, b \mathbf{I&lt;em&gt;r})\cdot \prod&lt;/em&gt;{i=1}^{N} \mathcal{A}\left(\boldsymbol{\mu}_{i}, \mathbf{v&lt;em&gt;l}&lt;/em&gt;{i}, \mathbf{v&lt;em&gt;r}&lt;/em&gt;{i}\right)
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://faithio.cn/post/logistic-regression-solvers-defintions-in-sklearn/</link>
      <pubDate>Sat, 29 Feb 2020 02:46:04 -0500</pubDate>
      <guid>https://faithio.cn/post/logistic-regression-solvers-defintions-in-sklearn/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;./image-20200417094156740.png&#34; alt=&#34;image-20200417094156740&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094302078.png&#34; alt=&#34;image-20200417094302078&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094413409.png&#34; alt=&#34;image-20200417094413409&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094619802.png&#34; alt=&#34;image-20200417094619802&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094717318.png&#34; alt=&#34;image-20200417094717318&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094753505.png&#34; alt=&#34;image-20200417094753505&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094841163.png&#34; alt=&#34;image-20200417094841163&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094900567.png&#34; alt=&#34;image-20200417094900567&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094920044.png&#34; alt=&#34;image-20200417094920044&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417094943372.png&#34; alt=&#34;image-20200417094943372&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417095008190.png&#34; alt=&#34;image-20200417095008190&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;./image-20200417095025101.png&#34; alt=&#34;image-20200417095025101&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;A hypothesis &lt;code&gt;h(x)&lt;/code&gt;, takes an &lt;em&gt;input&lt;/em&gt; and gives us the &lt;em&gt;estimated output value&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This hypothesis can be a as simple as a one variable linear equation, .. up to a very complicated and long multivariate equation with respect to the type of the algorithm weâ€™re using (&lt;em&gt;i.e. linear regression, logistic regression..etc&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9e4pcalj308k05xgls.jpg&#34; alt=&#34;h(x)&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Our task is to find the &lt;strong&gt;best Parameters&lt;/strong&gt; (a.k.a Thetas or Weights) that give us the &lt;strong&gt;least error&lt;/strong&gt; in predicting the output. We call this error a &lt;strong&gt;Cost or Loss Function&lt;/strong&gt; and apparently our goal is to &lt;strong&gt;minimize&lt;/strong&gt; it in order to get the best predicted output!&lt;/p&gt;

&lt;p&gt;One more thing to recall, that the relation between the parameter value and its effect on the cost function (i.e. the error) looks like a &lt;strong&gt;bell curve&lt;/strong&gt; (i.e. &lt;strong&gt;Quadratic&lt;/strong&gt;; recall this because itâ€™s very important) .&lt;/p&gt;

&lt;p&gt;So if we start at any point in that curve and if we keep taking the derivative (i.e. tangent line) of each point we stop at, we will end up at what so called the &lt;strong&gt;Global Optima&lt;/strong&gt; as shown in this image:
&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9empzi9j30bm07twem.jpg&#34; alt=&#34;J(w) bell curve&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If we take the partial derivative at minimum cost point (i.e. global optima) we find the &lt;strong&gt;slope&lt;/strong&gt; of the tangent line = &lt;strong&gt;0&lt;/strong&gt; (then we know that we reached our target).&lt;/p&gt;

&lt;p&gt;Thatâ€™s valid only if we have &lt;em&gt;Convex&lt;/em&gt; Cost Function, but if we donâ€™t, we may end up stuck at what so called &lt;strong&gt;Local Optima&lt;/strong&gt;; consider this non-convex function:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9ffxzczj30ac07qwex.jpg&#34; alt=&#34;non-convex&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Now you should have the intuition about the hack relationship between what we are doing and the terms: &lt;em&gt;Deravative&lt;/em&gt;, &lt;em&gt;Tangent Line&lt;/em&gt;, &lt;em&gt;Cost Function&lt;/em&gt;, &lt;em&gt;Hypothesis&lt;/em&gt; ..etc.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Side Note: The above mentioned intuition also related to the Gradient Descent Algorithm (see later).&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Linear Approximation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Given a function, &lt;code&gt;f(x)&lt;/code&gt;, we can find its tangent at &lt;code&gt;x=a&lt;/code&gt;. The equation of the tangent line L(x) is: &lt;code&gt;L(x)=f(a)+fâ€²(a)(xâˆ’a)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Take a look at the following graph of a function and its tangent line:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9dc2jkaj309r05tq2w.jpg&#34; alt=&#34;tangent line&#34; /&gt;&lt;/p&gt;

&lt;p&gt;From this graph we can see that near &lt;code&gt;x=a&lt;/code&gt;, the tangent line and the function have nearly the same graph. On occasion we will use the tangent line, &lt;code&gt;L(x)&lt;/code&gt;, as an approximation to the function, &lt;code&gt;f(x)&lt;/code&gt;, near &lt;code&gt;x=a&lt;/code&gt;. In these cases we call the tangent line the linear approximation to the function at &lt;code&gt;x=a&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Quadratic Approximation:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Same like linear approximation but this time we are dealing with a curve but we &lt;strong&gt;cannot&lt;/strong&gt; find the point near to &lt;strong&gt;0&lt;/strong&gt; by using the tangent line.&lt;/p&gt;

&lt;p&gt;Instead, we use a &lt;strong&gt;parabola&lt;/strong&gt; (&lt;em&gt;which is a curve where any point is at an equal distance from a fixed point or a fixed straight line&lt;/em&gt;), like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9dhnyppj30ay075mxe.jpg&#34; alt=&#34;quadratic function&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And in order to fit a good parabola, both parabola and quadratic function should have same value, same first derivative, AND second derivative, &amp;hellip; the formula will be (&lt;em&gt;just out of curiosity&lt;/em&gt;): &lt;code&gt;Qa(x) = f(a) + f&#39;(a)(x-a) + f&#39;&#39;(a)(x-a)2/2&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Now we should be ready to do the comparison in details.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;comparison-between-the-methods&#34;&gt;Comparison between the methods&lt;/h2&gt;

&lt;h3 id=&#34;1-newton-s-method-newton-cg&#34;&gt;&lt;strong&gt;1. Newtonâ€™s Method(newton-cg):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Recall the motivation for gradient descent step at x: we minimize the quadratic function (i.e. Cost Function).&lt;/p&gt;

&lt;p&gt;Newtonâ€™s method uses in a sense a &lt;strong&gt;better&lt;/strong&gt; quadratic function minimisation. A better because it uses the quadratic approximation (i.e. first AND &lt;em&gt;second&lt;/em&gt; partial derivatives).&lt;/p&gt;

&lt;p&gt;You can imagine it as a twisted Gradient Descent with The Hessian (&lt;em&gt;The Hessian is a square matrix of second-order partial derivatives of order nxn&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Moreover, the geometric interpretation of Newton&amp;rsquo;s method is that at each iteration one approximates &lt;code&gt;f(x)&lt;/code&gt; by a quadratic function around &lt;code&gt;xn&lt;/code&gt;, and then takes a step towards the maximum/minimum of that quadratic function (in higher dimensions, this may also be a saddle point). Note that if &lt;code&gt;f(x)&lt;/code&gt; happens to be a quadratic function, then the exact extremum is found in one step.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Itâ€™s computationally &lt;strong&gt;expensive&lt;/strong&gt; because of The Hessian Matrix (i.e. second partial derivatives calculations).&lt;/li&gt;
&lt;li&gt;It attracts to &lt;strong&gt;Saddle Points&lt;/strong&gt; which are common in multivariable optimization (i.e. a point its partial derivatives disagree over whether this input should be a maximum or a minimum point!).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;2-limited-memory-broyden-fletcher-goldfarb-shanno-algorithm-lbfgs&#34;&gt;&lt;strong&gt;2. Limited-memory Broydenâ€“Fletcherâ€“Goldfarbâ€“Shanno Algorithm(lbfgs):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In a nutshell, it is analogue of the Newtonâ€™s Method but here the Hessian matrix is &lt;strong&gt;approximated&lt;/strong&gt; using updates specified by gradient evaluations (or approximate gradient evaluations). In other words, using an estimation to the inverse Hessian matrix.&lt;/p&gt;

&lt;p&gt;The term Limited-memory simply means it stores only a few vectors that represent the approximation implicitly.&lt;/p&gt;

&lt;p&gt;If I dare say that when dataset is &lt;strong&gt;small&lt;/strong&gt;, L-BFGS relatively performs the best compared to other methods especially it saves a lot of memory, however there are some â€œ*serious*â€ drawbacks such that if it is unsafeguarded, it may not converge to anything.&lt;/p&gt;

&lt;h3 id=&#34;3-a-library-for-large-linear-classification-liblinear&#34;&gt;&lt;strong&gt;3. A Library for Large Linear Classification(liblinear):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Itâ€™s a linear classification that supports logistic regression and linear support vector machines (&lt;em&gt;A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics i.e feature value&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;The solver uses a coordinate descent (CD) algorithm that solves optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;LIBLINEAR&lt;/code&gt; is the winner of ICML 2008 large-scale learning challenge. It applies &lt;em&gt;Automatic parameter selection&lt;/em&gt; (a.k.a L1 Regularization) and itâ€™s recommended when you have high dimension dataset (&lt;em&gt;recommended for solving large-scale classification problems&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It may get stuck at a &lt;em&gt;non-stationary point&lt;/em&gt; (i.e. non-optima) if the level curves of a function are not smooth.&lt;/li&gt;
&lt;li&gt;Also cannot run in parallel.&lt;/li&gt;
&lt;li&gt;It cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a â€œone-vs-restâ€ fashion so separate binary classifiers are trained for all classes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;Side note: According to Scikit Documentation: The â€œliblinearâ€ solver is used by default for historical reasons.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;4-stochastic-average-gradient-sag&#34;&gt;&lt;strong&gt;4. Stochastic Average Gradient(sag):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;SAG method optimizes the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method&amp;rsquo;s iteration cost is independent of the number of terms in the sum. However, by &lt;strong&gt;incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate&lt;/strong&gt; than black-box SG methods.&lt;/p&gt;

&lt;p&gt;It is &lt;strong&gt;faster&lt;/strong&gt; than other solvers for &lt;em&gt;large&lt;/em&gt; datasets, when both the number of samples and the number of features are large.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;It only supports L2 penalization.&lt;/li&gt;
&lt;li&gt;Its memory cost of &lt;code&gt;O(N)&lt;/code&gt;, which can make it impractical for large N (&lt;em&gt;because it remembers the most recently computed values for approx. all gradients&lt;/em&gt;).&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;5-saga-saga&#34;&gt;&lt;strong&gt;5. SAGA(saga):&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;The SAGA solver is a &lt;em&gt;variant&lt;/em&gt; of SAG that also supports the non-smooth &lt;em&gt;penalty=l1&lt;/em&gt; option (i.e. L1 Regularization). This is therefore the solver of choice for &lt;strong&gt;sparse&lt;/strong&gt; multinomial logistic regression and itâ€™s also suitable &lt;strong&gt;very Large&lt;/strong&gt; dataset.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Side note: According to Scikit Documentation: The SAGA solver is often the best choice.&lt;/em&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;

&lt;p&gt;The following table is taken from &lt;a href=&#34;http://scikit-learn.org/stable/modules/linear_model.html&#34; target=&#34;_blank&#34;&gt;Scikit Documentation&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://tva1.sinaimg.cn/large/006y8mN6gy1g8a9g3deuhj30ik04ejs7.jpg&#34; alt=&#34;Solver Comparison&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
