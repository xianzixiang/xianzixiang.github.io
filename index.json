[{"authors":["admin"],"categories":null,"content":"Zixiang Xian is a master student of Machine Learning at Concordia University AI Lab. His research interests include Machine Learning and Computer Vision. He is under the supervision of Professor Nizar Bouguila.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582962603,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://faithio.cn/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Zixiang Xian is a master student of Machine Learning at Concordia University AI Lab. His research interests include Machine Learning and Computer Vision. He is under the supervision of Professor Nizar Bouguila.","tags":null,"title":"Zixiang Xian","type":"authors"},{"authors":["zi_xian"],"categories":null,"content":"Xian Zixiang is a master student of Machine Learning at Concordia University AI Lab. His research interests include Machine Learning and Computer Vision. He is under the supervision of Professor Nizar Bouguila.\nBefore his graduate study, he is a senior backend engineer at Kingsoft Company, skilling at Golang, Java, JavaScript, and Python.\nHe is so motivative and passion for exploring the new world of computer science.\nFeel free to contact me.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582962603,"objectID":"b531f07f2ad34c1997a37315504c48d8","permalink":"https://faithio.cn/authors/zi_xian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zi_xian/","section":"authors","summary":"Xian Zixiang is a master student of Machine Learning at Concordia University AI Lab. His research interests include Machine Learning and Computer Vision. He is under the supervision of Professor Nizar Bouguila.\nBefore his graduate study, he is a senior backend engineer at Kingsoft Company, skilling at Golang, Java, JavaScript, and Python.\nHe is so motivative and passion for exploring the new world of computer science.\nFeel free to contact me.","tags":null,"title":"XIAN ZIXIANG","type":"authors"},{"authors":[],"categories":[],"content":" Pre-training  unlabled data有大量，可以先训练好，最后再用labled data train 一下就好\n CNN ","date":1586485145,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586648375,"objectID":"947f4a41a0400747495afbd92f3bb7e7","permalink":"https://faithio.cn/post/auto-encoder/","publishdate":"2020-04-09T22:19:05-04:00","relpermalink":"/post/auto-encoder/","section":"post","summary":"Pre-training unlabled data有大量，可以先训练好，最后再用labled data train 一下就好 CNN","tags":[],"title":"Auto Encoder","type":"post"},{"authors":[],"categories":[],"content":" HMM Step1 Step2 ","date":1586466969,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586648375,"objectID":"dfad30ea5c8945132281581b39194905","permalink":"https://faithio.cn/post/structured-learning/","publishdate":"2020-04-09T17:16:09-04:00","relpermalink":"/post/structured-learning/","section":"post","summary":" HMM Step1 Step2 ","tags":[],"title":"Structured Learning","type":"post"},{"authors":[],"categories":[],"content":" http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN%20backprop.ecm.mp4/\nGate function uses sigmoid, but not ReLu\nRNN 的big problems  RNN不好训练不是因为activation function，而是来自time sequence，同样的weight，在不同的时间点反复使用\n LSTM 可以让RNN的error surface 不那么崎岖，可以把平坦的地方拿掉，gradient vanishing but not gradient explode。\nlearning rate 小的时候训练，没有平台训练。\n为什么handling gradient vanishing？\nRNN：memory的值每次都会被洗掉\nLSTM：memory 乘以forget gate再加起input的值， memory和input相加的。只要forget gate open就不会清除memory，所以不会有gradient vanishing。\n 建议不要给forget gate很大的bias，确保它多数情况开启。\n  参数比较少，LSTM如果overfiting，可以尝试GRU。\nGRU： input gate/ forget gate联动，input 打开的时候，forget关闭，洗掉memory。要清掉memory才能放新值。\n  如果用identity matrix 初始化，就是ReLu 作为activation function，效果很好。吊打LSTM！\n如果是random，就用sigmoid 作为activation function。\n Attention-based model(Memory) RNN output \u0026ndash;\u0026gt; HMM/CRF(input)\n","date":1586403161,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586466803,"objectID":"726ed725854b5f265031c5e90bcb7807","permalink":"https://faithio.cn/post/rnn/","publishdate":"2020-04-08T23:32:41-04:00","relpermalink":"/post/rnn/","section":"post","summary":"http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/Lecture/DNN%20backprop.ecm.mp4/ Gate function uses sigmoid, but not ReLu RNN 的big problems RNN不好训练不是因为activation function，而是来自time sequence，同样的weight","tags":[],"title":"RNN","type":"post"},{"authors":[],"categories":[],"content":" dropout linear 好 \u0026ndash;\u0026gt;ReLU/maxout 好\n ","date":1586399414,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586466803,"objectID":"ef8aeccdbe56db07f0af0a93a9e02c54","permalink":"https://faithio.cn/post/cnn/","publishdate":"2020-04-08T22:30:14-04:00","relpermalink":"/post/cnn/","section":"post","summary":"dropout linear 好 \u0026ndash;\u0026gt;ReLU/maxout 好","tags":[],"title":"CNN","type":"post"},{"authors":[],"categories":[],"content":"$$ \\begin{array}{l}P(Y | X)=\\frac{P(X | Y) P(Y)}{P(X)} \\propto(P(X | Y) \\cdot P(Y) \\ M A P \\ P(Y=0 | X) \\Rightarrow P(Y=1 | X)\\end{array} $$\n","date":1586273008,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586273848,"objectID":"ac7cff08c56dba1284b27c934a68fb6b","permalink":"https://faithio.cn/post/linear-classification/","publishdate":"2020-04-07T11:23:28-04:00","relpermalink":"/post/linear-classification/","section":"post","summary":"$$ \\begin{array}{l}P(Y | X)=\\frac{P(X | Y) P(Y)}{P(X)} \\propto(P(X | Y) \\cdot P(Y) \\ M A P \\ P(Y=0 | X) \\Rightarrow P(Y=1 | X)\\end{array} $$","tags":[],"title":"Linear Classification","type":"post"},{"authors":[],"categories":[],"content":"","date":1586272919,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586273848,"objectID":"039a5aded1ea3595932b885cc9d1a1b6","permalink":"https://faithio.cn/post/svm/","publishdate":"2020-04-07T11:21:59-04:00","relpermalink":"/post/svm/","section":"post","summary":"","tags":[],"title":"SVM","type":"post"},{"authors":[],"categories":[],"content":" click here for link\nGAN $$ \\mathcal{KL}(p|q) = E_p[\\log \\frac{p}{q}] \\ \u0026mdash;-\nP_g \\rightarrow \\theta_g \\text{ MLE: } \\thetag = \\arg \\max{\\thetag} \\sum{i=1}^N \\log P_g(xi) = \\arg \\min \\mathcal{KL}(P{data}|P_g) $$\n","date":1586184049,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586648375,"objectID":"1d122806ed4491e33c03463a7e4f4592","permalink":"https://faithio.cn/post/generative-adversarial-network/","publishdate":"2020-04-06T10:40:49-04:00","relpermalink":"/post/generative-adversarial-network/","section":"post","summary":"click here for link\nGAN $$ \\mathcal{KL}(p|q) = E_p[\\log \\frac{p}{q}] \\ \u0026mdash;-\nP_g \\rightarrow \\theta_g \\text{ MLE: } \\thetag = \\arg \\max{\\thetag} \\sum{i=1}^N \\log P_g(xi) = \\arg \\min \\mathcal{KL}(P{data}|P_g) $$","tags":[],"title":"Generative Adversarial Network","type":"post"},{"authors":[],"categories":[],"content":" Neural Networks Basics Logistic Regression as a Neural Network $$ \\hat{y}=\\sigma\\left(w^{T} x+b\\right), \\text { where } \\sigma(z)=\\frac{1}{1+e^{-z}} \\\n\\text { Given }\\left{\\left(x^{(1)}, y^{(1)}\\right), \\ldots,\\left(x^{(m)}, y^{(m)}\\right)\\right}, \\text { want } \\hat{y}^{(i)} \\approx y^{(i)} $$\nLoss(error) function:\nDon\u0026rsquo;t use this, non-convex $$ \\ell(\\hat{y}, y)=\\frac{1}{2}(\\hat{y}-y)^{2} $$ The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set. $$ J(w, b)=\\frac{1}{m} \\sum{i=1}^{m} \\mathcal{L}\\left(\\hat{y}^{(i)}, y^{(i)}\\right)=-\\frac{1}{m} \\sum{i=1}^{m} y^{(i)} \\log \\widehat{y}^{(i)}+\\left(1-y^{(i)}\\right) \\log \\left(1-\\hat{y}^{(i)}\\right) $$\n$$ \\text { Want to find } w, b \\text { that minimize } J(w, b) $$\nGradient Desent $$ \\begin{array}{l}\\text { Step } 1: \\frac{d L}{d a} \\ L=-(y \\times \\log (a)+(1-y) \\times \\log (1-a)) \\ \\frac{d L}{d a}=-y \\times \\frac{1}{a}-(1-y) \\times \\frac{1}{1-a} \\times-1\\end{array} $$\n$$ \\begin{align} \\dfrac{d}{dx} \\sigma(x) \u0026amp;= \\dfrac{d}{dx} \\left[ \\dfrac{1}{1 + e^{-x}} \\right] \u0026amp;= \\dfrac{d}{dx} \\left( 1 + \\mathrm{e}^{-x} \\right)^{-1} \u0026amp;= -(1 + e^{-x})^{-2}(-e^{-x}) \u0026amp;= \\dfrac{e^{-x}}{\\left(1 + e^{-x}\\right)^2} \u0026amp;= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{e^{-x}}{1 + e^{-x}} \u0026amp;= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\dfrac{(1 + e^{-x}) - 1}{1 + e^{-x}} \u0026amp;= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( \\dfrac{1 + e^{-x}}{1 + e^{-x}} - \\dfrac{1}{1 + e^{-x}} \\right) \u0026amp;= \\dfrac{1}{1 + e^{-x}\\ } \\cdot \\left( 1 - \\dfrac{1}{1 + e^{-x}} \\right) \u0026amp;= \\sigma(x) \\cdot (1 - \\sigma(x)) \\end{align} $$\n$$ \\begin{array}{l}\\text { In the previous video, Andrew refers to } d z=a(1-a) \\ \\text { Note that Andrew is using \u0026ldquo;dz\u0026rdquo; as a shorthand to refer to } \\frac{d a}{d z}=a(1-a) \\text { . } \\ \\text { To clarify, earlier in this week\u0026rsquo;s videos, Andrew used the name \u0026ldquo;dz\u0026rdquo; to refer to a different derivative: } \\frac{d L}{d z}=a-y . \\ \\text { Recall that the relationship between } \\frac{d L}{d z} \\text { and } \\frac{d a}{d z} \\text { is: } \\ \\frac{d L}{d z}=\\frac{d L}{d a} \\times \\frac{d a}{d z} \\ \\frac{d L}{d z}=\\frac{a-y}{a(1-a)} \\times a(1-a)=a-y\\end{array} $$\nVectorization Code For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px $$ num_px $$ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns.\nExercise: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px $$ num_px $$ 3, 1).\nA trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b$$c$$d, a) is to use:\nX_flatten = X.reshape(X.shape[0], -1).T # X.T is the transpose of X  To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\nOne common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel).\nLet\u0026rsquo;s standardize our dataset.\nNeural Network Shallow neural networks Representation X：竖方向 features\n横方向：training example\nActivation function：\n Sigmoid: 如果output是binary classification，可以考虑sigmoid function，一般nerver used tanh function： ReLU Rectified Linear Unit $a = \\max(0, z)$ : by default, 很多人用，faster，slope 1，or 0 Leaky ReLU  Why need non-linear activation functions Derivatives of activation functions Gradient descent for Neural Networks np.sum(keepdims=True): prevent rank one arrays\nBackpropagation intuition https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60\nhttps://www.coursera.org/learn/neural-networks-deep-learning/discussions/weeks/3/threads/a38VuhyMEei5zw6yFhWyOg\nRandom Initialization LR can be initialized as 0, but not neural network\nSupplement 基础 graph LR; MachineLearning--\u0026gt;频率/统计机器学习; 频率/统计机器学习--\u0026gt;正则化/L1/L2 频率/统计机器学习--\u0026gt;核化 频率/统计机器学习--\u0026gt;集成化 频率/统计机器学习--\u0026gt;层次化 层次化--\u0026gt;MultiayerPercepton/MLP 层次化--\u0026gt;Autoencoder 层次化--\u0026gt;CNN 层次化--\u0026gt;RNN MachineLearning--\u0026gt;贝叶斯派/PGM 贝叶斯派/PGM--\u0026gt;BayesianNetwork/有向图 贝叶斯派/PGM--\u0026gt;MarkovNetwork/无向图 贝叶斯派/PGM--\u0026gt;MixedNetwork/有向图无向图 BayesianNetwork/有向图--\u0026gt;DeepDirectedNetwork DeepDirectedNetwork--\u0026gt;SigmoidBeliefNetwork DeepDirectedNetwork--\u0026gt;VAE DeepDirectedNetwork--\u0026gt;GAN MarkovNetwork/无向图--\u0026gt;DeepBoltzmannNetwork MixedNetwork/有向图无向图--\u0026gt;DeepBeliefNetwork  统计机器学习  正则化：Loss Function + regularizer(L1/L2)\n 核化: Kernel SVM\n 集成化: AdaBoost, RandomForest\n 层次化: Neural Network/Deep Neural Network\n MLP(Mutilayer Perceptron)\n Autoencoder\n CNN\n RNN\n   贝叶斯派  BayesianNetwork $\\Rightarrow$ Deep Directed Network  Sigmoid Belief Network Variational Autoencoder(VAE) GAN  Markov Network $\\Rightarrow$ Deep Boltzmann Network Mixed Netowork $\\Rightarrow$ Deep Belief Network   上述都是：Deep Generative Model\n 狭义的DeepLearning：Deep Neural Network\n其实应该包括：\n Deep Neural Network Deep Generative Model（当层次非常多，推断非常困难）  时间线  深度学习的理论在2006年已经成型，直到现在，理论并没有根本性突破。 为什么take off  data 分布式 硬件 GPU 效果   Non-Linear Problem Neural Network 符合运算-》复合表达式-〉复合函数\n人工智能 两大阵营 三大主义 ","date":1585099595,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586649069,"objectID":"80470b31effc764701791a977ec585e6","permalink":"https://faithio.cn/post/deep-learning/","publishdate":"2020-03-24T21:26:35-04:00","relpermalink":"/post/deep-learning/","section":"post","summary":"Neural Networks Basics Logistic Regression as a Neural Network $$ \\hat{y}=\\sigma\\left(w^{T} x+b\\right), \\text { where } \\sigma(z)=\\frac{1}{1+e^{-z}} \\ \\text { Given }\\left{\\left(x^{(1)}, y^{(1)}\\right), \\ldots,\\left(x^{(m)}, y^{(m)}\\right)\\right}, \\text { want } \\hat{y}^{(i)} \\approx y^{(i)} $$ Loss(error) function: Don\u0026rsquo;t use this, non-convex $$ \\ell(\\hat{y}, y)=\\frac{1}{2}(\\hat{y}-y)^{2} $$ The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set. $$","tags":[],"title":"Deep Learning","type":"post"},{"authors":[],"categories":[],"content":" [pdf]\nclick here for link\n概率图模型  inference-\u0026gt; $P(Z|X)$-\u0026gt;积分问题（MCMC）\n GMM：样本之间是独立同分布\nHMM: Dynamic Model y: System state 隐变量\n state 离散：HMM state 线性: Kalman Filter state 非线性: Particle Filter  $$ \\lambda = (\\underbrace{\\pi}{初始prob dist}, \\underbrace{A}{状态转移矩阵}, \\underbrace{B}_{发射矩阵emission}) 状态变量i: i_1, i_2, \\cdots i_t \\cdots \\rightarrow Q={q_1, q_2, \\cdots, q_m} 观测变量o: o_1, o_2, \\cdots o_t \\cdots \\rightarrow V={v_1, v_2, \\cdots, vm} A=\\left[a{i j}\\right], a{i j}=P\\left(i{t+1}=q_{i} | it=q{i}\\right) B=\\left[b{j k} \\right], b{j k}=P\\left(Q{t}=v{k} | it=q{j}\\right) $$\n transition 和 emission probability 是independent\n 两个假设：\n 齐次Markov 假设\n $$ P(i_{t+1}|it, t{t-1}, \\cdots, t_1, ot, o{t-1}, \\dots, o1) = p(i{t+1}|i_t) $$\n 观察独立假设 $$ P(o_{t}|it, t{t-1}, \\cdots, t_1, ot, o{t-1}, \\dots, o1) = p(o{t}|i_t) $$\n  三个问题：\n Evaluation: $P(O| \\lambda) \\Rightarrow$ 前向后向 Forward-backward learning $\\lambda=\\arg \\max P(O|\\lambda)$ EM algorithm\\baum welch Decoding $\\lambda=\\arg \\max_{i} P(I|O)$  预测：$P(i_{t+1}|o_1, o_2, \\cdots, o_t)$ 滤波：$P(i_{t}|o_1, o_2, \\cdots, o_t)$   HMM-Evaluation $$ \\begin{aligned} Give \u0026amp;\\lambda, 求 P(O|\\lambda) P(O | \\lambda) \u0026amp;=\\sum{1} P(I, O | \\lambda)=\\sum{1} P(O | I, \\lambda) \\cdot P(I | \\lambda) \\\nP(I | \\lambda)\u0026amp;=P\\left(i{1}, i{2}, \\cdots, i{T} | \\lambda\\right)=\\underbrace{P\\left(i{T} | i, i{2},\\cdots,i{T-1}, \\lambda\\right)}_{P(iT|i{T-1})=a{i{T-1}, i_T}} P(i_1, i2, \\cdots, i{T-1}|\\lambda) = a{i{T-1}, iT} \\cdot a{i{T-2}, i{T-1}} \\cdots a{i{1}, i_{2}} \\cdot \\pi(i_1) \\\n\u0026amp;\\pi\\text{ 是初始分布} \u0026amp;= \\pi\\left(a{i}\\right) \\cdot \\prod{t=2}^{T} a{i{t-1}, it} \u0026mdash;-\np(O | I, \\lambda) \u0026amp; =\\prod{t=1}^{T} b_{it}\\left(O{t}\\right) \\end{aligned} $$\nPPT ","date":1585082750,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586649069,"objectID":"a26198dec54033211ebab263a5e00927","permalink":"https://faithio.cn/post/hmm/","publishdate":"2020-03-24T16:45:50-04:00","relpermalink":"/post/hmm/","section":"post","summary":"[pdf] click here for link 概率图模型 inference-\u0026gt; $P(Z|X)$-\u0026gt;积分问题（MCMC） GMM：样本之间是独立同分布 HMM: Dynamic Model y: System state 隐变量 state 离散：HMM state 线性: Kalman Filter","tags":[],"title":"HMM","type":"post"},{"authors":[],"categories":[],"content":" 语音识别 HMM Method 1:Tandem Method 2: DNN-HMM Hybrid ","date":1585068369,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585155865,"objectID":"4f4a6c327fba1cf1c0a126544a3a4547","permalink":"https://faithio.cn/post/nlp/","publishdate":"2020-03-24T12:46:09-04:00","relpermalink":"/post/nlp/","section":"post","summary":"语音识别 HMM Method 1:Tandem Method 2: DNN-HMM Hybrid","tags":[],"title":"NLP","type":"post"},{"authors":[],"categories":[],"content":" 概率分布采样 Standard distributions PDF如果很复杂，CDF求不出来。\nRejection Sampling Importance Sampling 因为大部分采样得到的样本重要性很低，反之仅有少量的样本重要性非常大。\nSampling Importance Resampling\nMonte Carlo Method 基于随机采样的近似方法\nMarkov Chain: 时间状态都是离散的。特殊的随机过程\n齐次（一阶）Markov Chain：未来只依赖于当前，和过去没有关系 ${xt}$ P为转移矩阵$[P{ij}]$ $$ P(X_{t+1} = x| x_1, x_2, \\cdots, xt) = P(X{t+1}|x_t) $$\n$$ P{i j} \\quad P{i j}=P\\left(X{i+1}=j | X{t=i}\\right) $$\ngraph LR; x1--\u0026gt;x2; x2--\u0026gt;x3; x3--\u0026gt;xt; xt--\u0026gt;xt+1;  平稳分布\n","date":1585028300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585155865,"objectID":"2e809b9eacbd8c09c0c32ead5e993a3d","permalink":"https://faithio.cn/post/markov-chain-monte-carlo/","publishdate":"2020-03-24T01:38:20-04:00","relpermalink":"/post/markov-chain-monte-carlo/","section":"post","summary":"概率分布采样 Standard distributions PDF如果很复杂，CDF求不出来。 Rejection Sampling Importance Sampling 因为大部分采样得到的样本重要性很低，反之仅有少量的样本重要性非常大。 Sampling Importance Resampling Monte Carlo Method 基","tags":[],"title":"Markov Chain Monte Carlo","type":"post"},{"authors":[],"categories":[],"content":" 混合模型都是生成模型，N个样本，先生成x1，x2\n$$ \\begin{aligned} p(x) \u0026amp;= \\sumz P(x,z) \u0026amp;= \\sum{k=1}^z P(x, z=Ck) \u0026amp;= \\sum{k=1}^z p(z=C_k) \\cdot p(x|z=Ck) \u0026amp;= \\sum{k=1}^z p_k \\mathcal{N}(x| \\mu_k, \\Sigma_k) \\end{aligned} $$ $$ \\log (\\Delta + \\Delta + \\Delta ) \\rightarrow \\text{连加符号很难求导，令导数为0，连乘比较方便} \\text{单变量的高斯分布可以直接用MLE求出来} $$\n用EM求解 $$ \\text{ E-Step: } P\\left(z | x, \\theta^{(t)}\\right) \\rightarrow E_{z|x, \\theta^{(t)}}[\\log P(x, z | \\theta)] \\\n\\text{ M-Step: } \\theta^{(t+1)}=\\arg \\max {\\theta} \\underbrace{ E{z|x, \\theta^{(t)}}[\\log P(x, z | \\theta)]}_{Q(\\theta, \\theta^{(t)})} \\\n\\text{E-step:} \\\nQ(\\theta, \\theta^{(t)}) = \\int_z \\log P(X, Z|\\theta) P(Z|X, \\theta^{(t)}) dz \\text{独立同分布} = \\sumz \\log \\prod{i=1}^N P(x_i, z_i|\\theta) \\cdot \\sumz \\log \\prod{i=1}^N P(z_i|xi, \\theta^{(t)})\n=\\sum{z_{1} z2 \\cdots z{N}} \\sum{i=1}^{N} \\log P\\left(x{i} z{i} | \\theta\\right) \\cdot \\prod{i=1}^{N} p\\left(z{i} | x{i}, \\theta^{(t)}\\right) \\\n=\\sum{z{1} z2 \\cdots z{N}} \\left[\\log P\\left(x{1}, z{1} | \\theta\\right)+\\log P\\left(x{2}, z{2} | \\theta\\right)+\\cdots+\\log P\\left(x{N}, z{N} | \\theta\\right)\\right] \\cdot \\prod{i=1}^{N} p\\left(z{i} | x{i}, \\theta^{(t)}\\right) = \\sum{z{1}} \\log p\\left(x{1}, z{1} | \\theta\\right) \\cdot p\\left(z{1} | x{1}, \\theta\\right) + \\cdots + \\sum{z{N}} \\log p\\left(x{N}, z{N} | \\theta\\right) \\cdot p\\left(z{N} | x{N}, \\theta\\right) =\\sum{i=1}^{N} \\sum_{zi} \\log p\\left(x{i}, z{i} | \\theta\\right) \\cdot p\\left(z{i} | x_{i}, \\theta^{(i)}\\right) \\\n=\\sum{i=1}^{N} \\sum{z{i}} \\log p{z{i}} \\mathcal{N}\\left(x{i} | \\mu{z{i}} \\Sigma{z{i}}\\right) \\cdot \\frac{p{z{i}} \\cdot \\mathcal{N}\\left(x{i} | \\mu{i}, \\Sigma{i}\\right)}{\\sum{k=1}^{K} p{x} \\mathcal{N}\\left(x{i} | \\mu_{k}, \\Sigmak\\right)} \u0026mdash; \\begin{aligned} P(x, z) \u0026amp;=P(z) \\cdot p(x | z) \\ \u0026amp;=p{z} \\cdot N\\left(x | \\mu{z}, z{z}\\right) \\ p(z | x) \u0026amp;=\\frac{p(x, z)}{p(x)}=\\frac{p{z} \\cdot \\mathcal{N}\\left(x | \\mu{z} \\Sigma{z}\\right)}{\\sum{k=1}^{K} p{k} \\cdot \\mathcal{N}\\left(x | \\mu{k}, \\Sigma_{k}\\right)} \\end{aligned} $$\n$$ \\text{取第一项} \\sum{z{1} z2 \\cdots z{N}} \\log p\\left(x{1}, z{1} |\\theta\\right) \\cdot \\underbrace{\\prod{i=1}^{N} p\\left(z{i} | x{i}, \\theta^{(t)}\\right)}{ p\\left(z_1,\\left|x1, \\theta^{(t)}\\right) \\cdot \\prod{i=2}^{N} p\\left(z{i} | x{i}, \\theta^{(t)}\\right)\\right. } \\\n=\\sum{z{1}} \\log p\\left(x{1}, z{1} | \\theta\\right) \\cdot p\\left(z{1} | x{1}, \\theta\\right) \\sum_{z_2 \\cdots zN} \\prod{i=2}^{N} p\\left(z{i} | x{i}, \\theta^{(t)}\\right) = \\sum{z{1}} \\log p\\left(x{1}, z{1} | \\theta\\right) \\cdot p\\left(z{1} | x{1}, \\theta\\right) \u0026mdash;- \\prod{i=2}^{N} p\\left(z{i} | x{i}, \\theta^{(t)}\\right) = \\prod{i=2}^{N} P(z_2|x_2)P(z_3|x_3)\\cdots P(z_N|xN) = \\sum{z_2}P(z_2|x2)\\cdot \\sum{z_3}P(z_3|x3)\\cdot \\cdots \\sum{z_N}P(z_N|x_N) = 1 $$\nE-Step: $$ \\text{ M-Step: } \\theta^{(t+1)}=\\arg \\max {\\theta} \\underbrace{ E{z|x, \\theta^{(t)}}[\\log P(x, z | \\theta)]}_{Q(\\theta, \\theta^{(t)})} \\begin{aligned} Q(\\theta, \\theta^{(t)}) \u0026amp;= \\intz \\log P(X, Z|\\theta) P(Z|X, \\theta^{(t)}) dz \u0026amp;=\\sum{i=1}^{N} \\sum{z{i}} \\log p{z{i}} \\mathcal{N}\\left(x{i} | \\mu{z{i}} \\Sigma{z{i}}\\right) \\cdot \\underbrace{ \\frac{p{z{i}} \\cdot \\mathcal{N}\\left(x{i} | \\mu{i}, \\Sigma{i}\\right)}{\\sum{k=1}^{K} p{x} \\mathcal{N}\\left(x{i} | \\mu{k}, \\Sigmak\\right)} }{P(z_i|x_i, \\theta^{(t)}) \\rightarrow \\theta^{(t)} \\text{ is a constant.}} \\\n\u0026amp;= \\intz \\log P(X, Z|\\theta) P(Z|X, \\theta^{(t)}) dz \u0026amp;=\\sum{i=1}^{N} \\sum{z{i}} \\log [p{z{i}} \\mathcal{N}\\left(x{i} | \\mu{z{i}} \\Sigma{z_{i}}\\right)] \\cdot P(z_i|xi, \\theta^{(t)}) \u0026amp;= \\sum{z{i}} \\sum{i=1}^{N} \\log [p{z{i}} \\mathcal{N}\\left(x{i} | \\mu{z{i}} \\Sigma{z_{i}}\\right)] \\cdot P(z_i|xi, \\theta^{(t)}) \u0026amp;= \\sum{k=1}^{k} \\sum{i=1}^{N} \\log [p{k} \\cdot \\mathcal{N}\\left(x{i} | \\mu{k}, \\Sigma{k}\\right)] \\cdot p\\left(z{i}=C{k} | x{i}, \\theta^{(t)}\\right) \\\n\u0026amp;= \\sum{k=1}^{k} \\sum{i=1}^{N} [\\log p{k} + \\log \\mathcal{N}\\left(x{i} | \\mu{k}, \\Sigma{k}\\right)] \\cdot p\\left(z{i}=C{k} | x_{i}, \\theta^{(t)}\\right) \\end{aligned} $$ 求 $p_k^{t+1} = (p_1^{t+1}, p_2^{t+1}, \\cdots, pk^{t+1})$ $$ \\left{ \\begin{array}{l} p{k}^{(k+1)}=\\operatorname{argmax}{p{k}} \\sum{k=1}^{k} \\sum{i=1}^{N} \\log p{k} \\cdot p\\left(z{i} = Ck | x{i}, \\theta^{(t)}\\right) s.t. \\sum{k=1}^{k} p{k}=1 \\end{array} \\right. $$ 用拉格朗日乘子法： $$ \\mathcal{L}(p, \\lambda)=\\sum{k=1}^{K} \\sum{i=1}^{N} \\log p{k} \\cdot p\\left(z{i}=C{k} | x{i}, \\theta^{(t)}\\right)+\\lambda(\\sum_{k=1}^k - 1) $$\n$$ \\frac{\\partial \\mathcal{L}}{\\partial p{k}}= \\sum{i=1}^{N} \\frac{1}{p{k}} \\cdot p\\left(z{i}=Ck | x{i} \\cdot \\theta^{(k)}\\right)+\\lambda \\triangleq 0 \\rightarrow \\sum{i=1}^{N} \\cdot p\\left(z{i}=Ck | x{i} \\cdot \\theta^{(k)}\\right)+p_k \\lambda = 0 \\\n\\Rightarrow^{(k=1,\\cdots , K)} \\sum{i=1}^{N} \\underbrace{\\sum{i=1}^K \\cdot p\\left(z_{i}=Ck | x{i} \\cdot \\theta^{(k)}\\right)}1 + \\underbrace{\\sum{i=1}^K p_k}_1 \\lambda \\Rightarrow N+ \\lambda = 0 \\Rightarrow \\lambda = -N $$\n$$ p{k}^{(t+1)}=\\frac{1}{N} \\sum{i=1}^{N} p\\left(z{i}=C{k} | x_{i}, \\theta^{(t)}\\right) $$\n","date":1585021364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585028218,"objectID":"1ae0708a82dfe0be1cc5160b25eca272","permalink":"https://faithio.cn/post/gaussian-mixture-model/","publishdate":"2020-03-23T23:42:44-04:00","relpermalink":"/post/gaussian-mixture-model/","section":"post","summary":"混合模型都是生成模型，N个样本，先生成x1，x2 $$ \\begin{aligned} p(x) \u0026amp;= \\sumz P(x,z) \u0026amp;= \\sum{k=1}^z P(x, z=Ck) \u0026amp;= \\sum{k=1}^z p(z=C_k) \\cdot p(x|z=Ck) \u0026amp;= \\sum{k=1}^z p_k \\mathcal{N}(x| \\mu_k, \\Sigma_k) \\end{aligned} $$ $$ \\log (\\Delta + \\Delta + \\Delta ) \\rightarrow \\text{连加符号很难求导","tags":[],"title":"Gaussian Mixture Model","type":"post"},{"authors":[],"categories":[],"content":" click here for link\n$MLE: P(X|\\theta)$\n$$ \\theta_{MLE}=\\arg \\max _{\\theta} \\log P(X | \\theta) \\\n\\theta^{(t+1)}=\\arg \\max {\\theta} \\int{z} \\log p(x, z | \\theta) \\cdot p\\left(z | x, \\theta^{(t)}\\right) d z = \\arg \\max {\\theta} E{\\mathbf{Z} | x \\theta^{t}}[\\log P(x, z | \\theta)] \\\n\\text{ E-Step: } P\\left(z | x, \\theta^{(t)}\\right) \\rightarrow E_{z|x, \\theta^{(t)}}[\\log P(x, z | \\theta)] \\\n\\text{ M-Step: } \\theta^{(t+1)}=\\arg \\max {\\theta} E{z|x, \\theta^{(t)}}[\\log P(x, z | \\theta)] $$\n需要证明一定会收敛Convergence $$ \\theta^{(t)} \\rightarrow \\theta^{(t+1)} \\log p\\left(x | \\theta^{(t)}\\right) \\leqslant \\log p\\left(x | \\theta^{(t+1)}\\right) $$\n$$ \\frac{P(x, z | \\theta)}{P(z | x)} \\\n\\log P(x | \\theta)=\\log P(x, z | \\theta)-\\log P(z | x, \\theta) \\\n\\begin{aligned} left \u0026amp;=\\int{z} p\\left(z | x, \\theta^{(t)}\\right) \\cdot \\log p(x | \\theta) d z \\ \u0026amp;=\\log P(x | \\theta) \\underbrace{ \\int{z} P\\left(z | x, \\theta^{(t)}\\right) d z }_1 \\ \u0026amp;= \\log P(x | \\theta) \\end{aligned} \\\nright =\\underbrace{\\int{\\mathbb{Z}} p\\left(z | x, \\theta^{(t)}\\right) \\cdot \\log P(x, z | \\theta) d z}{Q\\left(\\theta, \\theta^{(t)}\\right)} -\\underbrace{\\int{\\mathbb{Z}} p\\left(z | x, \\theta^{(t)}\\right) \\cdot \\log p(z | x, \\theta) d z}{H\\left(\\theta, \\theta^{(t)}\\right)}\\\nQ\\left(\\theta^{(t+1)}, \\theta^{(t)}\\right) \\geqslant Q\\left(\\theta^{(t)}, \\theta^{(t)}\\right) \\\n\\begin{aligned} \u0026amp; H\\left(\\theta^{(t+1)} \\cdot \\theta^{(t)}\\right)-H\\left(\\theta^{(t)} \\cdot \\theta^{(t)}\\right) \\ \u0026amp;=\\int{z} p\\left(z | x, \\theta^{(t)}\\right) \\cdot \\log p\\left(z | x \\theta^{(t+1)}\\right) d z \\ \u0026amp;-\\int{z} p\\left(z | x, \\theta^{(t)}\\right) \\cdot \\log p\\left(z | x, \\theta^{(t)}\\right) d z \\\u0026amp; = \\int_{z} P\\left(z | x, \\theta^{(t)}\\right) \\cdot \\log \\frac{p\\left(z | x, \\theta^{(t+1)}\\right)}{p\\left(z | x, \\theta^{(t)}\\right)} d z \\ \u0026amp; = -\\mathcal{KL}\\left(P(z | x, \\theta^{(t)}) | p\\left(z | x, \\theta^{(t+1)}\\right) \\right) \\ \u0026amp; \\leqslant 0\n\\end{aligned} \\\n\\begin{aligned} \u0026amp; E[\\ln x] \\leqslant \\log E[x] \\ \\leqslant \\log \\underbrace{\\int_{z} p\\left(z|x, \\theta^{(t+ 1)}\\right) d z}_1=\\log 1=0\n\\end{aligned} $$\n$$ p(x, z)=p(z | x) \\cdot p(x) \\log p(x) = \\log \\frac{p(x, z)}{p(z | x) } = \\log p(x, z) - \\log p(z | x)\nadd \\quad \\theta \u0026mdash;\n\\log P(x | \\theta)=\\log P(x, z | \\theta)-\\log P(z | x, \\theta) =\\log \\frac{P(x, z | \\theta)}{q(z)}-\\log \\frac{P(z | x, \\theta)}{q(z)} \\quad q(z) \\neq 0 \\text{左右两边对于} q(z) \\text{求期望} left = \\int{z} q(z) \\cdot \\log p(x | \\theta) d z=\\log p(x | \\theta) \\cdot \\underbrace{\\int{z} q(z) d z}{1}=\\log p(x | \\theta) rigth = \\underbrace{ \\int{z} q(z) \\log \\frac{p(x, z | \\theta)}{q(z)} d z}{ELBO: evidence lower bound} \\underbrace{ - \\int{z} q(z) \\log \\frac{p(z | x, \\theta)}{q(z)} d z}_{\\mathcal{KL}(q(z) | p(z | x, \\theta)} \\log p(x | \\theta)=ELBO+ KL(q|p) \\log p(x | \\theta) \\geqslant ELBO \\text{maximize ELBO, then posterior maximize, 最大化ELBO(期望)， 然后更新后验概率的参数} \\theta $$\n$$ \\hat{\\theta}=\\arg \\max _{\\theta} E L B O = \\arg \\max _{\\theta} \\int q(z) \\log \\frac{p (x, z|\\theta)}{q(z)} d z\n当\\log p(x | \\theta) \\geqslant ELBO 取等于号(KL=0)，q(z) = p(z|x, \\theta^{(t)}) =\\arg \\max _{\\theta} \\int p\\left(z | x, \\theta^{(t)}\\right) \\log \\frac{p(x, z | \\theta)}{p\\left(z | x,\\theta^{(t)}\\right)} d z =\\arg \\max {\\theta} \\int p\\left(z | x, \\theta^{(t)}\\right) \\left[ \\log p(x, z | \\theta) - \\underbrace{ \\log p\\left(z | x,\\theta^{(t)}\\right)}{与\\theta无关，已经变成常数} \\right]dz = \\arg \\max {\\theta} \\int{z} p\\left(z | x, \\theta^{(t)}\\right) \\cdot \\log p(x, z | \\theta) d z\n$$\n新的角度推导EM $$ \\begin{aligned} \\log P(x|\\theta) \u0026amp; = \\log \\intz P(x, z| \\theta) \u0026amp; = \\log \\int{z} \\frac{P(x, z | \\theta)}{q(z)} \\cdot q(z) d z \u0026amp; = \\log E{q(z)}\\left[\\frac{p(x, z | \\theta)}{q(z)}\\right] \u0026amp; \\text{use jensen inequalty} \u0026amp; \\geqslant E{q(z)}\\left[\\log \\frac{p(x, z | \\theta)}{q(z)}\\right] \\rightarrow ELBO \u0026amp; \\Leftrightarrow \\frac{P(x, z | \\theta)}{q(z)}=C q(z) \u0026amp; =\\frac{1}{c} p(x, z | \\theta) 1 =\\intz q(z) d z \u0026amp;=\\int{z} \\frac{1}{c} p(x, z | \\theta) d z \u0026amp; =\\frac{1}{c} \\int_{x} p(x, z | \\theta) d z 1 \u0026amp; =\\frac{1}{c} P(x | \\theta) c \u0026amp; =P(x | \\theta) \\end{aligned} $$\n$$ q(z)=\\frac{1}{p(x | \\theta)} \\cdot p(x, z | \\theta)=p(z | x, \\theta) $$\n$$ t \\in[0.1] c= ta+ (1-t)b f\u0026copy; =f(ta+ (1-t)b) \\geqslant tf(a) + (1-t)f(b) $$\n$$ \\begin{array}{l}x(t)=x{1}+t\\left(x{2}-x{1}\\right) \\ y(t)=f\\left(x{1}\\right)+t\\left(f\\left(x{2}\\right)-f\\left(x{1}\\right)\\right) \\ t \\in \\mathbb{R}\\end{array} \\begin{array}{l}\\text { Combining like terms and replacing } t \\text { with }(1-t) \\text { (which is fine since } t \\text { is an arbitrary parameter; } \\ \\text { furthermore } 0 \\leq 1-t \\leq 1 \\Longleftrightarrow 0 \\leq t \\leq 1 \\text { ), we can write the secant line as the set of } \\ \\text { points } \\ \\qquad \\begin{array}{l}x(t)=t x{1}+(1-t) x{2} \\ y(t)=t f\\left(x{1}\\right)+(1-t) f\\left(x{2}\\right) \\ t \\in \\mathbb{R}\\end{array}\\end{array}\n$$ http://www.gtmath.com/2016/03/convexity-and-jensens-inequality.html\n广义EM 概率生成模型问题，通过z生成x，然后再通过积分把z消掉。参数learning问题\ngraph LR; z--\u0026gt;x;  $$ \\hat{\\theta} = \\arg \\max _{\\theta} \\log P(X|\\theta)\n= \\arg \\max {\\theta} \\log \\prod{i=1}^N P(x_i|\\theta) $$\n$$ \\log P(x | \\theta)=E L B O+K L(q | p) \\left{\\begin{array}{l}E L B O=E_{q(z)}\\left[\\log \\frac{p(x, z | \\theta)}{q(z)}\\right] \\ KL(q | p)=\\int q(z) \\cdot \\log \\frac{q(z)}{p(z | x, \\theta)} d z\\end{array}\\right. \\\n\\text{E-step: 固定 }\\theta \\rightarrow \\hat{q}=\\arg \\min _{q} K L(q | p)=\\arg \\max_q \\mathcal{L}(q, \\theta) \\text{M-step: 固定 }\\hat{q} \\rightarrow \\theta=\\arg \\max _{\\theta} \\mathcal{L}(\\hat{q}, \\theta) $$\n$$ \\left{\\begin{array}{l} \\text { E-stes: } q^{(t+1)}=\\arg \\max _{q} \\mathcal{L}\\left(q, \\theta^{(t)}\\right) \\ \\text { M-step: } \\theta^{(t+1)}=\\arg \\max _{\\theta} \\mathcal{L}(q^{t+1}, \\theta) \\end{array}\\right. $$\n所以EM也叫MM算法，轮流迭代q and $\\theta$\nSMO：坐标上升法/梯度上升法\nM步和E步其实可以交换\nE-Step 其实是求后验概率，假如intractable 然后用VI求出后验 就叫VBEM/VEM，如果用MC求后验那就是MCEM\nVI \u0026lt;=\u0026gt; VB\n既然是优化问题，能否用梯度的方法求解呢？\n","date":1584676534,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586648375,"objectID":"9ea6f7a01fddd5d96157297ddf58bdd4","permalink":"https://faithio.cn/post/expectation-maximization/","publishdate":"2020-03-19T23:55:34-04:00","relpermalink":"/post/expectation-maximization/","section":"post","summary":"click here for link $MLE: P(X|\\theta)$ $$ \\theta_{MLE}=\\arg \\max _{\\theta} \\log P(X | \\theta) \\ \\theta^{(t+1)}=\\arg \\max {\\theta} \\int{z} \\log p(x, z | \\theta) \\cdot p\\left(z | x, \\theta^{(t)}\\right) d z = \\arg \\max {\\theta} E{\\mathbf{Z} | x \\theta^{t}}[\\log P(x, z | \\theta)] \\ \\text{ E-Step: } P\\left(z | x, \\theta^{(t)}\\right) \\rightarrow E_{z|x, \\theta^{(t)}}[\\log P(x, z | \\theta)] \\ \\text{ M-Step: } \\theta^{(t+1)}=\\arg \\max {\\theta} E{z|x, \\theta^{(t)}}[\\log P(x, z |","tags":[],"title":"Expectation Maximization","type":"post"},{"authors":[],"categories":[],"content":" click here for link\nCommon X: data\n$X=\\left(x_1, x_2 \\cdots xN\\right){N \\times p}^{T}$ $$ \\theta = \\left(\\begin{array}{cccc}x{0} \u0026amp; x{a} \u0026amp; \\ldots \u0026amp; x{1 x} \\ x{1} \u0026amp; x{12} \u0026amp; \\ldots \u0026amp; x{21} \\ \\vdots \u0026amp; \u0026amp; \u0026amp; x{n y} \\ x{m} \u0026amp; x_{n x} \u0026amp; \\ldots \u0026amp; \\end{array}\\right) $$\n频率（统计机器学习） 优化问题 $$ x \\sim p(x | \\theta) $$ $\\theta$ 未知常量， X：r v $$ MLE：\\theta_{M L E}=\\arg \\max _\\theta \\log P(X | \\theta) $$\n贝叶斯(概率图模型) 求积分（MCMC）\n$\\theta$ r v， $$ \\theta \\sim p(\\theta) $$\n$$ P(\\theta | x)=\\frac{P(x | \\theta) \\cdot P(\\theta)}{P(x)} \\propto P(x | \\theta) \\cdot P(\\theta) P(x) = \\int_{\\theta} P(x | \\theta) \\cdot P(\\theta) d \\theta $$\n$$ MAP: \\theta_{MAP}=\\arg \\max _{\\theta} P(\\theta | x)=\\arg \\max _{\\theta} P(x | \\theta) \\cdot P(\\theta) $$\nBias and Variance Avoid overfiting book  李航 统计学习方法（频率派）感K朴决逻， 支提E隐条 周志华 “西瓜书” PRML 回分神核稀 图混近采连 顺组 - 贝叶斯 MLAPP 贝叶斯 ESL 频率派 Deep Learning 圣经/张志华翻译  高斯分布 马氏距离/欧氏距离\n$X^TAX$ : 欧氏距离，中间是单位矩阵，马氏距离，中间是协方差矩阵（其实是二次型）\n","date":1584659753,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586648375,"objectID":"bc2fa12456182893105148befb3b4799","permalink":"https://faithio.cn/post/foundation-of-machine-learning/","publishdate":"2020-03-19T19:15:53-04:00","relpermalink":"/post/foundation-of-machine-learning/","section":"post","summary":"click here for link Common X: data $X=\\left(x_1, x_2 \\cdots xN\\right){N \\times p}^{T}$ $$ \\theta = \\left(\\begin{array}{cccc}x{0} \u0026amp; x{a} \u0026amp; \\ldots \u0026amp; x{1 x} \\ x{1} \u0026amp; x{12} \u0026amp; \\ldots \u0026amp; x{21} \\ \\vdots \u0026amp; \u0026amp; \u0026amp; x{n y} \\ x{m} \u0026amp; x_{n x} \u0026amp; \\ldots \u0026amp; \\end{array}\\right) $$ 频率（统计机器学习） 优化问题 $$ x \\sim p(x | \\theta) $$","tags":[],"title":"Foundation of Machine Learning","type":"post"},{"authors":[],"categories":[],"content":"Approximate Inference.pdf\nApproximate Inference.html\n","date":1584656517,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586648375,"objectID":"42e77f450ce9f46625dae22d128fa311","permalink":"https://faithio.cn/post/approximate-inference/","publishdate":"2020-03-19T18:21:57-04:00","relpermalink":"/post/approximate-inference/","section":"post","summary":"Approximate Inference.pdf\nApproximate Inference.html","tags":[],"title":"Approximate Inference","type":"post"},{"authors":[],"categories":[],"content":"Exponential.pdf\nExponential.html\n","date":1584656503,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584657777,"objectID":"b00c82fc12ffaf3b2f6fd5dba8d770b8","permalink":"https://faithio.cn/post/the-exponential-family/","publishdate":"2020-03-19T18:21:43-04:00","relpermalink":"/post/the-exponential-family/","section":"post","summary":"Exponential.pdf\nExponential.html","tags":[],"title":"The Exponential Family","type":"post"},{"authors":[],"categories":[],"content":" pdf\ngraph TD; 概率图--\u0026gt;Representation-表示; 概率图--\u0026gt;Inference-推断; 概率图--\u0026gt;Learning-学习; Representation-表示--\u0026gt;有向图BayesianNetwork; Representation-表示--\u0026gt;高斯图-连续; Representation-表示--\u0026gt;无向图MarkovNetwork; 高斯图-连续--\u0026gt;GaussianBN; 高斯图-连续--\u0026gt;GaussianMN; Inference-推断--\u0026gt;精确推断; Inference-推断--\u0026gt;ApproximateInference; ApproximateInference--\u0026gt;DeterministicApproximation(Variantional Inference); ApproximateInference--\u0026gt;StochasticcApproximation(MCMC); Learning-学习--\u0026gt;参数学习; Learning-学习--\u0026gt;结构学习; 参数学习--\u0026gt;完备数据; 参数学习--\u0026gt;隐变量; 隐变量--\u0026gt;EM;  $$ \\begin{array}{l} \\text{Sum Rule: } P(x_1)=\\int P\\left(x_1, x2\\right) d x{2} \\ \\text{Product Rule: } P\\left(x1, x{2}\\right)=P\\left(x_{1}\\right) \\cdot P\\left(x_2 | x1\\right)=P\\left(x{2}\\right) \\cdot P\\left(x{1} | x{2}\\right)\n\\end{array} $$\nChain Rule:\nBayesian Network  拓扑排序构建图\n graph TD; a--\u0026gt;b; a--\u0026gt;c;   $$ Chain Rule: P\\left(x{1}, x{2}, \\cdots, x{p}\\right)=P\\left(x{1}\\right) \\cdot \\prod{i=2}^{p} p\\left(x{i} | x_{1:i-1}\\right) P(a, b, c) = P(a)P(b|a)(c|a) \\rightarrow 因子分解\nP(a, b, c) = P(a)P(b|a)(c|a,b) \\rightarrow Chain rule\nP(c | a)=P(c | a, b) \\Rightarrow c \\perp b | a p(c | a) \\cdot p(b | a)=p(c|a,b) \\cdot p(b | a)=p( b, c | a) p(c | a) \\cdot p(b | a)=p(b, c | a) $$\nTail to tail, 若a被观测，则路径被堵塞$tail \\rightarrow head$\ngraph LR; a--\u0026gt;b; b--\u0026gt;c;  head to tail $$ P(a,b, c) = P(a)P(b|a)P(c|b) P(a, b, c) = P(a) P(b|a) P(c|a,b) P(c|b) = P(c|a,b) $$\n$$ a \\perp c | b $$ 若b被观测，则路径被阻塞（independent）\ngraph TD; a--\u0026gt;c; b--\u0026gt;c;  head to head\n默认情况下，$a \\perp b$，路径阻塞的\n若c被观测，则路径是通的 $$ P(a, b, c)=P(a) \\cdot P(b) \\cdot P(c | a, b) P(a, b, c) = P(a)\\cdot P(b|a) \\cdot (c|a,b) P(b) = P(b|a) $$\nInference $$ \\begin{aligned} \\text { sum rule } \u0026amp; p(X)=\\sum_{Y} p(X, Y) \\ \\text { product rule } \u0026amp; p(X, Y)=p(Y | X) p(X) \\end{aligned} $$ 求概率： $ P(x)=P\\left(x_0, x_1, \\cdots, x_p\\right) $\n边缘概率marginal probability： $$ P\\left(x{i}\\right)=\\sum{x1} \\cdot \\sum{x{i-1}} \\sum{x{i+1}} \\ldots \\sum{x_{p}} p(x) $$ 条件概率conditional probability： $$ P\\left(x_A | x_B\\right) \\quad x=x_A \\cup x_B $$ MAP Inference: $$ \\hat{z}=\\arg \\max _{z} P(z | x) \\propto \\arg \\max P(z, x) $$\ngraph LR; Inference--\u0026gt;精确推论; Inference--\u0026gt;近似推断; 精确推论--\u0026gt;variableElimination/VE; 精确推论--\u0026gt;BeliefPropagation/SumProductAlgorithm树结构; 精确推论--\u0026gt;JunctionTreeAlgorithm普通图结构BasedBP; 近似推断--\u0026gt;LoopBeliefPropagation有环图BasedBP; 近似推断--\u0026gt;MenteCarloInference:ImportanceSampling,MCMC; 近似推断--\u0026gt;VariationalInference  Variable Elimination-乘法分配律 $$ M A P \\quad \\tilde{X}_{A}=\\arg \\max {X} P\\left(x{A} | x{B}\\right)=\\arg \\max P\\left(x{A}, x_{B}\\right) $$\ngraph LR; a--\u0026gt;b; b--\u0026gt;c; c--\u0026gt;d;  假设a,b,c,d均是离散的二值r,v {0,1}\n$$ p(d) = \\sum_{a, b, c} p(a, b, c, d) \\\n= \\sum_{a, b, c} p(a) \\cdot p(b | a) \\cdot p(c | b) \\cdot p(d | c) \\\n= p(a=0) \\cdot p(b=0 | a=0) \\cdot p(c=0 | b=0) \\cdot p(d=0|c=0) + p(a=1) \\cdot p(b=0 | a=1) \\cdot p(c=0 | b=0) \\cdot p(d=0|c=0) \\\n \\cdots  p(a=1) \\cdot p(b=1 | a=1) \\cdot p(c=1 | b=1) \\cdot p(d=1|c=1) = \\sum{b, c} p(c | b) \\cdot p(d | c) \\cdot \\underbrace{\\sum{a} p(a) \\cdot p(b | a)}_{\\phi_a(b)}   = \\sumc p(d | c) \\cdot \\underbrace{\\sum{b} p(c | b)\\cdot \\phia(b) }{\\phi_b\u0026copy;} = \\phi_c(d) $$ 乘法对加法的分配律$ab+cb = b(a+c)$\nCons:\n Memoryless.重复计算 Ordering NP-hard  Belief Propagation graph LR; a--\u0026gt;b; b--\u0026gt;c; c--\u0026gt;d; d--\u0026gt;e;  $$ \\text{ Forward Algorithm} P(a, b, c, d, e)=P(a) P(b | a) \\cdot P(c | b) \\cdot P(d | c) \\cdot P(e|d) P(e)=\\sum{a, b,c, d} P(a, b, c, d, e) =\\sum{d} p(e | d) \\sum{c} p(d | c) \\underbrace{ \\sum{b} p(c | b) \\underbrace{ \\sum{a} p(b | a) p(a) }{m{a\\rightarrow b }(b)}}{m_{b \\rightarrow c}\u0026copy;} $$\n$$ p\u0026copy;=\\sum{a, b, d, e} p(a, b, c, d, e) = \\left(\\sum{b} p(c | b) \\cdot \\sum{a} p(b | a) \\cdot p(a) \\right) \\left(\\sum{d} p(d | c) \\sum_{e} p(e | d)\\right)\n\n\\text{ Forward-Backward Algorithm} $$\ngraph TD; a---b; b---d; b---c;  $$ \\begin{aligned} \u0026amp; p(a, b, c, d) \\=\u0026amp; \\frac{1}{z} \\psi{a}(a) \\psi{b}(b) \\cdot \\psi{c}\u0026copy; \\cdot \\varphi(d) \\ \u0026amp; \\cdot \\psi{a, b}(a, b) \\cdot \\psi{b, c}(b, c) \\cdot \\psi{b, d}(b, d) \\end{aligned} $$\n","date":1584654998,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584678125,"objectID":"97dc121d7ddd7c67d1b89d3e4f7cf1a1","permalink":"https://faithio.cn/post/probabilistic-graphical-model/","publishdate":"2020-03-19T17:56:38-04:00","relpermalink":"/post/probabilistic-graphical-model/","section":"post","summary":"pdf graph TD; 概率图--\u0026gt;Representation-表示; 概率图--\u0026gt;Inference-推断; 概率图--\u0026gt;Learning-","tags":[],"title":"Probabilistic Graphical Model","type":"post"},{"authors":[],"categories":[],"content":"When I run some python code from github, it occur the following problem as screenshot.\n RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of \u0026lsquo;python\u0026rsquo; with \u0026lsquo;pythonw\u0026rsquo;. See \u0026lsquo;Working with Matplotlib on OSX\u0026rsquo; in the Matplotlib FAQ for more information.\n Solution:(https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python)\nProblem Cause In mac os image rendering back end of matplotlib (what-is-a-backend to render using the API of Cocoa by default). There is Qt4Agg and GTKAgg and as a back-end is not the default. Set the back end of macosx that is differ compare with other windows or linux os.\nI resolve this issue following ways:\n I assume you have installed the pip matplotlib, there is a directory in you root called ~/.matplotlib. Create a file ~/.matplotlib/matplotlibrc there and add the following code: backend: TkAgg  From this link you can try different diagram.\n","date":1583775429,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583802329,"objectID":"f401314725b6ff0b004bf2f679a80813","permalink":"https://faithio.cn/post/run-issue-with-matplotlib-in-mac-os-x/","publishdate":"2020-03-09T13:37:09-04:00","relpermalink":"/post/run-issue-with-matplotlib-in-mac-os-x/","section":"post","summary":"When I run some python code from github, it occur the following problem as screenshot.\n RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends.","tags":[],"title":"Run Issue With Matplotlib in Mac OS X","type":"post"},{"authors":[],"categories":[],"content":" Math Formula Factor graphs Shows how a function of several variables can be factored into a product of simpler functions.\n $$ f(x,y,z) = (x+y) \\cdot (y + z) \\cdot (x +z) $$\n Very useful for representing posteriors.\n$ $$ P(x1, ..., x_n) = P(x_1) \\Pi P( x_i | x_{i-1} ) $$ $\n$$ P(m|x1, \u0026hellip;, x_n) = P(m) \\cdot \\Pi P(x_i|m)$$\nmodeling  What graph should I use for this data?  Inference  Given the graph and data, what is the mean of x algorithm  Sampling Variable elimination Message-passing(Expectation Propagation, Variational Bayes)   Cutter problem  Want to estimate x given multiple y\u0026rsquo;s $$ p(x) = \\mathcal{N}(x; 0, 100) $$ $$ p(y_i|x) = (0.5)\\mathcal{N}(y_i; x, 1) + (0.5)\\mathcal{N} (y_i;0,10)$$  -\u0026gt; $ P(x|y1, \u0026hellip;, y_n) = P(x) \\cdot \\Pi P(y_i|x)$\nif we only have 2 points:\n$$ P(x) \\cdot P(y_1|x) \\cdot P(y_2|x) \\rightarrow p(y_i|x) = (0.5)\\mathcal{N}(y_i; x, 1) + (0.5)\\mathcal{N} (y_i;0,10)$$\n2 points have 4 Gaussians -\u0026gt; N points $$2^N$$ Gaussians\n https://zhuanlan.zhihu.com/p/75617364 $$ p(z | w)=\\frac{p(w | z) p(z)}{p(w)}=\\frac{p(w | z) p(z)}{\\int_{z} p(w | z) p(z) d z} $$ Because it extends belief propagation. Belief propagation passes the entire distribution is the message. While EP will only pass onto the distribution certain expectation distribution allows you to you get a very compact message.\n Expectation Propagation  Fits an exponential-family approximation to the posterior. Belief propagation is a special case Kalman filtering is a special case Does not always converge.  May get stuck due to improper distributions May oscillate due to loopy graph   AGM $$ p(\\mathbf{X} | \\Theta)=\\sum{j=1}^{M} p{j} p\\left(\\mathbf{X} | \\xi_{j}\\right) $$\n $\\xi_j$ is the set of the parameters of component j. $ p_j$ are the mixing proptions which must be positive and sum to one. $\\Theta = {p_1, \\ldots, p_M, \\xi_1, \\ldots, \\xi_M}$ is the complete set of parameters fully characterizing the mixture. $ M \\geq 1$ is number of components in the mixture.  $$ p\\left(X | \\theta\\right)=\\prod{d=1}^{D} \\sqrt{\\frac{2}{\\pi}} \\frac{1}{\\left(\\sigma{l{d}}+\\sigma{r{d}}\\right)} \\times\\left{\\begin{array}{ll}\\exp \\left[-\\frac{\\left(X{d}-\\mu{d}\\right)^{2}}{2 \\sigma{l{d} }^{2}}\\right] \u0026amp; \\text { if } X{d}\u0026lt;\\mu{d} \\ \\exp \\left[-\\frac{\\left(X{d}-\\mu{d}\\right)^{2}}{2 \\sigma{r{d}}^{2}}\\right] \u0026amp; \\text { if } X{d} \\geq \\mu{d}\\end{array}\\right. $$ - $\\vec{\\mu}=\\left(\\mu{1}, \\ldots, \\mu{D}\\right)$ is the mean - $\\vec{\\sigma}{l}=\\left(\\vec{\\sigma}{l{1}}, \\ldots, \\vec{\\sigma}{l{D}}\\right)$ is the left standard deviation - $\\vec{\\sigma}{r}=\\left(\\vec{\\sigma}{r{1}}, \\ldots, \\vec{\\sigma}{r_{D}}\\right)$ is the right standard deviation\n$$ \\log P = \\sum{d=1}^{D} \\log \\sqrt{\\frac{2}{\\pi}} - \\frac{1}{2}\\log (\\sqrt{v{l{d}}} + \\sqrt{v{r{d}}}) - \\left{\\begin{array}{ll} \\frac{\\left(X{d}-\\mu{d}\\right)^{2}}{2 v{l{d} }} \u0026amp; \\text { if } X{d}\u0026lt;\\mu{d} \\frac{\\left(X{d}-\\mu{d}\\right)^{2}}{2 v{r{d} }} \u0026amp; \\text { if } X{d} \\geq \\mu{d} \\end{array}\\right. \\frac{\\partial \\log P}{\\partial v{l{d}}} = -\\frac{1}{4}\\frac{1}{v{l{d}} + \\sqrt{v{l{d}} v{r{d}} }} + \\left{\\begin{array}{ll} \\frac{\\left(X{d}-\\mu{d}\\right)^{2}}{2 v{l{d}}^2} \u0026amp; \\text { if } X{d}\u0026lt;\\mu{d} 0 \u0026amp; \\text { if } X{d} \\geq \\mu_{d} \\end{array}\\right. $$\n$$ p(X, \\boldsymbol{\\theta})=\\prod{i} f{i}(\\boldsymbol{\\theta}) = \\prod_{i} p(xi|\\boldsymbol{\\theta})\n%p\\left(\\theta | X \\right) = \\frac{1}{p(X)} \\prod{i} f_{i}(\\boldsymbol{\\theta}) $$\n$$ %p(X)= \\int \\prod{i} f{i}(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta} $$\nHere, $p(\\vec{X})$ is very intractable to calculate and we don\u0026rsquo;t know $ f_{i}(\\boldsymbol{\\theta}) $.\nNow we consider using EP. The approximation, $q\\left(\\theta_j \\right)$ , of the posterior, $p\\left( \\theta_j | \\vec{X} \\right)$ , is assumed to have same functional form. $$ q(\\theta_j)=\\frac{1}{Z} \\prod_i \\widetilde{f}_i(\\theta_j) $$\nWe hope that: $$ \\mathrm{KL}(p | q)=\\mathrm{KL}\\left(\\frac{1}{p(X)} \\prod{i} f{i}(\\boldsymbol{\\theta}) | \\frac{1}{Z} \\prod{i} \\widetilde{f}{i}(\\boldsymbol{\\theta})\\right) $$\nIn general, this minimization will be intractable because the KL divergence involves averaging with respect to the true distribution.\nBut We can use EP:\n first choose a factor $\\widetilde{f}_{j}$ to approximate.  Begin Loop, until the following steps are convergence.\n second compute the cavity distribution $q^{\\backslash j}(\\boldsymbol{\\theta})$:  $$ q^{\\backslash j}(\\boldsymbol{\\theta})=\\frac{q(\\boldsymbol{\\theta})}{\\widetilde{f}_{j}(\\boldsymbol{\\theta})} \\\n\\hat{p} = \\frac{1}{Z{j}} f{j}(\\boldsymbol{\\theta}) q^{\\backslash j}(\\boldsymbol{\\theta}) $$\nHere $q^{\\backslash j}(\\boldsymbol{\\theta})$ is called the cavity distribution. $\\hat{p}$ is defined as a product of the exact factor $f_i$ with the rest of the factors approximated, normalised to 1, and the cavity distribution needs to be computed in order to express $\\hat{p}$.\n $$ q^{\\text {new }}(\\boldsymbol{\\theta}) \\propto \\widetilde{f}{j}(\\boldsymbol{\\theta}) \\prod{i \\neq j} \\tilde{f}{i}(\\boldsymbol{\\theta}) p \\propto \\hat{p} = f{j}(\\boldsymbol{\\theta}) \\prod{i \\neq j} \\tilde{f}{i}(\\boldsymbol{\\theta}) $$\n  Then compute the approximative distribution $q^{new}$  $$ \\arg \\min \\mathrm{KL}(\\hat{p} | q^{new}(\\theta)) = \\arg \\min \\mathrm{KL}\\left(\\frac{f{j}(\\boldsymbol{\\theta}) q^{\\backslash j}(\\boldsymbol{\\theta})}{Z{j}} | q^{\\mathrm{new}}(\\boldsymbol{\\theta})\\right) $$\n More generally, it is straightforward to obtain the required expectations for any member of the exponential family, provided it can be normalized, because the expected statistics can be related to the derivatives of the normalization coefficient. $$ \\text{bishop:} p(\\mathbf{x} | \\boldsymbol{\\eta})=h(\\mathbf{x}) g(\\boldsymbol{\\eta}) \\exp \\left{\\boldsymbol{\\eta}^{\\mathrm{T}} \\mathbf{u}(\\mathbf{x})\\right} -\\nabla \\ln g(\\boldsymbol{\\eta})=\\mathbb{E}[\\mathbf{u}(\\mathbf{x})] $$\n  Update the factor  $$ q^{\\text {new }}(\\boldsymbol{\\theta}) \\propto \\hat{p} = \\frac{1}{Z{j}} f{j}(\\boldsymbol{\\theta}) q^{\\backslash j}(\\boldsymbol{\\theta}) $$\nThen we easily obtain the formula for the approximation of $fi$: $$ f{i} \\approx \\tilde{f}{i}=Z{i} \\frac{q^{\\text {new }}(\\boldsymbol{\\theta})}{q^{\\backslash j}(\\boldsymbol{\\theta})} $$ This division of distributions is from exponetial family, so does the result $\\tilde{f}_{i}$. Now repeat it until parameter covergence.\nEnd Loop.\n Evaluate the approximation to the model evidence  After the algorithm has converged to a set of factors $\\left{\\tilde{f}{i}\\right}$, the approximate posterior as well as the model evidence can be computed as following: $$ p(X, \\boldsymbol{\\theta}) \\simeq \\prod{i} \\tilde{f}{i}(\\boldsymbol{\\theta}) p(X) \\simeq \\int \\prod{i} \\tilde{f}_{i}(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta} $$\n$$ p(\\mathbf{X} | \\boldsymbol{\\theta})=(1-w) \\mathcal{A}(\\mathbf{X} | \\boldsymbol{\\theta}, \\mathbf{I_l}, \\mathbf{I_r})+w \\mathcal{A}(\\mathbf{X} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}) $$\nwhere w is the proportion of background clutter. And the prior over $\\mathbf{\\theta}$(mean) is taken to be Asymmetric Gaussian.\nAnd $$ p(\\boldsymbol{\\theta})= \\mathcal{A}(\\mathbf{X} | \\mathbf{0}, b \\mathbf{I_l}, b \\mathbf{Ir}) $$ $$ p(X, \\boldsymbol{\\theta})=p(\\boldsymbol{\\theta}) \\prod{n=1}^{N} p\\left(\\mathbf{x}_{n} | \\boldsymbol{\\theta}\\right) $$\n1. initialize the approximating factors we select an approximating distribution from the exponential family to approximate the stochastic variables $\\theta$ $$ q_0(\\boldsymbol{\\theta})\n= \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{0}, b \\mathbf{I_l}, b \\mathbf{I_r}) $$\n$$ \\widetilde{f}_{n}(\\boldsymbol{\\theta})=s_n \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}n, \\mathbf{\\sigma{rn}^2}, \\mathbf{\\sigma{l_n}^2} \\right) = s_n \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}n, \\mathbf{v{rn} I}, \\mathbf{v{l_n} I} \\right) $$\n$$ sn = \\prod{d=1}^{D} \\sqrt{\\frac{2}{\\pi}} \\frac{1}{\\left(\\sigma{l{d}}+\\sigma{r{d}}\\right)} $$ While $\\sigma_{ln} \\rightarrow \\infty, \\sigma{r_n} \\rightarrow \\infty $ and $ \\mu_n = 0 $.\n###2. initialize the posterior approximation $q(\\boldsymbol{\\theta})$\nWe chooses the parameter values a = 10, b = 100 and w = 0.5 and use $v$ denote $ \\sigma^2$ as following, then $\\mathbf{v_r} = \\mathbf{v_l} = b = 100$\n3. Until all $(\\mun, v{ln}, v{r_n}, s_n)$ converge: $$ q^{\\backslash n}(\\boldsymbol{\\theta})=\\frac{q(\\boldsymbol{\\theta})}{\\widetilde{f}_n(\\boldsymbol{\\theta})} = \\frac{\\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu}, \\mathbf{v_r I}, \\mathbf{v_l I})}{s_n \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}n, \\mathbf{v{rn} I}, \\mathbf{v{l_n} I} \\right)} \\propto \\left{\\begin{array}{ll}\n{\\frac{\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mu})^{T}(v_l \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu})\\right}}{\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mun})^{T}(v{l_n} \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu_n})\\right}}} \u0026amp;\u0026amp; \\text { if } X\u0026lt;\\mu \\\n{\\frac{\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mu})^{T}(v_r \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu})\\right}}{\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mun})^{T}(v{r_n} \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu_n})\\right}}} \u0026amp;\u0026amp; \\text { if } X\u0026gt;\\mu\n\\end{array}\\right. \\\n= \\left{\\begin{array}{ll}\n\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mu})^{T}(v_l \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu}) + \\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mun})^{T}(v{l_n} \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu_n})\\right} \u0026amp; \\text { if } X\u0026lt;\\mu \\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mu})^{T}(v_r \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu}) +\n\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mun})^{T}(v{r_n} \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu_n}) \\right} \u0026amp; \\text { if } X\u0026gt;\\mu \\end{array}\\right. $$ - Remove the current estimate $\\widetilde{f}_j(\\boldsymbol{\\theta})$ from $q(\\theta)$, then we has mean and inverse variance given by: $$ \\left{\\begin{array}{ll} \\left({v_l}^{\\backslash n}\\right)^{-1}={v_l}^{-1}-{vl}{n}^{-1} \u0026amp; \\text { if } X\u0026lt;\\mu \\left({v_r}^{\\backslash n}\\right)^{-1}={v_r}^{-1}-{vr}{n}^{-1} \u0026amp; \\text { if } X\u0026gt;\\mu \\end{array}\\right. $$\n$$ \\mathbf{\\mu}^{\\backslash n}= \\mathbf{\\mu}+\n\\left{\\begin{array}{ll} {v_l}^{\\backslash n} {vl}{n}^{-1}\\left(\\mathbf{\\mu}-\\mathbf{\\mu}_{n}\\right) \u0026amp; \\text { if } X\u0026lt;\\mu \\\n{v_r}^{\\backslash n} {vr}{n}^{-1}\\left(\\mathbf{\\mu}-\\mathbf{\\mu}_{n}\\right) \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. \u0026mdash;-\n\\begin{aligned} {v^{\\backslash{n}}}^{-1} \u0026amp;= v^{-1} - v_n^{-1} {\\mu}^{\\backslash n} \u0026amp;= v^{\\backslash{n}}(\\mu v^{-1} - \\mu_n v_n^{-1}) \u0026amp;= v^{\\backslash{n}}[\\mu ({v^{\\backslash{n}}}^{-1} + v_n^{-1}) - \\mu_n v_n^{-1}] \u0026amp;= \\mu + v^{\\backslash{n}} v_n^{-1} \\mu - v^{\\backslash{n}} v_n^{-1} \\mu_n \u0026amp;= \\mu + v^{\\backslash{n}} v_n^{-1} (\\mu -\\mun) \\end{aligned} $$ \u0026gt; Cavity Distribution: \u0026gt; $$ \u0026gt; q^{\\backslash n}(\\boldsymbol{\\theta})=\\frac{q(\\boldsymbol{\\theta})}{\\widetilde{f}{n}(\\boldsymbol{\\theta})} \u0026gt; $$\n Recompute $(\\mu, v, Z)$ from $(\\mathbf{\\mu}^{\\backslash n}, {v_l}^{\\backslash n}, {vr}^{\\backslash n})$ $$ Z{n}=(1-w) \\mathcal{A}\\left(\\mathbf{x}_{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(vr^{\\backslash n}+1\\right) \\mathbf{I}\\right)+w \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{Ir}\\right) $$ \u0026gt;$$ \u0026gt;\\begin{aligned} \u0026gt;Z{n} \u0026amp;=\\int q^{\\backslash n}(\\boldsymbol{\\theta}) f_{n}(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta} \\ \u0026gt;\u0026amp;=\\int q^{\\backslash n}(\\boldsymbol{\\theta}) P(X|\\mu) \\mathrm{d} \\boldsymbol{\\theta} \u0026gt; \u0026gt;\u0026amp;=\\int \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu^{\\backslash n}}, v_l^{\\backslash n} \\mathbf{I}, v_r^{\\backslash n} \\mathbf{I}) \\cdot { (1-w) \\mathcal{A}(\\mathbf{x_n} | \\boldsymbol{\\mu}, \\mathbf{I_l}, \\mathbf{I_r})+w \\mathcal{A}(\\mathbf{x_n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r})} \\mathrm{d} \\boldsymbol{\\theta} \u0026gt; \u0026gt;\u0026amp;= (1-w)\\int \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu^{\\backslash n}}, v_l^{\\backslash n} \\mathbf{I}, v_r^{\\backslash n} \\mathbf{I}) \\mathcal{A}(\\mathbf{x_n} | \\boldsymbol{\\mu}, \\mathbf{I_l}, \\mathbf{I_r}) \\mathrm{d} \\boldsymbol{\\theta} \u0026gt;\u0026amp;+ w \\int \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu^{\\backslash n}}, v_l^{\\backslash n} \\mathbf{I}, v_r^{\\backslash n} \\mathbf{I}) \u0026gt;\\mathcal{A}(\\mathbf{x_n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{Ir})} \\mathrm{d} \\boldsymbol{\\theta} \u0026gt;\u0026amp;=(1-w) \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(vr^{\\backslash n}+1\\right) \\mathbf{I}\\right)+w \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}\\right) \u0026gt;\\end{aligned} \u0026gt;$$  we assumed that $f{0}(\\boldsymbol{\\theta})=p(\\boldsymbol{\\theta})$ and $ f{n}(\\boldsymbol{\\theta})=p\\left(\\mathbf{x}_{n} | \\boldsymbol{\\theta}\\right) = (1-w) \\mathcal{A}(\\mathbf{X} | \\boldsymbol{\\mu}, \\mathbf{I_l}, \\mathbf{I_r})+w \\mathcal{A}(\\mathbf{X} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}) $, also $q(\\boldsymbol{\\theta})=\\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{m}, v_l \\mathbf{I}, vr \\mathbf{I}) $ $$ \\begin{aligned} \\rho{n} \u0026amp;=\\frac{1}{Z{n}}(1-w) \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(vr^{\\backslash n}+1\\right) \\mathbf{I}\\right) \u0026amp;= \\frac{1}{Z{n}}(1-w)\\cdot \\frac{Zn - w \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}\\right)}{1-w} \u0026amp;= 1 - \\frac{w}{Zn} \\cdot \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{Ir}\\right) \\end{aligned} $$ \u0026gt;Then our goal is to minimize: \u0026gt;$$ \u0026gt;\\mathrm{KL}\\left(\\frac{f{n}(\\boldsymbol{\\theta}) q^{\\backslash n}(\\boldsymbol{\\theta})}{Z_{n}} | q^{\\mathrm{new}}(\\boldsymbol{\\theta})\\right) \u0026gt;$$\nBasic rule for Asymmetric Gaussian:\nhttps://stats.stackexchange.com/questions/27436/how-to-take-derivative-of-multivariate-normal-density $$ \\nabla_{\\boldsymbol{\\mu}} \\mathcal{A}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{v_l}, \\mathbf{v_r})=\n\\left{\\begin{array}{ll}\n\\mathcal{A}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{v_l}, \\mathbf{v_r}) \\cdot(\\mathbf{x}-\\boldsymbol{\\mu}) \\mathbf{v_l}^{-1} \u0026amp; \\text { if } X\u0026lt;\\mu \\\n\\mathcal{A}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{v_l}, \\mathbf{v_r}) \\cdot(\\mathbf{x}-\\boldsymbol{\\mu}) \\mathbf{v_r}^{-1} \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$ So we compute the mean and variance: $$ \\begin{aligned} \\nabla{\\mathbf{\\mu}^{\\backslash n}} \\ln Z{n} \u0026amp;=\\frac{1}{Z{n}} \\cdot \\nabla{\\mathbf{\\mu}^{\\backslash n}} Z_{n} \\\n\u0026amp;=\\frac{1}{Z{n}} \\cdot \\nabla{\\mathbf{\\mu}^{\\backslash n}} \\int q^{\\backslash n}(\\boldsymbol{\\theta})f_{n}(\\boldsymbol{\\theta}) d \\boldsymbol{\\theta} \\\n\u0026amp;=\\frac{1}{Z{n}} \\cdot \\nabla{\\mathbf{\\mu}^{\\backslash n}} \\int q^{\\backslash n}(\\boldsymbol{\\theta}) p\\left(\\mathbf{x}_{n} | \\boldsymbol{\\theta}\\right) d \\boldsymbol{\\theta} \\\n\u0026amp;=\\frac{1}{Z{n}} \\cdot \\int\\left{\\nabla{\\mathbf{\\mu}^{\\backslash n}} q^{\\backslash n}(\\boldsymbol{\\theta})\\right} \\cdot p\\left(\\mathbf{x}_{n} | \\boldsymbol{\\theta}\\right) d \\boldsymbol{\\theta} \\\n\u0026amp;=\\frac{1}{Z{n}} \\cdot \\int \\frac{1}{v^{\\backslash n}}\\left(\\boldsymbol{\\theta}-\\mathbf{\\mu}^{\\backslash n}\\right) \\cdot q^{\\backslash n}(\\boldsymbol{\\theta}) \\cdot p\\left(\\mathbf{x}{n} | \\boldsymbol{\\theta}\\right) d \\theta\\\n\u0026amp;=\\frac{1}{Z{n}} \\cdot \\frac{1}{v^{\\backslash n}} \\cdot\\left{\\int \\boldsymbol{\\theta} \\cdot q^{\\backslash n}(\\boldsymbol{\\theta}) \\cdot p\\left(\\mathbf{x}{n} | \\boldsymbol{\\theta}\\right) d \\boldsymbol{\\theta}-\\int \\mathbf{\\mu}^{\\backslash n} \\cdot q^{\\backslash n}(\\boldsymbol{\\theta}) \\cdot p\\left(\\mathbf{x}_{n} | \\boldsymbol{\\theta}\\right) d \\boldsymbol{\\theta}\\right} \\\n\u0026amp;=\\frac{1}{v^{\\backslash n}} \\cdot\\left{\\mathbb{E}[\\boldsymbol{\\theta}]-\\mathbf{\\mu}^{\\backslash n}\\right} \\\n\u0026amp;= \\left{\\mathbb{E}[\\boldsymbol{\\theta}]-\\mathbf{\\mu}^{\\backslash n}\\right} \\cdot \\left{\\begin{array}{ll} \\frac{1}{vl^{\\backslash n}} \u0026amp; \\text { if } X{d}\u0026lt;\\mu_{d} \\frac{1}{vr^{\\backslash n}} \u0026amp; \\text { if } X{d} \\geqslant \\mu_{d} \\end{array}\\right. \\end{aligned} $$\n$$ \\text{According to the following}: q^{\\backslash n}(\\boldsymbol{\\theta})=\\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu^{\\backslash n}}, v_l^{\\backslash n} \\mathbf{I}, vr^{\\backslash n} \\mathbf{I}) q^{\\backslash n}(\\boldsymbol{\\theta}) \\cdot p\\left(\\mathbf{x}{n} | \\boldsymbol{\\theta}\\right)=Z_{n} \\cdot q^{new}(\\theta) $$\n$$ \\begin{aligned} \\mathbb{E}[\\boldsymbol{\\theta}] \u0026amp;=\\mathbf{\\mu}^{\\backslash n}+v^{\\backslash n} \\cdot \\nabla{\\mathbf{\\mu}^{\\backslash n}} \\ln Z{n} \\ \u0026amp;=\\mathbf{\\mu}^{\\backslash n}+v^{\\backslash n} \\cdot \\frac{1}{Z{n}} \\nabla{\\mathbf{\\mu}^{\\backslash n}} Z_n \\\n\u0026amp;=\\mathbf{\\mu}^{\\backslash n}+v^{\\backslash n} \\cdot \\frac{1}{Z{n}} \\nabla{\\mathbf{\\mu}^{\\backslash n}} (1-w) \\mathcal{A}\\left(\\mathbf{x}_{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(vr^{\\backslash n}+1\\right) \\mathbf{I}\\right)+w \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}\\right) \\\n\u0026amp;=\\mathbf{\\mu}^{\\backslash n}+v^{\\backslash n} \\cdot \\frac{1}{Z{n}}(1-w) \\nabla{\\mathbf{\\mu}^{\\backslash n}} \\mathcal{A}\\left(\\mathbf{x}_{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(v_r^{\\backslash n}+1\\right) \\mathbf{I}\\right)\n%\\cdot \\frac{1}{v^{\\backslash n}+1}\\left(\\mathbf{x}_{n}-\\mathbf{\\mu}^{\\backslash n}\\right)\n\\\n\u0026amp;=\\mathbf{\\mu}^{\\backslash n}+v^{\\backslash n} \\cdot \\rho{n} \\cdot \\frac{1}{v^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \\\n\u0026amp;=\\mathbf{\\mu}^{\\backslash n}+ \\rho_{n} \\cdot\n\\left{\\begin{array}{ll} \\frac{1}{vl^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \\cdot v_l^{\\backslash n}\n\u0026amp; \\text { if } X{d}\u0026lt;\\mu{d} \\frac{1}{vr^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \\cdot vl^{\\backslash n} \u0026amp; \\text { if } X{d} \\geqslant \\mu_{d} \\end{array}\\right.\n\\\n\\text{According to: }\u0026amp; \\rho{n} = 1 - \\frac{w}{Zn} \\cdot \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}\\right) \\\n\u0026amp;\\text{Here we match first moment}: \\mathbb{E}[\\boldsymbol{\\theta}] = \\mathbf{\\mu^{new}} \\end{aligned} $$\nNow we consider when: $$ \\text { if } X{d}\u0026lt;\\mu{d} $$\n$$ \\begin{aligned} \\nabla_{vl^{\\backslash n}} \\ln Z{n} \u0026amp;=\\frac{1}{Z{n}} \\cdot \\nabla{vl^{\\backslash n}} Z{n} \\ \u0026amp;=\\frac{1}{Z{n}} \\cdot \\nabla{vl^{\\backslash n}} \\int q^{\\backslash n}(\\boldsymbol{\\theta}) p\\left(\\mathbf{x}{n} | \\boldsymbol{\\theta}\\right) d \\boldsymbol{\\theta} \\\n\u0026amp;=\\frac{1}{Z{n}} \\cdot \\int\\left{\\nabla{vl^{\\backslash n}} q^{\\backslash n}(\\boldsymbol{\\theta})\\right} p\\left(\\mathbf{x}{n} | \\boldsymbol{\\theta}\\right) d \\boldsymbol{\\theta} \\\n\u0026amp;=\\frac{1}{Z_{n}} \\cdot \\int\\left{\n\\frac{1}{2\\left(v_l^{\\backslash n}\\right)^{2}}\\left| \\boldsymbol{\\theta} - \\mathbf{\\mu}^{\\backslash n}\\right|^{2}-\\frac{D}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } }\n\\right}\nq^{\\backslash n}(\\boldsymbol{\\theta}) \\cdot p\\left(\\mathbf{x}_{n} | \\boldsymbol{\\theta}\\right) d \\boldsymbol{\\theta} \\\n\u0026amp;=\\int q^{\\mathrm{new}}(\\boldsymbol{\\theta}) \\cdot\\left{\\frac{1}{2\\left(v_l^{\\backslash n}\\right)^{2}}\\left(\\mathbf{\\mu}^{\\backslash n}-\\boldsymbol{\\theta}\\right)^{T}\\left(\\mathbf{\\mu}^{\\backslash n}-\\boldsymbol{\\theta}\\right)-\\frac{D}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } }\\right} d \\boldsymbol{\\theta} \\\n\u0026amp;=\\frac{1}{2\\left(v_l^{\\backslash n}\\right)^{2}}\\left{\\mathbb{E}\\left[\\boldsymbol{\\theta} \\boldsymbol{\\theta}^{T}\\right]-2 \\mathbb{E}[\\boldsymbol{\\theta}] \\mathbf{\\mu}^{\\backslash n}+\\left|\\mathbf{\\mu}^{\\backslash n}\\right|^{2}\\right}-\\frac{D}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } } \\end{aligned} $$\nSo we rearrange the above equation: $$ \\mathbb{E}\\left[\\boldsymbol{\\theta} \\boldsymbol{\\theta}^{T}\\right]=2\\left(vl^{\\backslash n}\\right)^{2} \\cdot \\nabla{vl^{\\backslash n}} \\ln Z{n}+2 \\mathbb{E}[\\boldsymbol{\\theta}] \\mathbf{m}^{\\backslash n}-\\left|\\mathbf{m}^{\\backslash n}\\right|^{2}+ \\frac{D \\cdot \\left(v_l^{\\backslash n}\\right)^{2}}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot vr^{\\backslash n} } } $$ Also according to: $$ Z{n}=(1-w) \\mathcal{A}\\left(\\mathbf{x}_{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(vr^{\\backslash n}+1\\right) \\mathbf{I}\\right)+w \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}\\right) $$\n$$ \\nabla_{vl^{\\backslash n}} \\ln Z{n} = (1-w) \\mathcal{A}\\left(\\mathbf{x}_{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(v_r^{\\backslash n}+1\\right) \\mathbf{I}\\right) \\cdot \\\n\\left( \\frac{1}{2\\left(v_l^{\\backslash n} + 1\\right)^{2}}\\left| \\mathbf{x_n} - \\mathbf{\\mu}^{\\backslash n}\\right|^{2}-\\frac{D}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } }\n\\right) \\\n= \\rho_n \\cdot \\left( \\frac{1}{2\\left(v_l^{\\backslash n} + 1\\right)^{2}}\\left| \\mathbf{x_n} - \\mathbf{\\mu}^{\\backslash n}\\right|^{2}-\\frac{D}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } }\n\\right) $$\nAccording to the bellow formula: $$ v \\mathbf{I}=\\mathbb{E}\\left[\\boldsymbol{\\theta} \\boldsymbol{\\theta}^{T}\\right]-\\mathbb{E}[\\boldsymbol{\\theta}] \\mathbb{E}\\left[\\boldsymbol{\\theta}^{T}\\right] $$\n$$ \\begin{aligned} v_l^{new} \u0026amp;=\\frac{1}{D} \\cdot\\left{\\mathbb{E}\\left[\\boldsymbol{\\theta}^{T} \\boldsymbol{\\theta}\\right]-\\mathbb{E}\\left[\\boldsymbol{\\theta}^{T}\\right] \\mathbb{E}[\\boldsymbol{\\theta}]\\right}=\\frac{1}{D} \\cdot\\left{\\mathbb{E}\\left[\\boldsymbol{\\theta}^{T} \\boldsymbol{\\theta}\\right]-|\\mathbb{E}[\\boldsymbol{\\theta}]|^{2}\\right} \u0026amp;=\\frac{1}{D} \\cdot\\left{ 2\\left(vl^{\\backslash n}\\right)^{2} \\cdot \\nabla{vl^{\\backslash n}} \\ln Z{n}+2 \\mathbb{E}[\\boldsymbol{\\theta}] \\mathbf{\\mu}^{\\backslash n}-\\left|\\mathbf{\\mu}^{\\backslash n}\\right|^{2}\n \\frac{D \\cdot \\left(v_l^{\\backslash n}\\right)^{2}}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } } - |\\mathbb{E}[\\boldsymbol{\\theta}]|^{2} \\right}\n  \u0026amp;=\\frac{1}{D} \\cdot\\left{ 2\\left(vl^{\\backslash n}\\right)^{2} \\cdot \\nabla{vl^{\\backslash n}} \\ln Z{n} - \\left|\\mathbb{E}[\\boldsymbol{\\theta}]-\\mathbf{\\mu}^{\\backslash n}\\right|^{2} + \\frac{D \\cdot \\left(v_l^{\\backslash n}\\right)^{2}}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } } \\right} \\\n\u0026amp;= \\frac{1}{D} \\cdot\\left{ 2\\left(vl^{\\backslash n}\\right)^{2} \\cdot \\nabla{vl^{\\backslash n}} \\ln Z{n} - \\left| \\rho_n \\cdot \\frac{1}{vl^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \\cdot v_l^{\\backslash n} \\right|^{2} + \\frac{D \\cdot \\left(v_l^{\\backslash n}\\right)^{2}}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } } \\right}\n\\end{aligned} $$ substitute $\\nabla_{vl^{\\backslash n}} \\ln Z{n}$: $$ \\begin{aligned} v_l^{new} \u0026amp;= \\frac{1}{D} \\cdot\\left{ 2\\left(vl^{\\backslash n}\\right)^{2} \\cdot \\nabla{vl^{\\backslash n}} \\ln Z{n} - \\left| \\rho_n \\cdot \\frac{1}{vl^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \\cdot v_l^{\\backslash n} \\right|^{2} + \\frac{D \\cdot \\left(v_l^{\\backslash n}\\right)^{2}}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } } \\right} \\\n\u0026amp;= \\frac{1}{D} \\cdot\\left{ 2\\left(v_l^{\\backslash n}\\right)^{2} \\cdot\n\\rho_n \\cdot \\left( \\frac{1}{2\\left(v_l^{\\backslash n} + 1\\right)^{2}}\\left| \\mathbf{x_n} - \\mathbf{\\mu}^{\\backslash n}\\right|^{2}-\\frac{D}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } }\n\\right)\n \\left| \\rho_n \\cdot \\frac{1}{vl^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \\cdot v_l^{\\backslash n} \\right|^{2} + \\frac{D \\cdot \\left(v_l^{\\backslash n}\\right)^{2}}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } } \\right} \u0026amp;=\\frac{\\left(v_l^{\\backslash n}\\right)^{2} - 2 \\left(v_l^{\\backslash n}\\right)^{2} \\rho_n}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot vr^{\\backslash n} } } + \\rho{n}\\left(1-\\rho_{n}\\right) \\frac{\\left(vl^{\\backslash n}\\right)^{2}\\left|\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right|^{2}}{D\\left(v_l^{\\backslash n}+1\\right)^{2}}  \\end{aligned} $$ So in conclusions: we have:\n$$ \\mathbf{\\mu^{new}}=\\mathbf{\\mu}^{\\backslash n}+\n\\left{\\begin{array}{ll}\n\\rho_{n} \\frac{v_l^{\\backslash n}}{vl^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \u0026amp; \\text { if } X\u0026lt;\\mu \\\n\\rho_{n} \\frac{v_r^{\\backslash n}}{vr^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$\n$$ \\left{\\begin{array}{ll} v_l^{new}= \\frac{\\left(v_l^{\\backslash n}\\right)^{2} - 2 \\left(v_l^{\\backslash n}\\right)^{2} \\rho_n}{4 v_l^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot vr^{\\backslash n} } } + \\rho{n}\\left(1-\\rho_{n}\\right) \\frac{\\left(vl^{\\backslash n}\\right)^{2}\\left|\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right|^{2}}{D\\left(v_l^{\\backslash n}+1\\right)^{2}}\n\u0026amp; \\text { if } X\u0026lt;\\mu \\\nv_r^{new}= \\frac{\\left(v_r^{\\backslash n}\\right)^{2} - 2 \\left(v_r^{\\backslash n}\\right)^{2} \\rho_n}{4 v_r^{\\backslash n} + \\sqrt{ v_l^{\\backslash n} \\cdot v_r^{\\backslash n} } }\n+\\rho{n}\\left(1-\\rho{n}\\right) \\frac{\\left(vr^{\\backslash n}\\right)^{2}\\left|\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right|^{2}}{D\\left(v_r^{\\backslash n}+1\\right)^{2}} \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$ - Evaluate and store the new factor\n Update formula for $\\hat{f}i$ $$ \\left{\\begin{array}{ll} \\left({v{l_n}}\\right)^{-1}={(v_l^{new})}^{-1}-({vl}^{\\backslash n})^{-1} \u0026amp; \\text { if } X\u0026lt;\\mu \\left({v{r_n}}\\right)^{-1}={(v_r^{new})}^{-1}-({v_r}^{ \\backslash n})^{-1} \u0026amp; \\text { if } X\u0026gt;\\mu \\end{array}\\right. $$\n $$ \\mathbf{\\mu}_{n}=\\mathbf{\\mu}^{\\backslash n}+ \\left{\\begin{array}{ll}\n\\left(v_{n}+v^{\\backslash n}\\right)\\left(v^{\\backslash n}\\right)^{-1}\\left(\\mathbf{\\mu}^{\\mathrm{new}}-\\mathbf{\\mu}^{\\backslash n}\\right) \u0026amp; \\text { if } X\u0026lt;\\mu \\\n\\left(v_{n}+v^{\\backslash n}\\right)\\left(v^{\\backslash n}\\right)^{-1}\\left(\\mathbf{\\mu}^{\\mathrm{new}}-\\mathbf{\\mu}^{\\backslash n}\\right) \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$\n$$ \\begin{aligned} \u0026amp; \\widetilde{f}{n}(\\boldsymbol{\\theta})=Z{n} \\frac{q^{\\mathrm{new}}(\\boldsymbol{\\theta})}{q^{\\backslash n}(\\boldsymbol{\\theta})} \\\n\\Rightarrow \u0026amp; Z_n q^{\\mathrm{new}}(\\boldsymbol{\\theta}) = s_n \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}n, \\mathbf{v{rn} I}, \\mathbf{v{l_n} I} \\right) q^{\\backslash n}(\\boldsymbol{\\theta}) = s_n \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}n, \\mathbf{v{rn} I}, \\mathbf{v{l_n} I} \\right) \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}^{\\backslash n}, \\mathbf{v_r^{\\backslash n} I}, \\mathbf{v_l^{\\backslash n} I} \\right) \\\n\\Rightarrow \u0026amp; \\int Z_n q^{\\mathrm{new}}(\\boldsymbol{\\theta}) d\\theta = \\int s_n \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}n, \\mathbf{v{rn} I}, \\mathbf{v{l_n} I} \\right) \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}^{\\backslash n}, \\mathbf{v_r^{\\backslash n} I}, \\mathbf{v_l^{\\backslash n} I} \\right) d \\theta \\\n\\Rightarrow \u0026amp; Z_n = s_n \\int q^{\\mathrm{new}}(\\boldsymbol{\\theta}) d\\theta =\n\\int s_n \\mathcal{A}\\left( \\mathbf{\\mu}n - \\boldsymbol{\\theta} | 0, \\mathbf{v{rn} I}, \\mathbf{v{l_n} I} \\right) \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}^{\\backslash n}, \\mathbf{v_r^{\\backslash n} I}, \\mathbf{v_l^{\\backslash n} I} \\right) d \\theta \\\n\\Rightarrow \u0026amp; Z_n = s_n \\mathcal{A}\\left(\\mathbf{\\mu}_n | \\mathbf{\\mu}^{\\backslash n}, \\mathbf{(vr^{\\backslash n}+ v{r_n}) I}, \\mathbf{(vl^{\\backslash n}+v{r_n}) I} \\right) \\end{aligned} $$\n$$ s_n = \\frac{Z_n} {\\mathcal{A}\\left(\\mathbf{\\mu}_n | \\mathbf{\\mu}^{\\backslash n}, \\mathbf{(vr^{\\backslash n}+ v{r_n}) I}, \\mathbf{(vl^{\\backslash n}+v{r_n}) I} \\right)} $$\n Evaluate the approximation to the model evidence - Posterior probability. (When $(\\mathbf{\\mu}_n, {vl} n, {v_r}_n, S_n)$ unchanged )  $$ p(X) \\simeq q(\\boldsymbol{\\theta})= \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu}, \\mathbf{v_l}, \\mathbf{vr}) = \\prod{n=0}^{N} \\widetilde{f}{n}(\\boldsymbol{\\theta})=f{0}(\\boldsymbol{\\theta}) \\prod{n=1}^{N} \\widetilde{f}{n}(\\boldsymbol{\\theta})\n= \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{0}, b \\mathbf{I_l}, b \\mathbf{Ir})\\cdot \\prod{i=1}^{N} \\mathcal{A}\\left(\\boldsymbol{\\mu}_{i}, \\mathbf{vl}{i}, \\mathbf{vr}{i}\\right) $$\n","date":1582994068,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586649401,"objectID":"985b28a7398b99c38e892b86d015b478","permalink":"https://faithio.cn/post/stochastic-expectation-propagation/","publishdate":"2020-02-29T11:34:28-05:00","relpermalink":"/post/stochastic-expectation-propagation/","section":"post","summary":"Math Formula Factor graphs Shows how a function of several variables can be factored into a product of simpler functions.\n $$ f(x,y,z) = (x+y) \\cdot (y + z) \\cdot (x +z) $$\n Very useful for representing posteriors.\n$ $$ P(x1, ..., x_n) = P(x_1) \\Pi P( x_i | x_{i-1} ) $$ $\n$$ P(m|x1, \u0026hellip;, x_n) = P(m) \\cdot \\Pi P(x_i|m)$$\nmodeling  What graph should I use for this data?","tags":[],"title":"Stochastic Expectation Propagation","type":"post"},{"authors":[],"categories":[],"content":" Introduction A hypothesis h(x), takes an input and gives us the estimated output value.\nThis hypothesis can be a as simple as a one variable linear equation, .. up to a very complicated and long multivariate equation with respect to the type of the algorithm we’re using (i.e. linear regression, logistic regression..etc).\nOur task is to find the best Parameters (a.k.a Thetas or Weights) that give us the least error in predicting the output. We call this error a Cost or Loss Function and apparently our goal is to minimize it in order to get the best predicted output!\nOne more thing to recall, that the relation between the parameter value and its effect on the cost function (i.e. the error) looks like a bell curve (i.e. Quadratic; recall this because it’s very important) .\nSo if we start at any point in that curve and if we keep taking the derivative (i.e. tangent line) of each point we stop at, we will end up at what so called the Global Optima as shown in this image: If we take the partial derivative at minimum cost point (i.e. global optima) we find the slope of the tangent line = 0 (then we know that we reached our target).\nThat’s valid only if we have Convex Cost Function, but if we don’t, we may end up stuck at what so called Local Optima; consider this non-convex function:\nNow you should have the intuition about the hack relationship between what we are doing and the terms: Deravative, Tangent Line, Cost Function, Hypothesis ..etc.\nSide Note: The above mentioned intuition also related to the Gradient Descent Algorithm (see later).\nBackground Linear Approximation:\nGiven a function, f(x), we can find its tangent at x=a. The equation of the tangent line L(x) is: L(x)=f(a)+f′(a)(x−a).\nTake a look at the following graph of a function and its tangent line:\nFrom this graph we can see that near x=a, the tangent line and the function have nearly the same graph. On occasion we will use the tangent line, L(x), as an approximation to the function, f(x), near x=a. In these cases we call the tangent line the linear approximation to the function at x=a.\nQuadratic Approximation:\nSame like linear approximation but this time we are dealing with a curve but we cannot find the point near to 0 by using the tangent line.\nInstead, we use a parabola (which is a curve where any point is at an equal distance from a fixed point or a fixed straight line), like this:\nAnd in order to fit a good parabola, both parabola and quadratic function should have same value, same first derivative, AND second derivative, \u0026hellip; the formula will be (just out of curiosity): Qa(x) = f(a) + f'(a)(x-a) + f''(a)(x-a)2/2\nNow we should be ready to do the comparison in details.\nComparison between the methods 1. Newton’s Method(newton-cg): Recall the motivation for gradient descent step at x: we minimize the quadratic function (i.e. Cost Function).\nNewton’s method uses in a sense a better quadratic function minimisation. A better because it uses the quadratic approximation (i.e. first AND second partial derivatives).\nYou can imagine it as a twisted Gradient Descent with The Hessian (The Hessian is a square matrix of second-order partial derivatives of order nxn).\nMoreover, the geometric interpretation of Newton\u0026rsquo;s method is that at each iteration one approximates f(x) by a quadratic function around xn, and then takes a step towards the maximum/minimum of that quadratic function (in higher dimensions, this may also be a saddle point). Note that if f(x) happens to be a quadratic function, then the exact extremum is found in one step.\nDrawbacks:\n It’s computationally expensive because of The Hessian Matrix (i.e. second partial derivatives calculations). It attracts to Saddle Points which are common in multivariable optimization (i.e. a point its partial derivatives disagree over whether this input should be a maximum or a minimum point!).  2. Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm(lbfgs): In a nutshell, it is analogue of the Newton’s Method but here the Hessian matrix is approximated using updates specified by gradient evaluations (or approximate gradient evaluations). In other words, using an estimation to the inverse Hessian matrix.\nThe term Limited-memory simply means it stores only a few vectors that represent the approximation implicitly.\nIf I dare say that when dataset is small, L-BFGS relatively performs the best compared to other methods especially it saves a lot of memory, however there are some “*serious*” drawbacks such that if it is unsafeguarded, it may not converge to anything.\n3. A Library for Large Linear Classification(liblinear): It’s a linear classification that supports logistic regression and linear support vector machines (A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics i.e feature value).\nThe solver uses a coordinate descent (CD) algorithm that solves optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes.\nLIBLINEAR is the winner of ICML 2008 large-scale learning challenge. It applies Automatic parameter selection (a.k.a L1 Regularization) and it’s recommended when you have high dimension dataset (recommended for solving large-scale classification problems)\nDrawbacks:\n It may get stuck at a non-stationary point (i.e. non-optima) if the level curves of a function are not smooth. Also cannot run in parallel. It cannot learn a true multinomial (multiclass) model; instead, the optimization problem is decomposed in a “one-vs-rest” fashion so separate binary classifiers are trained for all classes.  Side note: According to Scikit Documentation: The “liblinear” solver is used by default for historical reasons.\n4. Stochastic Average Gradient(sag): SAG method optimizes the sum of a finite number of smooth convex functions. Like stochastic gradient (SG) methods, the SAG method\u0026rsquo;s iteration cost is independent of the number of terms in the sum. However, by incorporating a memory of previous gradient values the SAG method achieves a faster convergence rate than black-box SG methods.\nIt is faster than other solvers for large datasets, when both the number of samples and the number of features are large.\nDrawbacks:\n It only supports L2 penalization. Its memory cost of O(N), which can make it impractical for large N (because it remembers the most recently computed values for approx. all gradients).  5. SAGA(saga): The SAGA solver is a variant of SAG that also supports the non-smooth penalty=l1 option (i.e. L1 Regularization). This is therefore the solver of choice for sparse multinomial logistic regression and it’s also suitable very Large dataset.\nSide note: According to Scikit Documentation: The SAGA solver is often the best choice.\nSummary The following table is taken from Scikit Documentation\n","date":1582962364,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584657777,"objectID":"25e50f2219880ef0b8c0c1f286c67275","permalink":"https://faithio.cn/post/logistic-regression-solvers-defintions-in-sklearn/","publishdate":"2020-02-29T02:46:04-05:00","relpermalink":"/post/logistic-regression-solvers-defintions-in-sklearn/","section":"post","summary":"Introduction A hypothesis h(x), takes an input and gives us the estimated output value.\nThis hypothesis can be a as simple as a one variable linear equation, .. up to a very complicated and long multivariate equation with respect to the type of the algorithm we’re using (i.e. linear regression, logistic regression..etc).\nOur task is to find the best Parameters (a.k.a Thetas or Weights) that give us the least error in predicting the output.","tags":[],"title":"Logistic Regression   Solvers' Defintions in Sklearn","type":"post"}]