[{"authors":["admin"],"categories":null,"content":"Zixiang Xian is a master student of Machine Learning at Concordia University AI Lab. His research interests include Machine Learning and Computer Vision. He is under the supervision of Professor Nizar Bouguila.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582962603,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://faithio.cn/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Zixiang Xian is a master student of Machine Learning at Concordia University AI Lab. His research interests include Machine Learning and Computer Vision. He is under the supervision of Professor Nizar Bouguila.","tags":null,"title":"Zixiang Xian","type":"authors"},{"authors":["zi_xian"],"categories":null,"content":"Xian Zixiang is a master student of Machine Learning at Concordia University AI Lab. His research interests include Machine Learning and Computer Vision. He is under the supervision of Professor Nizar Bouguila.\nBefore his graduate study, he is a senior backend engineer at Kingsoft Company, skilling at Golang, Java, JavaScript, and Python.\nHe is so motivative and passion for exploring the new world of computer science.\nFeel free to contact me.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1582962603,"objectID":"b531f07f2ad34c1997a37315504c48d8","permalink":"https://faithio.cn/authors/zi_xian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zi_xian/","section":"authors","summary":"Xian Zixiang is a master student of Machine Learning at Concordia University AI Lab. His research interests include Machine Learning and Computer Vision. He is under the supervision of Professor Nizar Bouguila.\nBefore his graduate study, he is a senior backend engineer at Kingsoft Company, skilling at Golang, Java, JavaScript, and Python.\nHe is so motivative and passion for exploring the new world of computer science.\nFeel free to contact me.","tags":null,"title":"XIAN ZIXIANG","type":"authors"},{"authors":[],"categories":[],"content":"When I run some python code from github, it occur the following problem as screenshot.\n RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of \u0026lsquo;python\u0026rsquo; with \u0026lsquo;pythonw\u0026rsquo;. See \u0026lsquo;Working with Matplotlib on OSX\u0026rsquo; in the Matplotlib FAQ for more information.\n Solution:(https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python)\nProblem Cause In mac os image rendering back end of matplotlib (what-is-a-backend to render using the API of Cocoa by default). There is Qt4Agg and GTKAgg and as a back-end is not the default. Set the back end of macosx that is differ compare with other windows or linux os.\nI resolve this issue following ways:\n I assume you have installed the pip matplotlib, there is a directory in you root called ~/.matplotlib. Create a file ~/.matplotlib/matplotlibrc there and add the following code: backend: TkAgg  From this link you can try different diagram.\n","date":1583775429,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583802329,"objectID":"f401314725b6ff0b004bf2f679a80813","permalink":"https://faithio.cn/post/run-issue-with-matplotlib-in-mac-os-x/","publishdate":"2020-03-09T13:37:09-04:00","relpermalink":"/post/run-issue-with-matplotlib-in-mac-os-x/","section":"post","summary":"When I run some python code from github, it occur the following problem as screenshot.\n RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends.","tags":[],"title":"Run Issue With Matplotlib in Mac OS X","type":"post"},{"authors":[],"categories":[],"content":" Math Formula Factor graphs Shows how a function of several variables can be factored into a product of simpler functions.\n $$ f(x,y,z) = (x+y) \\cdot (y + z) \\cdot (x +z) $$\n Very useful for representing posteriors.\n$ $$ P(x1, ..., x_n) = P(x_1) \\Pi P( x_i | x_{i-1} ) $$ $\n$$ P(m|x1, \u0026hellip;, x_n) = P(m) \\cdot \\Pi P(x_i|m)$$\nmodeling  What graph should I use for this data?  Inference  Given the graph and data, what is the mean of x algorithm  Sampling Variable elimination Message-passing(Expectation Propagation, Variational Bayes)   Cutter problem  Want to estimate x given multiple y\u0026rsquo;s $$ p(x) = \\mathcal{N}(x; 0, 100) $$ $$ p(y_i|x) = (0.5)\\mathcal{N}(y_i; x, 1) + (0.5)\\mathcal{N} (y_i;0,10)$$  -\u0026gt; $ P(x|y1, \u0026hellip;, y_n) = P(x) \\cdot \\Pi P(y_i|x)$\nif we only have 2 points:\n$$ P(x) \\cdot P(y_1|x) \\cdot P(y_2|x) \\rightarrow p(y_i|x) = (0.5)\\mathcal{N}(y_i; x, 1) + (0.5)\\mathcal{N} (y_i;0,10)$$\n2 points have 4 Gaussians -\u0026gt; N points $$2^N$$ Gaussians\n https://zhuanlan.zhihu.com/p/75617364 $$ p(z | w)=\\frac{p(w | z) p(z)}{p(w)}=\\frac{p(w | z) p(z)}{\\int_{z} p(w | z) p(z) d z} $$ Because it extends belief propagation. Belief propagation passes the entire distribution is the message. While EP will only pass onto the distribution certain expectation distribution allows you to you get a very compact message.\n Expectation Propagation  Fits an exponential-family approximation to the posterior. Belief propagation is a special case Kalman filtering is a special case Does not always converge.  May get stuck due to improper distributions May oscillate due to loopy graph   AGM $ $$ p(\\mathbf{X} | \\Theta)=\\sum_{j=1}^{M} p_{j} p\\left(\\mathbf{X} | \\xi_{j}\\right) $$ $\n $\\xi_j$ is the set of the parameters of component j. $ p_j$ are the mixing proptions which must be positive and sum to one. $\\Theta = {p_1, \\ldots, p_M, \\xi_1, \\ldots, \\xi_M}$ is the complete set of parameters fully characterizing the mixture. $ M \\geq 1$ is number of components in the mixture.  $ $$ p\\left(\\vec{X} | \\theta_{j}\\right)=\\prod_{d=1}^{D} \\sqrt{\\frac{2}{\\pi}} \\frac{1}{\\left(\\sigma_{l_{j d}}+\\sigma_{r_{j d}}\\right)} \\times\\left\\{\\begin{array}{ll}\\exp \\left[-\\frac{\\left(X_{d}-\\mu_{j d}\\right)^{2}}{2 \\sigma_{l_{j d} }^{2}}\\right] \u0026amp; \\text { if } X_{d}\u0026lt;\\mu_{j d} \\\\ \\exp \\left[-\\frac{\\left(X_{d}-\\mu_{j d}\\right)^{2}}{2 \\sigma_{r_{j d}}^{2}}\\right] \u0026amp; \\text { if } X_{d} \\geq \\mu_{j d}\\end{array}\\right. $$ $\n $\\xi_{j}=\\left(\\vec{\\mu}_{j}, \\vec{\\sigma}_{l_{j}}, \\vec{\\sigma}_{r_{j}}\\right)$ is the set of the parameters of components $j$ $\\vec{\\mu}_{j}=\\left(\\mu_{j 1}, \\ldots, \\mu_{j D}\\right)$ is the mean $\\vec{\\sigma}_{l_{j}}=\\left(\\vec{\\sigma}_{l_{j 1}}, \\ldots, \\vec{\\sigma}_{l_{j D}}\\right)$ is the left standard deviation $\\vec{\\sigma}_{r_{j}}=\\left(\\vec{\\sigma}_{r_{j 1}}, \\ldots, \\vec{\\sigma}_{r_{j D}}\\right)$ is the right standard deviation  $$ p\\left(\\theta_j | \\vec{X} \\right)= \\frac{p(\\theta_j)\\times p\\left(\\vec{X} | \\thetaj\\right)}{p(\\vec{X})} = \\frac{1}{p(\\vec{X})} \\prod{i} f_{i}(\\boldsymbol{\\theta}) $$\n$$ p(\\vec{X})= \\int \\prod{i} f{i}(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta} $$\nHere, $p(\\vec{X})$ is very intractable to calculate and we don\u0026rsquo;t know $ f_{i}(\\boldsymbol{\\theta}) $.\nNow we consider using EP. The approximation, $q\\left(\\theta_j \\right)$ , of the posterior, $p\\left( \\theta_j | \\vec{X} \\right)$ , is assumed to have same functional form. $$ q(\\theta_j)=\\frac{1}{Z} \\prod_i \\widetilde{f}_i(\\theta_j) $$\nin which each factor $\\widetilde{f}_i(\\theta_j)$ in the approximation corresponds to one of the factors $f_i(\\theta_j)$ in the true posterior. $\\widetilde{f}_i(\\theta_j) $ is a asymetric Gaussian.\n$$ p(\\mathbf{X} | \\boldsymbol{\\theta})=(1-w) \\mathcal{A}(\\mathbf{X} | \\boldsymbol{\\theta}, \\mathbf{I_l}, \\mathbf{I_r})+w \\mathcal{A}(\\mathbf{X} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}) $$\nwhere w is the proportion of background clutter. And the prior over $\\mathbf{\\theta}$(mean) is taken to be Asymmetric Gaussian.\nAnd $$ p(\\boldsymbol{\\theta})= \\mathcal{A}(\\mathbf{X} | \\mathbf{0}, b \\mathbf{I_l}, b \\mathbf{Ir}) $$ $$ p(\\mathcal{X}, \\boldsymbol{\\theta})=p(\\boldsymbol{\\theta}) \\prod{n=1}^{N} p\\left(\\mathbf{x}_{n} | \\boldsymbol{\\theta}\\right) $$\n1. initialize the approximating factors we select an approximating distribution from the exponential family to approximate the stochastic variables $\\theta$ $$ q(\\boldsymbol{\\theta})=\\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu}, \\mathbf{\\sigma_r^2}, \\mathbf{\\sigma_l^2}) = \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu}, v_l \\mathbf{I}, v_r \\mathbf{I})\n= \\mathcal{A}(\\mathbf{X} | \\mathbf{0}, b \\mathbf{I_l}, b \\mathbf{I_r}) $$\n$$ \\widetilde{f}_{n}(\\boldsymbol{\\theta})=s_n \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}n, \\mathbf{\\sigma{rn}^2}, \\mathbf{\\sigma{l_n}^2} \\right) $$\n$$ sn = \\prod{d=1}^{D} \\sqrt{\\frac{2}{\\pi}} \\frac{1}{\\left(\\sigma{l{d}}+\\sigma{r{d}}\\right)} $$ While $\\sigma_{ln} \\rightarrow \\infty, \\sigma{r_n} \\rightarrow \\infty $ and $ \\mu_n = 0 $.\n###2. initialize the posterior approximation $q(\\boldsymbol{\\theta})$\nWe chooses the parameter values a = 10, b = 100 and w = 0.5 and use $v$ denote $ \\sigma^2$ as following, then $\\mathbf{v_r} = \\mathbf{v_l} = b = 100$\n3. Until all $(\\mun, v{ln}, v{r_n}, s_n)$ converge: $$ q^{\\backslash n}(\\boldsymbol{\\theta})=\\frac{q(\\boldsymbol{\\theta})}{\\widetilde{f}_n(\\boldsymbol{\\theta})} = \\frac{\\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu}, \\mathbf{v_r I}, \\mathbf{v_l I})}{s_n \\mathcal{A}\\left(\\boldsymbol{\\theta} | \\mathbf{\\mu}n, \\mathbf{v{rn} I}, \\mathbf{v{l_n} I} \\right)} \\propto \\left{\\begin{array}{ll}\n{\\frac{\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mu})^{T}(v_l \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu})\\right}}{\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mun})^{T}(v{l_n} \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu_n})\\right}}} \u0026amp;\u0026amp; \\text { if } X\u0026lt;\\mu \\\n{\\frac{\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mu})^{T}(v_r \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu})\\right}}{\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mun})^{T}(v{r_n} \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu_n})\\right}}} \u0026amp;\u0026amp; \\text { if } X\u0026gt;\\mu\n\\end{array}\\right. \\\n= \\left{\\begin{array}{ll}\n\\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mu})^{T}(v_l \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu}) + \\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mun})^{T}(v{l_n} \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu_n})\\right} \u0026amp; \\text { if } X\u0026lt;\\mu \\exp \\left{-\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mu})^{T}(v_r \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu}) +\n\\frac{1}{2}(\\boldsymbol{X}-\\mathbf{\\mun})^{T}(v{r_n} \\mathbf{I})^{-1}(\\boldsymbol{X}-\\mathbf{\\mu_n}) \\right} \u0026amp; \\text { if } X\u0026gt;\\mu \\end{array}\\right. $$ - Remove the current estimate $\\widetilde{f}_j(\\boldsymbol{\\theta})$ from $q(\\theta)$, then we has mean and inverse variance given by: $$ \\left{\\begin{array}{ll} \\left({v_l}^{\\backslash n}\\right)^{-1}={v_l}^{-1}-{vl}{n}^{-1} \u0026amp; \\text { if } X\u0026lt;\\mu \\left({v_r}^{\\backslash n}\\right)^{-1}={v_r}^{-1}-{vr}{n}^{-1} \u0026amp; \\text { if } X\u0026gt;\\mu \\end{array}\\right. $$\n$$ \\mathbf{\\mu}^{\\backslash n}= \\mathbf{\\mu}+\n\\left{\\begin{array}{ll} {v_l}^{\\backslash n} {vl}{n}^{-1}\\left(\\mathbf{\\mu}-\\mathbf{\\mu}_{n}\\right) \u0026amp; \\text { if } X\u0026lt;\\mu \\\n{v_r}^{\\backslash n} {vr}{n}^{-1}\\left(\\mathbf{\\mu}-\\mathbf{\\mu}_{n}\\right) \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$ \u0026gt; Cavity Distribution: \u0026gt; $$ \u0026gt; q^{\\backslash j}(\\boldsymbol{\\theta})=\\frac{q(\\boldsymbol{\\theta})}{\\widetilde{f}_{j}(\\boldsymbol{\\theta})} \u0026gt; $$\n Recompute $(\\mu, v, Z)$ from $(\\mathbf{\\mu}^{\\backslash n}, {v_l}^{\\backslash n}, {vr}^{\\backslash n})$ $$ Z{n}=(1-w) \\mathcal{A}\\left(\\mathbf{x}_{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(vr^{\\backslash n}+1\\right) \\mathbf{I}\\right)+w \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{Ir}\\right) $$ \u0026gt;$$ \u0026gt;\\begin{aligned} \u0026gt;Z{n} \u0026amp;=\\int q^{\\backslash n}(\\boldsymbol{\\theta}) f{n}(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta} \\ \u0026gt;\u0026amp;=\\int q^{\\backslash n}(\\boldsymbol{\\theta}) \\widetilde{f}{n}(\\boldsymbol{\\theta}) \\mathrm{d} \\boldsymbol{\\theta} \u0026gt; \u0026gt;\u0026amp;=\\int \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu^{\\backslash n}}, v_l^{\\backslash n} \\mathbf{I}, v_r^{\\backslash n} \\mathbf{I}) \\cdot { (1-w) \\mathcal{A}(\\mathbf{x_n} | \\boldsymbol{\\mu}, \\mathbf{I_l}, \\mathbf{I_r})+w \\mathcal{A}(\\mathbf{x_n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r})} \\mathrm{d} \\boldsymbol{\\theta} \u0026gt; \u0026gt;\u0026amp;= (1-w)\\int \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu^{\\backslash n}}, v_l^{\\backslash n} \\mathbf{I}, v_r^{\\backslash n} \\mathbf{I}) \\mathcal{A}(\\mathbf{x_n} | \\boldsymbol{\\mu}, \\mathbf{I_l}, \\mathbf{I_r}) \\mathrm{d} \\boldsymbol{\\theta} \u0026gt;\u0026amp;+ w \\int \\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{\\mu^{\\backslash n}}, v_l^{\\backslash n} \\mathbf{I}, v_r^{\\backslash n} \\mathbf{I}) \u0026gt;\\mathcal{A}(\\mathbf{x_n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{Ir})} \\mathrm{d} \\boldsymbol{\\theta} \u0026gt;\u0026amp;=(1-w) \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(vr^{\\backslash n}+1\\right) \\mathbf{I}\\right)+w \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}\\right) \u0026gt;\\end{aligned} \u0026gt;$$  we assumed that $f{0}(\\boldsymbol{\\theta})=p(\\boldsymbol{\\theta})$ and $ f{n}(\\boldsymbol{\\theta})=p\\left(\\mathbf{x}_{n} | \\boldsymbol{\\theta}\\right) = (1-w) \\mathcal{A}(\\mathbf{X} | \\boldsymbol{\\mu}, \\mathbf{I_l}, \\mathbf{I_r})+w \\mathcal{A}(\\mathbf{X} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}) $, also $q(\\boldsymbol{\\theta})=\\mathcal{A}(\\boldsymbol{\\theta} | \\mathbf{m}, v_l \\mathbf{I}, vr \\mathbf{I}) $ $$ \\begin{aligned} \\rho{n} \u0026amp;=\\frac{1}{Z{n}}(1-w) \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{\\mu}^{\\backslash n},\\left(v_l^{\\backslash n}+1\\right) \\mathbf{I}, \\left(vr^{\\backslash n}+1\\right) \\mathbf{I}\\right) \u0026amp;= \\frac{1}{Z{n}}(1-w)\\cdot \\frac{Zn - w \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{I_r}\\right)}{1-w} \u0026amp;= 1 - \\frac{w}{Zn} \\cdot \\mathcal{A}\\left(\\mathbf{x}{n} | \\mathbf{0}, a \\mathbf{I_l}, a \\mathbf{Ir}\\right) \\end{aligned} $$ Basic rule for Asymmetric Gaussian: $$ \\nabla{\\boldsymbol{\\mu}} \\mathcal{A}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{v_l}, \\mathbf{v_r})=\n\\left{\\begin{array}{ll}\n\\mathcal{A}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{v_l}, \\mathbf{v_r}) \\cdot(\\mathbf{x}-\\boldsymbol{\\mu}) \\mathbf{v_l}^{-1} \u0026amp; \\text { if } X\u0026lt;\\mu \\\n\\mathcal{A}(\\mathbf{x} | \\boldsymbol{\\mu}, \\mathbf{v_l}, \\mathbf{v_r}) \\cdot(\\mathbf{x}-\\boldsymbol{\\mu}) \\mathbf{v_r}^{-1} \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$ So we compute the mean and variance: $$ \\mathbf{\\mu^{new}}=\\mathbf{\\mu}^{\\backslash n}+\n\\left{\\begin{array}{ll}\n\\rho_{n} \\frac{v_l^{\\backslash n}}{vl^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \u0026amp; \\text { if } X\u0026lt;\\mu \\\n\\rho_{n} \\frac{v_r^{\\backslash n}}{vr^{\\backslash n}+1}\\left(\\mathbf{x}{n}-\\mathbf{\\mu}^{\\backslash n}\\right) \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$\n$$ \\left{\\begin{array}{ll} v_l^{new}=vl^{\\backslash n}-\\rho{n} \\frac{\\left(v_l^{\\backslash n}\\right)^{2}}{vl^{\\backslash n}+1}+\\rho{n}\\left(1-\\rho_{n}\\right) \\frac{\\left(vl^{\\backslash n}\\right)^{2}\\left|\\mathbf{x}{n}-\\mathbf{m}^{\\backslash n}\\right|^{2}}{D\\left(v_l^{\\backslash n}+1\\right)^{2}} \u0026amp; \\text { if } X\u0026lt;\\mu \\\nv_r^{new}=vr^{\\backslash n}-\\rho{n} \\frac{\\left(v_r^{\\backslash n}\\right)^{2}}{vr^{\\backslash n}+1}+\\rho{n}\\left(1-\\rho_{n}\\right) \\frac{\\left(vr^{\\backslash n}\\right)^{2}\\left|\\mathbf{x}{n}-\\mathbf{m}^{\\backslash n}\\right|^{2}}{D\\left(v_r^{\\backslash n}+1\\right)^{2}} \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$ - Evaluate and store the new factor $$ \\left{\\begin{array}{ll} \\left({v_{l_n}}\\right)^{-1}={(v_l^{new})}^{-1}-({vl}^{\\backslash n})^{-1} \u0026amp; \\text { if } X\u0026lt;\\mu \\left({v{r_n}}\\right)^{-1}={(v_r^{new})}^{-1}-({v_r}^{ \\backslash n})^{-1} \u0026amp; \\text { if } X\u0026gt;\\mu \\end{array}\\right. $$\n$$ \\mathbf{m}_{n}=\\mathbf{m}^{\\backslash n}+ \\left{\\begin{array}{ll}\n\\left(v_{n}+v^{\\backslash n}\\right)\\left(v^{\\backslash n}\\right)^{-1}\\left(\\mathbf{m}^{\\mathrm{new}}-\\mathbf{m}^{\\backslash n}\\right) \u0026amp; \\text { if } X\u0026lt;\\mu \\\n\\left(v_{n}+v^{\\backslash n}\\right)\\left(v^{\\backslash n}\\right)^{-1}\\left(\\mathbf{m}^{\\mathrm{new}}-\\mathbf{m}^{\\backslash n}\\right) \u0026amp; \\text { if } X\u0026gt;\\mu \\\n\\end{array}\\right. $$\n","date":1582994068,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1584158587,"objectID":"985b28a7398b99c38e892b86d015b478","permalink":"https://faithio.cn/post/stochastic-expectation-propagation/","publishdate":"2020-02-29T11:34:28-05:00","relpermalink":"/post/stochastic-expectation-propagation/","section":"post","summary":"Math Formula Factor graphs Shows how a function of several variables can be factored into a product of simpler functions.\n $$ f(x,y,z) = (x+y) \\cdot (y + z) \\cdot (x +z) $$\n Very useful for representing posteriors.\n$ $$ P(x1, ..., x_n) = P(x_1) \\Pi P( x_i | x_{i-1} ) $$ $\n$$ P(m|x1, \u0026hellip;, x_n) = P(m) \\cdot \\Pi P(x_i|m)$$\nmodeling  What graph should I use for this data?","tags":[],"title":"Stochastic Expectation Propagation","type":"post"}]